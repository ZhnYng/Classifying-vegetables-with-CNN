<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>51a5eac9d9ea4c77a3eb51ef92e70007</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="dele-ca1-part-a" class="cell markdown">
<h1><strong>DELE CA1 Part A</strong></h1>
<p>Name: Lim Zhen Yang</p>
<p>Admission Number: 2214506</p>
<p>Class: DAAA/FT/2B/04</p>
</section>
<div class="cell markdown">
<p><strong>All imports</strong></p>
</div>
<div class="cell code" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shutil</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> backend <span class="im">as</span> K</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> regularizers</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_tuner.tuners <span class="im">import</span> RandomSearch</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_tuner <span class="im">import</span> RandomSearch, HyperParameters</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.over_sampling <span class="im">import</span> SMOTE</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential, Model, load_model</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> imblearn.under_sampling <span class="im">import</span> RandomUnderSampler</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras_tuner <span class="im">import</span> HyperModel, HyperParameters</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.optimizers <span class="im">import</span> Adam, SGD, RMSprop</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> (</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    Dense,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    Conv2D,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    MaxPooling2D,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    MaxPool2D,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    Flatten,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    Dropout,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    BatchNormalization,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    GlobalAveragePooling2D,</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    Input,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    Add,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    Activation,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    AveragePooling2D,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> (</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    precision_recall_fscore_support,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    confusion_matrix,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    fbeta_score,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    accuracy_score,</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> (</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    EarlyStopping,</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    LearningRateScheduler,</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    ModelCheckpoint,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    ReduceLROnPlateau,</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    TensorBoard,</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:From c:\Users\zzhen\AppData\Local\Programs\Python\Python310\lib\site-packages\keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Check and limit GPU usage</p>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if TensorFlow can access GPUs</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> tf.test.gpu_device_name():</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Default GPU Device: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(tf.test.gpu_device_name()))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    gpus <span class="op">=</span> tf.config.experimental.list_physical_devices(<span class="st">&#39;GPU&#39;</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gpus:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Currently, memory growth needs to be the same across GPUs</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> gpu <span class="kw">in</span> gpus:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                tf.config.experimental.set_memory_growth(gpu, <span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            logical_gpus <span class="op">=</span> tf.config.experimental.list_logical_devices(<span class="st">&#39;GPU&#39;</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="bu">len</span>(gpus), <span class="st">&quot;Physical GPUs,&quot;</span>, <span class="bu">len</span>(logical_gpus), <span class="st">&quot;Logical GPUs&quot;</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Memory growth must be set before GPUs have been initialized</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(e)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;No GPU devices found. TensorFlow will use CPU.&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Default GPU Device: /device:GPU:0
1 Physical GPUs, 1 Logical GPUs
</code></pre>
</div>
</div>
<section id="background-research" class="cell markdown">
<h1><strong>BACKGROUND RESEARCH</strong></h1>
</section>
<section id="vegnet-dataset-for-machine-learning-applications"
class="cell markdown">
<h3>VegNet Dataset for Machine Learning Applications</h3>
<p>There have been efforts put into image classification on vegetables.
Most noteably, VegNet, which is a dataset of images of vegetables in
different qualities. E.g. Ripe, Dried, Damaged, etc...</p>
<p><a
href="https://www.sciencedirect.com/science/article/pii/S2352340922008629"
class="uri">https://www.sciencedirect.com/science/article/pii/S2352340922008629</a></p>
<p>Another dataset from Kaggle, named Vegetable Image Dataset, consists
of 21,000 colored images covering 15 types of vegetables. It has a very
similar dataset to ours.</p>
<p><a
href="https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset"
class="uri">https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset</a></p>
</section>
<section
id="deep-convolutional-neural-network-dcnn-and-image-classification"
class="cell markdown">
<h3>Deep Convolutional Neural Network (DCNN) and Image
Classification</h3>
<p>Extensive research has been conducted using various plant datasets,
including fruits and vegetables, for tasks like image augmentation,
segmentation, detection, and classification​​.</p>
<p><a
href="https://www.researchgate.net/publication/352846889_DCNN-Based_Vegetable_Image_Classification_Using_Transfer_Learning_A_Comparative_Study"
class="uri">https://www.researchgate.net/publication/352846889_DCNN-Based_Vegetable_Image_Classification_Using_Transfer_Learning_A_Comparative_Study</a></p>
<p>A specific DCNN model reached an accuracy rate of 92.1% on a
vegetable image dataset, a significant improvement over the SVM
classifier's 80.5% accuracy rate​​.</p>
<p><a href="https://www.nature.com/articles/s41438-021-00560-9"
class="uri">https://www.nature.com/articles/s41438-021-00560-9</a></p>
</section>
<section id="hybrid-deep-learning-based-approaches"
class="cell markdown">
<h3>Hybrid Deep Learning-Based Approaches</h3>
<p>Hybrid deep learning-based frameworks, such as attention-based
densely connected convolutional networks, are being developed for
precise fruit and vegetable classification​​.</p>
<p><a
href="https://link.springer.com/article/10.1007/s40747-020-00192-x#"
class="uri">https://link.springer.com/article/10.1007/s40747-020-00192-x#</a>:~</p>
</section>
<section id="vegetable-recognition-and-classification"
class="cell markdown">
<h3>Vegetable Recognition and Classification</h3>
<p>Another study involved training images on the VGG-M-BN network,
selecting subsets of images ranging from 1,500 to 24,000 from a total of
48,000 vegetable images for classification tasks​​.</p>
<p><a
href="https://www.researchgate.net/publication/341843391_Vegetable_Recognition_and_Classification_Based_on_Improved_VGG_Deep_Learning_Network_Model#"
class="uri">https://www.researchgate.net/publication/341843391_Vegetable_Recognition_and_Classification_Based_on_Improved_VGG_Deep_Learning_Network_Model#</a>:~:text=vegetable%20image%20classification%20and%20the,network%20fro%20m%2048000%20veg</p>
</section>
<section id="data-exploration" class="cell markdown">
<h1><strong>DATA EXPLORATION</strong></h1>
</section>
<section id="loading-data-into-dataframe" class="cell markdown">
<h3><strong>LOADING DATA INTO DATAFRAME</strong></h3>
</section>
<div class="cell code" data-execution_count="2">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dir_to_df(data_dir):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    filepaths <span class="op">=</span> []</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">dir</span> <span class="kw">in</span> os.listdir(data_dir):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> filepath <span class="kw">in</span> os.listdir(os.path.join(data_dir, <span class="bu">dir</span>)):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            labels.append(<span class="bu">dir</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            filepaths.append(os.path.join(data_dir, <span class="bu">dir</span>, filepath))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(filepaths, labels)), columns<span class="op">=</span>[<span class="st">&quot;filepath&quot;</span>, <span class="st">&quot;label&quot;</span>])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> dir_to_df(data_dir<span class="op">=</span><span class="vs">r&quot;.\Dataset for CA1 part A\test&quot;</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> dir_to_df(data_dir<span class="op">=</span><span class="vs">r&quot;.\Dataset for CA1 part A\train&quot;</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>val_data <span class="op">=</span> dir_to_df(data_dir<span class="op">=</span><span class="vs">r&quot;.\Dataset for CA1 part A\validation&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;The training dataset has </span><span class="sc">{</span>train_data<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> images and </span><span class="sc">{</span><span class="bu">len</span>(train_data[<span class="st">&#39;label&#39;</span>].unique())<span class="sc">}</span><span class="ss"> classes&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The training dataset has 9028 images and 15 classes
</code></pre>
</div>
</div>
<section id="number-of-samples" class="cell markdown">
<h3><strong>NUMBER OF SAMPLES</strong></h3>
<p>From the graph below we can see that there is an uneven number of
samples for each category in the training data</p>
<ul>
<li>Standard approaches would be to
<ul>
<li>adjust the class_weights argument in model.fit, which you can use to
make the model learn more from the minority class</li>
<li>reducing the size of the majority class</li>
<li>or accepting the imbalance. Deep learning can cope with this, it
just needs lots more data</li>
</ul></li>
</ul>
</section>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>filepath_counts <span class="op">=</span> train_data[<span class="st">&quot;label&quot;</span>].value_counts()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">&quot;Training data&quot;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">&quot;Validation data&quot;</span>, fontsize<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>ax1.tick_params(axis<span class="op">=</span><span class="st">&quot;x&quot;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>ax2.tick_params(axis<span class="op">=</span><span class="st">&quot;x&quot;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>sns.histplot(train_data, x<span class="op">=</span><span class="st">&quot;label&quot;</span>, color<span class="op">=</span><span class="st">&quot;#6fc6ab&quot;</span>, legend<span class="op">=</span><span class="va">False</span>, ax<span class="op">=</span>ax1)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>sns.histplot(val_data, x<span class="op">=</span><span class="st">&quot;label&quot;</span>, color<span class="op">=</span><span class="st">&quot;#fc8b5f&quot;</span>, legend<span class="op">=</span><span class="va">False</span>, ax<span class="op">=</span>ax2)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/979504211d4c56c8df049f65ed70a8148328e2e9.png" /></p>
</div>
</div>
<section id="visualize-dataset" class="cell markdown">
<h3><strong>VISUALIZE DATASET</strong></h3>
</section>
<div class="cell markdown">
<ol>
<li>Some images contain one item in frame, and others with more than one
item of the same item in frame</li>
<li>Some images contain different varients of the same item <em>(e.g.
Capsicum in yellow and red)</em></li>
</ol>
<p>These characteristics of the dataset may make it challenging for the
model to isolate specific objects from the image or relate objects of
similar characteristics as under the same label</p>
<p>Using grayscale images makes classification even harder for the
model, due to the lack of RGB data that would help it <em>(e.g. Capsicum
in yellow and red)</em></p>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading images</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>training_gen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    rescale<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># divides pixel values by 255 to make it within [0, 1]</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading training data</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> training_gen.flow_from_dataframe(</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;sparse&quot;</span>,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>g_dict <span class="op">=</span> train_gen.class_indices  <span class="co"># defines dictionary {&#39;class&#39;: index}</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> <span class="bu">list</span>(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    g_dict.keys()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># defines list of dictionary&#39;s kays (classes), classes names : string</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(train_gen)  <span class="co"># get a batch size samples from the generator</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">20</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>):</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> images[i]  <span class="co"># scales data to range (0 - 1)</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> np.argmax(labels[i])  <span class="co"># get image index</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    class_name <span class="op">=</span> classes[index]  <span class="co"># get class of image</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    plt.title(class_name, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">&quot;off&quot;</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/3fece5cd8aaa4903b67299cf52df9651b991f135.png" /></p>
</div>
</div>
<section id="image-averaging" class="cell markdown">
<h3><strong>IMAGE AVERAGING</strong></h3>
<p>Image Averaging involves stacking multiple photos on top of each
other and averaging them together. The main purpose is to see the noise
of the image and therefore reducing it.</p>
</section>
<div class="cell code">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize an empty list to store the images</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> []</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the number of batches in the generator</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="bu">len</span>(train_gen)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through each batch and collect the images</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_batches):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> train_gen.<span class="bu">next</span>()  <span class="co"># Get the next batch</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    batch_images <span class="op">=</span> batch[<span class="dv">0</span>]  <span class="co"># The first element of the batch contains the images</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    images.append(batch_images)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the list of batches into a single numpy array</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>all_images <span class="op">=</span> np.concatenate(images, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.mean(all_images, axis<span class="op">=</span><span class="dv">0</span>), cmap<span class="op">=</span><span class="st">&quot;Greys&quot;</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">&quot;off&quot;</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/97a25e7531d94961e600e639aa6d655dedf6a241.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>Theres a dark spot at the top of the image. This shows that there are
more images at the top of the image compared to anywhere else.</p>
<p>The rest of the image is mostly gray, which shows that these images
may not follow a specific pattern.</p>
</div>
<section id="data-preparation" class="cell markdown">
<h1><strong>DATA PREPARATION</strong></h1>
</section>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>DEFAULT_SHAPE <span class="op">=</span> (<span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># Original image size</span></span></code></pre></div>
</div>
<section id="oversampling" class="cell markdown">
<h3><strong>OVERSAMPLING</strong></h3>
</section>
<section id="oversampling-to-get-new-images" class="cell markdown">
<h4><strong>Oversampling to get new images</strong></h4>
<p>Using SMOTE to oversample</p>
</section>
<div class="cell code" data-execution_count="15">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">&quot;./Dataset for CA1 part A/train&quot;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> os.listdir(data_dir):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    label_path <span class="op">=</span> os.path.join(data_dir, label)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image_file <span class="kw">in</span> os.listdir(label_path):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        image_path <span class="op">=</span> os.path.join(label_path, image_file)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> cv2.imread(image_path)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> cv2.resize(image, DEFAULT_SHAPE)  <span class="co"># Resize to a desired size</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        data.append(image)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        labels.append(label)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert data and labels to numpy arrays</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(data)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array(labels)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape data into a 2D format</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>data_flattened <span class="op">=</span> data.reshape(data.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply random oversampling</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>oversampler <span class="op">=</span> SMOTE(sampling_strategy<span class="op">=</span><span class="st">&quot;all&quot;</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>data_resampled, labels_resampled <span class="op">=</span> oversampler.fit_resample(data_flattened, labels)</span></code></pre></div>
</div>
<section id="writing-new-images-to-oversampling-folder"
class="cell markdown">
<h4><strong>Writing new images to oversampling folder</strong></h4>
</section>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">&quot;./oversampled_images&quot;</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(output_dir):</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    shutil.rmtree(output_dir)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    os.makedirs(output_dir)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (image_data, label) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(data_resampled, labels_resampled)):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape the flattened image data back to its original shape</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image_data.reshape((<span class="op">*</span>DEFAULT_SHAPE, <span class="dv">3</span>))</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a subdirectory for each class label if it doesn&#39;t exist</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    class_output_dir <span class="op">=</span> os.path.join(output_dir, label)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(class_output_dir):</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        os.makedirs(class_output_dir)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the image with a unique filename</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    image_filename <span class="op">=</span> os.path.join(class_output_dir, <span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.png&quot;</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    cv2.imwrite(image_filename, image)</span></code></pre></div>
</div>
<section id="convert-to-dataframe" class="cell markdown">
<h5><strong>Convert to DataFrame</strong></h5>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The numbers of images after each class is now equal at 929</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>train_data_oversampled <span class="op">=</span> dir_to_df(</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    data_dir<span class="op">=</span><span class="vs">r&quot;./oversampled_images&quot;</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Convert directory to dataframe</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>train_data_oversampled.value_counts([<span class="st">&quot;label&quot;</span>])</span></code></pre></div>
<div class="output execute_result" data-execution_count="7">
<pre><code>label       
Bean            955
Bitter_Gourd    955
Bottle_Gourd    955
Brinjal         955
Broccoli        955
Cabbage         955
Capsicum        955
Carrot          955
Cauliflower     955
Cucumber        955
Papaya          955
Potato          955
Pumpkin         955
Radish          955
Tomato          955
Name: count, dtype: int64</code></pre>
</div>
</div>
<section id="undersampling" class="cell markdown">
<h3><strong>UNDERSAMPLING</strong></h3>
</section>
<section id="undersampling-to-get-less-images" class="cell markdown">
<h4><strong>Undersampling to get less images</strong></h4>
</section>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> []</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> []</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">&quot;./Dataset for CA1 part A/train&quot;</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> os.listdir(data_dir):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    label_path <span class="op">=</span> os.path.join(data_dir, label)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image_file <span class="kw">in</span> os.listdir(label_path):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        image_path <span class="op">=</span> os.path.join(label_path, image_file)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> cv2.imread(image_path)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> cv2.resize(image, DEFAULT_SHAPE)  <span class="co"># Resize to a desired size</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        data.append(image)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        labels.append(label)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert data and labels to numpy arrays</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(data)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array(labels)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape data into a 2D format</span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>data_flattened <span class="op">=</span> data.reshape(data.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply random oversampling</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>oversampler <span class="op">=</span> RandomUnderSampler(sampling_strategy<span class="op">=</span><span class="st">&quot;all&quot;</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>data_resampled, labels_resampled <span class="op">=</span> oversampler.fit_resample(data_flattened, labels)</span></code></pre></div>
</div>
<section id="updating-images-to-undersampling-folder"
class="cell markdown">
<h4><strong>Updating images to undersampling folder</strong></h4>
</section>
<div class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">&quot;./undersampled_images&quot;</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(output_dir):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    shutil.rmtree(output_dir)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    os.makedirs(output_dir)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (image_data, label) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(data_resampled, labels_resampled)):</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape the flattened image data back to its original shape</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> image_data.reshape((<span class="op">*</span>DEFAULT_SHAPE, <span class="dv">3</span>))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a subdirectory for each class label if it doesn&#39;t exist</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    class_output_dir <span class="op">=</span> os.path.join(output_dir, label)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(class_output_dir):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        os.makedirs(class_output_dir)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the image with a unique filename</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    image_filename <span class="op">=</span> os.path.join(class_output_dir, <span class="ss">f&quot;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">.png&quot;</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>    cv2.imwrite(image_filename, image)</span></code></pre></div>
</div>
<section id="convert-to-dataframe" class="cell markdown">
<h5><strong>Convert to DataFrame</strong></h5>
</section>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The numbers of images after each class is now equal at 929</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>train_data_undersampled <span class="op">=</span> dir_to_df(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    data_dir<span class="op">=</span><span class="vs">r&quot;./undersampled_images&quot;</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Convert directory to dataframe</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>train_data_undersampled.value_counts([<span class="st">&quot;label&quot;</span>])</span></code></pre></div>
<div class="output execute_result" data-execution_count="8">
<pre><code>label       
Bean            248
Bitter_Gourd    248
Bottle_Gourd    248
Brinjal         248
Broccoli        248
Cabbage         248
Capsicum        248
Carrot          248
Cauliflower     248
Cucumber        248
Papaya          248
Potato          248
Pumpkin         248
Radish          248
Tomato          248
Name: count, dtype: int64</code></pre>
</div>
</div>
<section id="image-data-generator" class="cell markdown">
<h3><strong>IMAGE DATA GENERATOR</strong></h3>
<p>It ensures that our model will receive variation in data at each
epoch, which helps <strong>prevent overfitting</strong>. At the same
time, the ImageDataGenerator uses <strong>less memory</strong> as it
loads all the images in batches rather than all at once.</p>
</section>
<div class="cell code" data-execution_count="17">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>INPUT_SHAPE_128 <span class="op">=</span> (<span class="dv">128</span>, <span class="dv">128</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>INPUT_SHAPE_31 <span class="op">=</span> (<span class="dv">31</span>, <span class="dv">31</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span></code></pre></div>
</div>
<section id="data-augmentation" class="cell markdown">
<h3>Data Augmentation</h3>
<ul>
<li>Rotation: Images of vegetables taken will not always be taken
exactly vertically, therefore this covers the cases of tilted
images.</li>
<li>Width Shift: Images of vegetables taken will not always be centered,
so this accounts for the left and right shift of the image.</li>
<li>Height Shift: Images of vegetables taken will not always be
centered, so this accounts for the top and bottom shift of the
image.</li>
<li>Zoom Range: Images of vegetables may be taken from different
distances, which would affect the size of the item in the image.</li>
<li>Horizontal Flip: Images may be inverted.</li>
<li>Vertical Flip: Images may be flipped.</li>
<li>Brightness Range: Vegetables may be dimly lit.</li>
</ul>
<p>Data augmentation not only covers these cases but also prevents
overfitting by testing the model on non-augmented images it has not seen
before.</p>
</section>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>test_gen_format <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>)  <span class="co"># Generator for test/validation</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generator for training data (with augmentation)</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>train_gen_augment <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    rescale<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>,</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">20</span>,  <span class="co"># rotation</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    width_shift_range<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># horizontal shift</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    height_shift_range<span class="op">=</span><span class="fl">0.1</span>,  <span class="co"># vertical shift</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.2</span>,  <span class="co"># zoom</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,  <span class="co"># horizontal flip</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    brightness_range<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">1.5</span>], <span class="co"># brightness</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    vertical_flip<span class="op">=</span><span class="va">True</span>, <span class="co"># vertical flip</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>)  </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Generator for training data (no augmentation)</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>train_gen_no_augment <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading validation data</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validation_generator(input_shape):</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>        val_data,</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>        x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>        y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>        target_size<span class="op">=</span>input_shape,</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>        class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>        color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>        batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<section id="model-training" class="cell markdown">
<h1><strong>MODEL TRAINING</strong></h1>
<p>For each model, I will be testing 128x128 and 31x31 image sizes</p>
</section>
<section id="optimizers" class="cell markdown">
<h3><strong>Optimizers</strong></h3>
</section>
<div class="cell markdown">
<p>There are a lot of different types of optimizers offered by
Tensorflow. The most common 2 are Adam and SGD optimizers.</p>
<p><strong>Adam</strong></p>
<p>Adam optimization is a stochastic gradient descent method that is
based on adaptive estimation of first-order and second-order
moments.</p>
<p><strong>SGD</strong></p>
<p>SGD also known as Stochastic gradient descent is an iterative method
for optimizing an objective function with suitable smoothness.</p>
<p><strong>Difference between Adam and SGD</strong></p>
<p>Adam outperforms SGD in terms of speed, thanks to its utilization of
coordinate-wise gradient clipping, which addresses heavy-tailed noise.
Additionally, Adam adjusts the learning rate for each network weight
individually. However, it's worth noting that SGD is recognized for its
superior performance in image classification tasks. While Adam may take
"shortcuts," which is advantageous for NLP and other applications, in
the realm of image classification, the ability to discern every detail
is crucial. Hence, for all upcoming models, we will employ SGD as our
chosen optimizer.</p>
</div>
<section id="callbacks" class="cell markdown">
<h3><strong>Callbacks</strong></h3>
</section>
<div class="cell markdown">
<p><strong>EarlyStopping:</strong></p>
<ul>
<li>Monitors a metric during training and stops the training process if
the metric stops improving. Helps prevent overfitting.</li>
</ul>
<p><strong>LearningRateScheduler:</strong></p>
<ul>
<li>Allows dynamic learning rate scheduling during training based on a
provided function.</li>
</ul>
<p><strong>ModelCheckpoint:</strong></p>
<ul>
<li>Saves the model's weights at specified intervals during training for
later loading and evaluation.</li>
</ul>
<p><strong>ReduceLROnPlateau:</strong></p>
<ul>
<li>Reduces the learning rate when a metric has stopped improving. Often
used in conjunction with EarlyStopping.</li>
</ul>
</div>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step_decay(epoch):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    initial_lrate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    drop <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    epochs_drop <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    lrate <span class="op">=</span> initial_lrate <span class="op">*</span> math.<span class="bu">pow</span>(drop, math.floor((<span class="dv">1</span> <span class="op">+</span> epoch) <span class="op">/</span> epochs_drop))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lrate</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>lr_scheduler <span class="op">=</span> LearningRateScheduler(step_decay)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_checkpoint(<span class="bu">file</span>):</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ModelCheckpoint(</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        filepath<span class="op">=</span><span class="ss">f&quot;./best weights/</span><span class="sc">{</span><span class="bu">file</span><span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">&quot;val_accuracy&quot;</span>,</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>early_stop <span class="op">=</span> EarlyStopping(</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_accuracy&quot;</span>, patience<span class="op">=</span><span class="dv">10</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>, mode<span class="op">=</span><span class="st">&quot;max&quot;</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>reduce_lr <span class="op">=</span> ReduceLROnPlateau(</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">&quot;val_accuracy&quot;</span>, factor<span class="op">=</span><span class="fl">0.5</span>, patience<span class="op">=</span><span class="dv">3</span>, min_lr<span class="op">=</span><span class="fl">0.0000001</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>tensorboard <span class="op">=</span> TensorBoard(log_dir<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span></code></pre></div>
</div>
<section id="utils" class="cell markdown">
<h5><strong>Utils</strong></h5>
</section>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_history(history):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> history.history[<span class="st">&quot;accuracy&quot;</span>]</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    val_acc <span class="op">=</span> history.history[<span class="st">&quot;val_accuracy&quot;</span>]</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> history.history[<span class="st">&quot;loss&quot;</span>]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    val_loss <span class="op">=</span> history.history[<span class="st">&quot;val_loss&quot;</span>]</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    epochs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(acc) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Accuracy subplot</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, acc, <span class="st">&quot;b-o&quot;</span>, label<span class="op">=</span><span class="st">&quot;Training Accuracy&quot;</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, val_acc, <span class="st">&quot;g-x&quot;</span>, label<span class="op">=</span><span class="st">&quot;Validation Accuracy&quot;</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Training and Validation Accuracy&quot;</span>)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Epoch&quot;</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Accuracy&quot;</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    plt.ylim([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic x-axis ticks</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    tick_spacing <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(epochs) <span class="op">//</span> <span class="dv">10</span>)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    plt.xticks(epochs[::tick_spacing])</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    max_val_acc <span class="op">=</span> <span class="bu">max</span>(val_acc)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    plt.text(</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(epochs) <span class="op">*</span> <span class="fl">0.8</span>,</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        max_val_acc <span class="op">*</span> <span class="fl">1.01</span>,</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Max Val Acc: </span><span class="sc">{</span>max_val_acc<span class="sc">:.4f}</span><span class="ss">&quot;</span>,</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loss subplot</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, loss, <span class="st">&quot;b-o&quot;</span>, label<span class="op">=</span><span class="st">&quot;Training Loss&quot;</span>)</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    plt.plot(epochs, val_loss, <span class="st">&quot;g-x&quot;</span>, label<span class="op">=</span><span class="st">&quot;Validation Loss&quot;</span>)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Training and Validation Loss&quot;</span>)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Epoch&quot;</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Loss&quot;</span>)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic x-axis ticks for loss plot</span></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    plt.xticks(epochs[::tick_spacing])</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>    min_val_loss <span class="op">=</span> <span class="bu">min</span>(val_loss)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    plt.text(</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">len</span>(epochs) <span class="op">*</span> <span class="fl">0.8</span>,</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        min_val_loss <span class="op">*</span> <span class="fl">1.01</span>,</span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f&quot;Min Val Loss: </span><span class="sc">{</span>min_val_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>,</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>allResults <span class="op">=</span> pd.DataFrame()</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> storeResult(modelInfo, description<span class="op">=</span><span class="st">&quot;No augmentation or regularization&quot;</span>):</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> modelInfo.history</span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>    best_val_idx <span class="op">=</span> np.argmax(history[<span class="st">&quot;val_accuracy&quot;</span>])</span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> {}</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Model Name&quot;</span>] <span class="op">=</span> modelInfo.model._name</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Description&quot;</span>] <span class="op">=</span> description</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Epochs&quot;</span>] <span class="op">=</span> <span class="bu">len</span>(history[<span class="st">&quot;loss&quot;</span>])</span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Batch Size&quot;</span>] <span class="op">=</span> BATCH_SIZE</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Train Loss&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;loss&quot;</span>][best_val_idx]</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Val Loss&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;val_loss&quot;</span>][best_val_idx]</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Train Acc&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;accuracy&quot;</span>][best_val_idx]</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;Val Acc&quot;</span>] <span class="op">=</span> history[<span class="st">&quot;val_accuracy&quot;</span>][best_val_idx]</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">&quot;[Train - Val] Acc&quot;</span>] <span class="op">=</span> result[<span class="st">&quot;Train Acc&quot;</span>] <span class="op">-</span> result[<span class="st">&quot;Val Acc&quot;</span>]</span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> pd.DataFrame([result])</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> allResults</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>    allResults <span class="op">=</span> pd.concat([allResults, result], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_model(model):</span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model:</span>
<span id="cb25-80"><a href="#cb25-80" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> model</span>
<span id="cb25-81"><a href="#cb25-81" aria-hidden="true" tabindex="-1"></a>    K.clear_session()</span>
<span id="cb25-82"><a href="#cb25-82" aria-hidden="true" tabindex="-1"></a>    tf.compat.v1.reset_default_graph()</span></code></pre></div>
</div>
<div class="cell markdown">
<p>Number of unique classes</p>
</div>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting constants</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>NUM_CLASSES <span class="op">=</span> <span class="bu">len</span>(train_gen.class_indices)</span></code></pre></div>
</div>
<section id="base-model" class="cell markdown">
<h3><strong>BASE MODEL</strong></h3>
<blockquote>
<p>A simple control to compare with our robust models</p>
</blockquote>
</section>
<div class="cell code" data-execution_count="46">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_base_model(input_shape, name<span class="op">=</span><span class="st">&quot;base_model&quot;</span>):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    base_model <span class="op">=</span> Sequential(name<span class="op">=</span>name)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    base_model.add(Conv2D(<span class="dv">32</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>, input_shape<span class="op">=</span>(<span class="op">*</span>input_shape, <span class="dv">1</span>)))</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    base_model.add(MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>)))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    base_model.add(Conv2D(<span class="dv">64</span>, (<span class="dv">3</span>, <span class="dv">3</span>), activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    base_model.add(MaxPooling2D((<span class="dv">2</span>, <span class="dv">2</span>)))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    base_model.add(Flatten())</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    base_model.add(Dense(NUM_CLASSES, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    base_model.<span class="bu">compile</span>(<span class="st">&quot;sgd&quot;</span>, loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>])</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> base_model</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(DEFAULT_SHAPE)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>base_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;base_model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 222, 222, 32)      320       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 186624)            0         
                                                                 
 dense (Dense)               (None, 15)                2799375   
                                                                 
=================================================================
Total params: 2,818,191
Trainable params: 2,818,191
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<section id="base-model-128x128-oversampled" class="cell markdown">
<h4><strong>BASE MODEL (128x128) (OVERSAMPLED)</strong></h4>
</section>
<div class="cell code" data-execution_count="47">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>train_gen_oversampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    train_data_oversampled,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_oversampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_128)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>base_model_oversampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    train_gen_oversampled,</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_oversampled_history, description<span class="op">=</span><span class="st">&quot;128x128 oversampled&quot;</span>)</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_oversampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 14325 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
447/447 [==============================] - 144s 267ms/step - loss: 2.5019 - accuracy: 0.2005 - val_loss: 2.1302 - val_accuracy: 0.3427 - lr: 0.1000
Epoch 2/100
447/447 [==============================] - 27s 61ms/step - loss: 1.8774 - accuracy: 0.4379 - val_loss: 1.7165 - val_accuracy: 0.4916 - lr: 0.1000
Epoch 3/100
447/447 [==============================] - 29s 64ms/step - loss: 1.1838 - accuracy: 0.6551 - val_loss: 1.5867 - val_accuracy: 0.5696 - lr: 0.1000
Epoch 4/100
447/447 [==============================] - 27s 61ms/step - loss: 0.6095 - accuracy: 0.8213 - val_loss: 1.6378 - val_accuracy: 0.5914 - lr: 0.1000
Epoch 5/100
447/447 [==============================] - 27s 61ms/step - loss: 0.2498 - accuracy: 0.9295 - val_loss: 1.8355 - val_accuracy: 0.6156 - lr: 0.1000
Epoch 6/100
447/447 [==============================] - 27s 61ms/step - loss: 0.0993 - accuracy: 0.9744 - val_loss: 2.1728 - val_accuracy: 0.5917 - lr: 0.1000
Epoch 7/100
447/447 [==============================] - 27s 61ms/step - loss: 0.0899 - accuracy: 0.9812 - val_loss: 2.2111 - val_accuracy: 0.5914 - lr: 0.1000
Epoch 8/100
447/447 [==============================] - 27s 60ms/step - loss: 0.0398 - accuracy: 0.9915 - val_loss: 2.3798 - val_accuracy: 0.6226 - lr: 0.1000
Epoch 9/100
447/447 [==============================] - 27s 61ms/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 2.4863 - val_accuracy: 0.6284 - lr: 0.1000
Epoch 10/100
447/447 [==============================] - 28s 62ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 2.6137 - val_accuracy: 0.6294 - lr: 0.0500
Epoch 11/100
447/447 [==============================] - 27s 60ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 2.7179 - val_accuracy: 0.6270 - lr: 0.0500
Epoch 12/100
447/447 [==============================] - 27s 60ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.7618 - val_accuracy: 0.6300 - lr: 0.0500
Epoch 13/100
447/447 [==============================] - 27s 61ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8362 - val_accuracy: 0.6280 - lr: 0.0500
Epoch 14/100
447/447 [==============================] - 27s 60ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.8914 - val_accuracy: 0.6270 - lr: 0.0500
Epoch 15/100
447/447 [==============================] - 27s 61ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9199 - val_accuracy: 0.6277 - lr: 0.0250
Epoch 16/100
447/447 [==============================] - 28s 62ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.9837 - val_accuracy: 0.6280 - lr: 0.0500
Epoch 17/100
447/447 [==============================] - 27s 61ms/step - loss: 9.4875e-04 - accuracy: 1.0000 - val_loss: 3.0033 - val_accuracy: 0.6284 - lr: 0.0500
Epoch 18/100
447/447 [==============================] - 27s 61ms/step - loss: 8.5959e-04 - accuracy: 1.0000 - val_loss: 2.9948 - val_accuracy: 0.6310 - lr: 0.0500
Epoch 19/100
447/447 [==============================] - 28s 62ms/step - loss: 7.9299e-04 - accuracy: 1.0000 - val_loss: 3.0474 - val_accuracy: 0.6294 - lr: 0.0500
Epoch 20/100
447/447 [==============================] - 26s 59ms/step - loss: 7.3183e-04 - accuracy: 1.0000 - val_loss: 3.0409 - val_accuracy: 0.6290 - lr: 0.0250
Epoch 21/100
447/447 [==============================] - 27s 61ms/step - loss: 7.0390e-04 - accuracy: 1.0000 - val_loss: 3.0437 - val_accuracy: 0.6307 - lr: 0.0125
Epoch 22/100
447/447 [==============================] - 28s 63ms/step - loss: 6.8183e-04 - accuracy: 1.0000 - val_loss: 3.0791 - val_accuracy: 0.6300 - lr: 0.0250
Epoch 23/100
447/447 [==============================] - 29s 64ms/step - loss: 6.5827e-04 - accuracy: 1.0000 - val_loss: 3.0918 - val_accuracy: 0.6294 - lr: 0.0250
Epoch 24/100
447/447 [==============================] - 27s 61ms/step - loss: 6.3561e-04 - accuracy: 1.0000 - val_loss: 3.0936 - val_accuracy: 0.6300 - lr: 0.0125
Epoch 25/100
447/447 [==============================] - 27s 61ms/step - loss: 6.1668e-04 - accuracy: 1.0000 - val_loss: 3.0982 - val_accuracy: 0.6287 - lr: 0.0250
Epoch 26/100
447/447 [==============================] - 27s 60ms/step - loss: 5.9834e-04 - accuracy: 1.0000 - val_loss: 3.1182 - val_accuracy: 0.6297 - lr: 0.0250
Epoch 27/100
447/447 [==============================] - 28s 62ms/step - loss: 5.7891e-04 - accuracy: 1.0000 - val_loss: 3.1259 - val_accuracy: 0.6297 - lr: 0.0125
Epoch 28/100
447/447 [==============================] - 27s 60ms/step - loss: 5.6304e-04 - accuracy: 1.0000 - val_loss: 3.1166 - val_accuracy: 0.6290 - lr: 0.0250
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/7c9db2ad667730a469cbe6c43535152f704caacd.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>There might be a need for regularization techniques, data
augmentation, or a review of the model's complexity to address
overfitting.</li>
<li>The peak validation accuracy is decent but might need improvement,
which could involve tuning hyperparameters, collecting more data, or
trying different model architectures.</li>
<li>Validation loss increasing after just 3 epochs shows the model
memorizing training data, not generalizing to validation data.</li>
</ol>
</div>
<section id="base-model-31x31-oversampled" class="cell markdown">
<h4><strong>BASE MODEL (31x31) (OVERSAMPLED)</strong></h4>
</section>
<div class="cell code" data-execution_count="48">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>train_gen_oversampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    train_data_oversampled,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_oversampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_31)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>base_model_oversampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    train_gen_oversampled,</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_oversampled_history, description<span class="op">=</span><span class="st">&quot;31x31 oversampled&quot;</span>)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_oversampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 14325 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
447/447 [==============================] - 26s 58ms/step - loss: 2.5895 - accuracy: 0.1506 - val_loss: 2.2788 - val_accuracy: 0.2826 - lr: 0.1000
Epoch 2/100
447/447 [==============================] - 25s 57ms/step - loss: 2.1993 - accuracy: 0.3168 - val_loss: 1.8952 - val_accuracy: 0.4298 - lr: 0.1000
Epoch 3/100
447/447 [==============================] - 25s 57ms/step - loss: 1.8748 - accuracy: 0.4193 - val_loss: 1.6358 - val_accuracy: 0.5074 - lr: 0.1000
Epoch 4/100
447/447 [==============================] - 26s 57ms/step - loss: 1.5830 - accuracy: 0.5234 - val_loss: 1.3653 - val_accuracy: 0.5877 - lr: 0.1000
Epoch 5/100
447/447 [==============================] - 26s 58ms/step - loss: 1.3264 - accuracy: 0.5983 - val_loss: 1.2950 - val_accuracy: 0.6142 - lr: 0.1000
Epoch 6/100
447/447 [==============================] - 26s 58ms/step - loss: 1.1228 - accuracy: 0.6586 - val_loss: 1.1457 - val_accuracy: 0.6549 - lr: 0.1000
Epoch 7/100
447/447 [==============================] - 25s 56ms/step - loss: 0.9476 - accuracy: 0.7128 - val_loss: 1.1657 - val_accuracy: 0.6603 - lr: 0.1000
Epoch 8/100
447/447 [==============================] - 26s 59ms/step - loss: 0.8091 - accuracy: 0.7546 - val_loss: 1.0402 - val_accuracy: 0.7033 - lr: 0.1000
Epoch 9/100
447/447 [==============================] - 29s 65ms/step - loss: 0.6681 - accuracy: 0.7978 - val_loss: 0.9769 - val_accuracy: 0.7177 - lr: 0.1000
Epoch 10/100
447/447 [==============================] - 25s 56ms/step - loss: 0.4415 - accuracy: 0.8727 - val_loss: 0.9603 - val_accuracy: 0.7261 - lr: 0.0500
Epoch 11/100
447/447 [==============================] - 26s 58ms/step - loss: 0.3766 - accuracy: 0.8923 - val_loss: 0.9293 - val_accuracy: 0.7416 - lr: 0.0500
Epoch 12/100
447/447 [==============================] - 25s 57ms/step - loss: 0.3256 - accuracy: 0.9099 - val_loss: 1.0002 - val_accuracy: 0.7278 - lr: 0.0500
Epoch 13/100
447/447 [==============================] - 26s 57ms/step - loss: 0.2875 - accuracy: 0.9204 - val_loss: 1.0238 - val_accuracy: 0.7211 - lr: 0.0500
Epoch 14/100
447/447 [==============================] - 25s 57ms/step - loss: 0.2463 - accuracy: 0.9346 - val_loss: 0.9969 - val_accuracy: 0.7339 - lr: 0.0250
Epoch 15/100
447/447 [==============================] - 26s 58ms/step - loss: 0.2067 - accuracy: 0.9488 - val_loss: 1.0598 - val_accuracy: 0.7278 - lr: 0.0500
Epoch 16/100
447/447 [==============================] - 26s 58ms/step - loss: 0.1805 - accuracy: 0.9561 - val_loss: 1.0919 - val_accuracy: 0.7238 - lr: 0.0500
Epoch 17/100
447/447 [==============================] - 25s 57ms/step - loss: 0.1518 - accuracy: 0.9665 - val_loss: 1.1237 - val_accuracy: 0.7174 - lr: 0.0250
Epoch 18/100
447/447 [==============================] - 25s 57ms/step - loss: 0.1266 - accuracy: 0.9723 - val_loss: 1.1188 - val_accuracy: 0.7255 - lr: 0.0500
Epoch 19/100
447/447 [==============================] - 26s 57ms/step - loss: 0.1057 - accuracy: 0.9805 - val_loss: 1.1567 - val_accuracy: 0.7238 - lr: 0.0500
Epoch 20/100
447/447 [==============================] - 26s 58ms/step - loss: 0.0693 - accuracy: 0.9925 - val_loss: 1.1367 - val_accuracy: 0.7345 - lr: 0.0125
Epoch 21/100
447/447 [==============================] - 26s 58ms/step - loss: 0.0595 - accuracy: 0.9951 - val_loss: 1.1467 - val_accuracy: 0.7372 - lr: 0.0250
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/c94178d3094e452f618ecaa8a3856f14ef9dd9ef.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>There might be a need for regularization techniques, data
augmentation, or a review of the model's complexity to address
overfitting.</li>
<li>The peak validation accuracy is decent but might need improvement,
which could involve tuning hyperparameters, collecting more data, or
trying different model architectures.</li>
<li>Compared to 128x128, this smaller image size performed way better on
this model. This is likely because this small CNN model was able to
generalize to the small images compared to the bigger 128x128
images.</li>
</ol>
</div>
<section id="base-model-128x128-undersampled" class="cell markdown">
<h4><strong>BASE MODEL (128x128) (UNDERSAMPLED)</strong></h4>
</section>
<div class="cell code" data-execution_count="49">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>train_gen_undersampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    train_data_undersampled,</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_undersampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_128)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>base_model_undersampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    train_gen_undersampled,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_undersampled_history, description<span class="op">=</span><span class="st">&quot;128x128 undersampled&quot;</span>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_undersampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3720 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
116/116 [==============================] - 142s 1s/step - loss: 2.6739 - accuracy: 0.1106 - val_loss: 2.6478 - val_accuracy: 0.0662 - lr: 0.1000
Epoch 2/100
116/116 [==============================] - 140s 1s/step - loss: 2.4049 - accuracy: 0.2343 - val_loss: 2.5279 - val_accuracy: 0.1966 - lr: 0.1000
Epoch 3/100
116/116 [==============================] - 138s 1s/step - loss: 2.1207 - accuracy: 0.3587 - val_loss: 2.0560 - val_accuracy: 0.3760 - lr: 0.1000
Epoch 4/100
116/116 [==============================] - 131s 1s/step - loss: 1.5675 - accuracy: 0.5247 - val_loss: 1.8744 - val_accuracy: 0.4422 - lr: 0.1000
Epoch 5/100
116/116 [==============================] - 134s 1s/step - loss: 0.9819 - accuracy: 0.7099 - val_loss: 1.7033 - val_accuracy: 0.5208 - lr: 0.1000
Epoch 6/100
116/116 [==============================] - 138s 1s/step - loss: 0.4924 - accuracy: 0.8525 - val_loss: 1.9828 - val_accuracy: 0.4879 - lr: 0.1000
Epoch 7/100
116/116 [==============================] - 129s 1s/step - loss: 0.2402 - accuracy: 0.9417 - val_loss: 2.7811 - val_accuracy: 0.4879 - lr: 0.1000
Epoch 8/100
116/116 [==============================] - 138s 1s/step - loss: 0.0974 - accuracy: 0.9797 - val_loss: 2.7735 - val_accuracy: 0.5118 - lr: 0.0500
Epoch 9/100
116/116 [==============================] - 132s 1s/step - loss: 0.0645 - accuracy: 0.9924 - val_loss: 2.9633 - val_accuracy: 0.5077 - lr: 0.1000
Epoch 10/100
116/116 [==============================] - 122s 1s/step - loss: 0.0242 - accuracy: 0.9981 - val_loss: 3.2167 - val_accuracy: 0.5151 - lr: 0.0500
Epoch 11/100
116/116 [==============================] - 136s 1s/step - loss: 0.0137 - accuracy: 0.9989 - val_loss: 3.3234 - val_accuracy: 0.5175 - lr: 0.0250
Epoch 12/100
116/116 [==============================] - 43s 361ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 3.4754 - val_accuracy: 0.5144 - lr: 0.0500
Epoch 13/100
116/116 [==============================] - 102s 880ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 3.5679 - val_accuracy: 0.5138 - lr: 0.0500
Epoch 14/100
116/116 [==============================] - 89s 772ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 3.6530 - val_accuracy: 0.5131 - lr: 0.0250
Epoch 15/100
116/116 [==============================] - 129s 1s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.7499 - val_accuracy: 0.5094 - lr: 0.0500
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/ac935142f2e3c0e66ad6a4f4c16126c1a86a7844.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>There might be a need for regularization techniques, data
augmentation, or a review of the model's complexity to address
overfitting.</li>
<li>The peak validation accuracy is fair, it needs improvement, which
could involve tuning hyperparameters, or more likely trying different
model architectures.</li>
<li>Validation loss increasing after just 5 epochs shows the model
memorizing training data, not generalizing to validation data.</li>
</ol>
</div>
<section id="base-model-31x31-undersampled" class="cell markdown">
<h4><strong>BASE MODEL (31x31) (UNDERSAMPLED)</strong></h4>
</section>
<div class="cell code" data-execution_count="50">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>train_gen_undersampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    train_data_undersampled,</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_undersampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_31)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>base_model_undersampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    train_gen_undersampled,</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_undersampled_history, description<span class="op">=</span><span class="st">&quot;31x31 undersampled&quot;</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_undersampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3720 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
116/116 [==============================] - 9s 74ms/step - loss: 2.6949 - accuracy: 0.0865 - val_loss: 2.6594 - val_accuracy: 0.1072 - lr: 0.1000
Epoch 2/100
116/116 [==============================] - 8s 71ms/step - loss: 2.5681 - accuracy: 0.1708 - val_loss: 2.6360 - val_accuracy: 0.0998 - lr: 0.1000
Epoch 3/100
116/116 [==============================] - 8s 72ms/step - loss: 2.3743 - accuracy: 0.2454 - val_loss: 2.3185 - val_accuracy: 0.2423 - lr: 0.1000
Epoch 4/100
116/116 [==============================] - 9s 74ms/step - loss: 2.1565 - accuracy: 0.3294 - val_loss: 2.0205 - val_accuracy: 0.3757 - lr: 0.1000
Epoch 5/100
116/116 [==============================] - 8s 71ms/step - loss: 2.0109 - accuracy: 0.3799 - val_loss: 1.9562 - val_accuracy: 0.4022 - lr: 0.1000
Epoch 6/100
116/116 [==============================] - 8s 72ms/step - loss: 1.8094 - accuracy: 0.4488 - val_loss: 1.7204 - val_accuracy: 0.4829 - lr: 0.1000
Epoch 7/100
116/116 [==============================] - 8s 72ms/step - loss: 1.6225 - accuracy: 0.5065 - val_loss: 1.7540 - val_accuracy: 0.4792 - lr: 0.1000
Epoch 8/100
116/116 [==============================] - 8s 72ms/step - loss: 1.4741 - accuracy: 0.5537 - val_loss: 1.4994 - val_accuracy: 0.5440 - lr: 0.1000
Epoch 9/100
116/116 [==============================] - 8s 73ms/step - loss: 1.3337 - accuracy: 0.5884 - val_loss: 1.4235 - val_accuracy: 0.5632 - lr: 0.1000
Epoch 10/100
116/116 [==============================] - 8s 72ms/step - loss: 1.0206 - accuracy: 0.7088 - val_loss: 1.3479 - val_accuracy: 0.5890 - lr: 0.0500
Epoch 11/100
116/116 [==============================] - 9s 76ms/step - loss: 0.9571 - accuracy: 0.7226 - val_loss: 1.3345 - val_accuracy: 0.5924 - lr: 0.0500
Epoch 12/100
116/116 [==============================] - 8s 73ms/step - loss: 0.8831 - accuracy: 0.7435 - val_loss: 1.2939 - val_accuracy: 0.6176 - lr: 0.0500
Epoch 13/100
116/116 [==============================] - 9s 75ms/step - loss: 0.8289 - accuracy: 0.7527 - val_loss: 1.2810 - val_accuracy: 0.6213 - lr: 0.0500
Epoch 14/100
116/116 [==============================] - 8s 73ms/step - loss: 0.7861 - accuracy: 0.7787 - val_loss: 1.2764 - val_accuracy: 0.6243 - lr: 0.0500
Epoch 15/100
116/116 [==============================] - 8s 73ms/step - loss: 0.7068 - accuracy: 0.8037 - val_loss: 1.2538 - val_accuracy: 0.6300 - lr: 0.0500
Epoch 16/100
116/116 [==============================] - 9s 74ms/step - loss: 0.6565 - accuracy: 0.8162 - val_loss: 1.2601 - val_accuracy: 0.6378 - lr: 0.0500
Epoch 17/100
116/116 [==============================] - 9s 74ms/step - loss: 0.6056 - accuracy: 0.8294 - val_loss: 1.2588 - val_accuracy: 0.6425 - lr: 0.0500
Epoch 18/100
116/116 [==============================] - 9s 74ms/step - loss: 0.5747 - accuracy: 0.8384 - val_loss: 2.5945 - val_accuracy: 0.3989 - lr: 0.0500
Epoch 19/100
116/116 [==============================] - 9s 74ms/step - loss: 0.5322 - accuracy: 0.8530 - val_loss: 1.2073 - val_accuracy: 0.6586 - lr: 0.0500
Epoch 20/100
116/116 [==============================] - 9s 75ms/step - loss: 0.3908 - accuracy: 0.9078 - val_loss: 1.1860 - val_accuracy: 0.6650 - lr: 0.0250
Epoch 21/100
116/116 [==============================] - 9s 75ms/step - loss: 0.3703 - accuracy: 0.9102 - val_loss: 1.1866 - val_accuracy: 0.6700 - lr: 0.0250
Epoch 22/100
116/116 [==============================] - 9s 75ms/step - loss: 0.3422 - accuracy: 0.9195 - val_loss: 1.1841 - val_accuracy: 0.6690 - lr: 0.0250
Epoch 23/100
116/116 [==============================] - 9s 74ms/step - loss: 0.3225 - accuracy: 0.9243 - val_loss: 1.1946 - val_accuracy: 0.6724 - lr: 0.0250
Epoch 24/100
116/116 [==============================] - 9s 74ms/step - loss: 0.3002 - accuracy: 0.9325 - val_loss: 1.2250 - val_accuracy: 0.6673 - lr: 0.0250
Epoch 25/100
116/116 [==============================] - 8s 72ms/step - loss: 0.2905 - accuracy: 0.9357 - val_loss: 1.2292 - val_accuracy: 0.6747 - lr: 0.0250
Epoch 26/100
116/116 [==============================] - 9s 74ms/step - loss: 0.2705 - accuracy: 0.9431 - val_loss: 1.2229 - val_accuracy: 0.6697 - lr: 0.0250
Epoch 27/100
116/116 [==============================] - 9s 77ms/step - loss: 0.2572 - accuracy: 0.9477 - val_loss: 1.2911 - val_accuracy: 0.6475 - lr: 0.0250
Epoch 28/100
116/116 [==============================] - 9s 74ms/step - loss: 0.2353 - accuracy: 0.9482 - val_loss: 1.2438 - val_accuracy: 0.6798 - lr: 0.0250
Epoch 29/100
116/116 [==============================] - 8s 71ms/step - loss: 0.2244 - accuracy: 0.9528 - val_loss: 1.2530 - val_accuracy: 0.6731 - lr: 0.0250
Epoch 30/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1894 - accuracy: 0.9656 - val_loss: 1.2371 - val_accuracy: 0.6801 - lr: 0.0125
Epoch 31/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1792 - accuracy: 0.9704 - val_loss: 1.2366 - val_accuracy: 0.6811 - lr: 0.0125
Epoch 32/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1728 - accuracy: 0.9715 - val_loss: 1.2434 - val_accuracy: 0.6757 - lr: 0.0125
Epoch 33/100
116/116 [==============================] - 9s 75ms/step - loss: 0.1693 - accuracy: 0.9726 - val_loss: 1.2541 - val_accuracy: 0.6791 - lr: 0.0125
Epoch 34/100
116/116 [==============================] - 9s 74ms/step - loss: 0.1634 - accuracy: 0.9737 - val_loss: 1.2536 - val_accuracy: 0.6818 - lr: 0.0125
Epoch 35/100
116/116 [==============================] - 9s 73ms/step - loss: 0.1548 - accuracy: 0.9772 - val_loss: 1.2572 - val_accuracy: 0.6794 - lr: 0.0125
Epoch 36/100
116/116 [==============================] - 8s 73ms/step - loss: 0.1497 - accuracy: 0.9770 - val_loss: 1.2716 - val_accuracy: 0.6778 - lr: 0.0125
Epoch 37/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1464 - accuracy: 0.9778 - val_loss: 1.2724 - val_accuracy: 0.6865 - lr: 0.0125
Epoch 38/100
116/116 [==============================] - 9s 73ms/step - loss: 0.1409 - accuracy: 0.9799 - val_loss: 1.2731 - val_accuracy: 0.6804 - lr: 0.0125
Epoch 39/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1360 - accuracy: 0.9802 - val_loss: 1.2774 - val_accuracy: 0.6851 - lr: 0.0125
Epoch 40/100
116/116 [==============================] - 8s 71ms/step - loss: 0.1238 - accuracy: 0.9848 - val_loss: 1.2814 - val_accuracy: 0.6808 - lr: 0.0031
Epoch 41/100
116/116 [==============================] - 9s 74ms/step - loss: 0.1207 - accuracy: 0.9859 - val_loss: 1.2925 - val_accuracy: 0.6811 - lr: 0.0063
Epoch 42/100
116/116 [==============================] - 9s 73ms/step - loss: 0.1202 - accuracy: 0.9864 - val_loss: 1.2955 - val_accuracy: 0.6815 - lr: 0.0063
Epoch 43/100
116/116 [==============================] - 8s 73ms/step - loss: 0.1179 - accuracy: 0.9856 - val_loss: 1.3103 - val_accuracy: 0.6737 - lr: 0.0031
Epoch 44/100
116/116 [==============================] - 9s 74ms/step - loss: 0.1161 - accuracy: 0.9873 - val_loss: 1.3062 - val_accuracy: 0.6801 - lr: 0.0063
Epoch 45/100
116/116 [==============================] - 8s 71ms/step - loss: 0.1136 - accuracy: 0.9864 - val_loss: 1.3073 - val_accuracy: 0.6811 - lr: 0.0063
Epoch 46/100
116/116 [==============================] - 8s 72ms/step - loss: 0.1110 - accuracy: 0.9878 - val_loss: 1.3184 - val_accuracy: 0.6737 - lr: 0.0031
Epoch 47/100
116/116 [==============================] - 8s 73ms/step - loss: 0.1097 - accuracy: 0.9878 - val_loss: 1.3130 - val_accuracy: 0.6828 - lr: 0.0063
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/428139aba451b0f74d44e9bbce1bcb076ed632b5.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>There might be a need for regularization techniques, data
augmentation, or a review of the model's complexity to address
overfitting.</li>
<li>There is an spike at 17 to 19 epochs</li>
<li>Compared to oversampling, validation accuracy is lower, and loss is
higher. It seems the decrease in image data made it more difficult for
the model to generalize.</li>
</ol>
</div>
<section id="base-model-128x128-imbalanced" class="cell markdown">
<h4><strong>BASE MODEL (128x128) (IMBALANCED)</strong></h4>
</section>
<div class="cell code" data-execution_count="51">
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>train_gen_undersampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_undersampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_128)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>base_model_undersampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>    train_gen_undersampled,</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_undersampled_history, description<span class="op">=</span><span class="st">&quot;128x128 imbalanced&quot;</span>)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_undersampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 52s 172ms/step - loss: 2.5543 - accuracy: 0.1723 - val_loss: 2.3454 - val_accuracy: 0.2722 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 12s 41ms/step - loss: 2.1924 - accuracy: 0.3446 - val_loss: 2.3360 - val_accuracy: 0.2765 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 11s 40ms/step - loss: 1.7967 - accuracy: 0.4689 - val_loss: 1.7303 - val_accuracy: 0.4862 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 11s 40ms/step - loss: 1.4336 - accuracy: 0.5646 - val_loss: 1.8410 - val_accuracy: 0.4516 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 11s 40ms/step - loss: 1.0297 - accuracy: 0.6906 - val_loss: 1.7840 - val_accuracy: 0.5017 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 11s 39ms/step - loss: 0.6326 - accuracy: 0.8096 - val_loss: 2.0337 - val_accuracy: 0.5433 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 11s 39ms/step - loss: 0.3789 - accuracy: 0.8853 - val_loss: 2.1989 - val_accuracy: 0.5538 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 12s 42ms/step - loss: 0.2683 - accuracy: 0.9237 - val_loss: 2.4294 - val_accuracy: 0.5538 - lr: 0.1000
Epoch 9/100
282/282 [==============================] - 12s 41ms/step - loss: 0.1875 - accuracy: 0.9492 - val_loss: 2.7530 - val_accuracy: 0.5773 - lr: 0.1000
Epoch 10/100
282/282 [==============================] - 12s 43ms/step - loss: 0.0744 - accuracy: 0.9828 - val_loss: 2.9670 - val_accuracy: 0.5897 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0399 - accuracy: 0.9928 - val_loss: 3.1875 - val_accuracy: 0.5911 - lr: 0.0500
Epoch 12/100
282/282 [==============================] - 12s 41ms/step - loss: 0.0270 - accuracy: 0.9958 - val_loss: 3.4483 - val_accuracy: 0.5931 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0195 - accuracy: 0.9971 - val_loss: 3.6074 - val_accuracy: 0.5927 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0141 - accuracy: 0.9981 - val_loss: 3.7527 - val_accuracy: 0.5904 - lr: 0.0500
Epoch 15/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0106 - accuracy: 0.9990 - val_loss: 3.8619 - val_accuracy: 0.5924 - lr: 0.0250
Epoch 16/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0081 - accuracy: 0.9994 - val_loss: 3.9982 - val_accuracy: 0.5961 - lr: 0.0500
Epoch 17/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0063 - accuracy: 0.9994 - val_loss: 4.0801 - val_accuracy: 0.5924 - lr: 0.0500
Epoch 18/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0051 - accuracy: 0.9997 - val_loss: 4.1791 - val_accuracy: 0.5911 - lr: 0.0500
Epoch 19/100
282/282 [==============================] - 11s 39ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 4.2602 - val_accuracy: 0.5917 - lr: 0.0250
Epoch 20/100
282/282 [==============================] - 11s 39ms/step - loss: 0.0034 - accuracy: 0.9999 - val_loss: 4.2746 - val_accuracy: 0.5931 - lr: 0.0250
Epoch 21/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 4.3112 - val_accuracy: 0.5934 - lr: 0.0250
Epoch 22/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 4.3220 - val_accuracy: 0.5951 - lr: 0.0125
Epoch 23/100
282/282 [==============================] - 12s 41ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 4.3656 - val_accuracy: 0.5938 - lr: 0.0250
Epoch 24/100
282/282 [==============================] - 12s 41ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.3756 - val_accuracy: 0.5948 - lr: 0.0250
Epoch 25/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.4063 - val_accuracy: 0.5927 - lr: 0.0125
Epoch 26/100
282/282 [==============================] - 13s 45ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 4.4528 - val_accuracy: 0.5890 - lr: 0.0250
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/113c8e8f688ae9311811d5a2e56dee12b483c251.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>Validation accuracy is low, and validation loss in is increasing.
This has been a consistent result for 128x128 images for all different
sampling techniques trained on the base model. There is most definitely
an architecture change that has to be made.</li>
</ol>
</div>
<section id="base-model-31x31-imbalanced" class="cell markdown">
<h4><strong>BASE MODEL (31x31) (IMBALANCED)</strong></h4>
</section>
<div class="cell code" data-execution_count="52">
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>train_gen_undersampled <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen_undersampled.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_31)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>base_model_undersampled_history <span class="op">=</span> base_model.fit(</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    train_gen_undersampled,</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stop, reduce_lr, lr_scheduler, tensorboard],</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_undersampled_history, description<span class="op">=</span><span class="st">&quot;31x31 imbalanced&quot;</span>)</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_undersampled_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 12s 42ms/step - loss: 2.5194 - accuracy: 0.1735 - val_loss: 2.3781 - val_accuracy: 0.2463 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 11s 39ms/step - loss: 2.1200 - accuracy: 0.3494 - val_loss: 2.0355 - val_accuracy: 0.3656 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 11s 38ms/step - loss: 1.7636 - accuracy: 0.4553 - val_loss: 1.6999 - val_accuracy: 0.4590 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 10s 37ms/step - loss: 1.5005 - accuracy: 0.5355 - val_loss: 1.5691 - val_accuracy: 0.5296 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 10s 36ms/step - loss: 1.2925 - accuracy: 0.6066 - val_loss: 1.3196 - val_accuracy: 0.5988 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 10s 36ms/step - loss: 1.1747 - accuracy: 0.6412 - val_loss: 1.3051 - val_accuracy: 0.6062 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 11s 38ms/step - loss: 0.9756 - accuracy: 0.6982 - val_loss: 1.0839 - val_accuracy: 0.6683 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 10s 36ms/step - loss: 0.8272 - accuracy: 0.7481 - val_loss: 1.0113 - val_accuracy: 0.6989 - lr: 0.1000
Epoch 9/100
282/282 [==============================] - 10s 36ms/step - loss: 0.7034 - accuracy: 0.7873 - val_loss: 1.0233 - val_accuracy: 0.6956 - lr: 0.1000
Epoch 10/100
282/282 [==============================] - 10s 36ms/step - loss: 0.4743 - accuracy: 0.8649 - val_loss: 0.9894 - val_accuracy: 0.7107 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 10s 36ms/step - loss: 0.4089 - accuracy: 0.8853 - val_loss: 0.8595 - val_accuracy: 0.7567 - lr: 0.0500
Epoch 12/100
282/282 [==============================] - 11s 37ms/step - loss: 0.3654 - accuracy: 0.8954 - val_loss: 0.9487 - val_accuracy: 0.7308 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 11s 37ms/step - loss: 0.3246 - accuracy: 0.9112 - val_loss: 0.9183 - val_accuracy: 0.7406 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 10s 37ms/step - loss: 0.2912 - accuracy: 0.9206 - val_loss: 0.8754 - val_accuracy: 0.7513 - lr: 0.0250
Epoch 15/100
282/282 [==============================] - 11s 37ms/step - loss: 0.2484 - accuracy: 0.9361 - val_loss: 0.9330 - val_accuracy: 0.7389 - lr: 0.0500
Epoch 16/100
282/282 [==============================] - 11s 40ms/step - loss: 0.2170 - accuracy: 0.9445 - val_loss: 0.8521 - val_accuracy: 0.7631 - lr: 0.0500
Epoch 17/100
282/282 [==============================] - 10s 37ms/step - loss: 0.1949 - accuracy: 0.9500 - val_loss: 0.8791 - val_accuracy: 0.7611 - lr: 0.0500
Epoch 18/100
282/282 [==============================] - 10s 37ms/step - loss: 0.1642 - accuracy: 0.9630 - val_loss: 0.8813 - val_accuracy: 0.7661 - lr: 0.0500
Epoch 19/100
282/282 [==============================] - 10s 36ms/step - loss: 0.1799 - accuracy: 0.9590 - val_loss: 0.8523 - val_accuracy: 0.7769 - lr: 0.0500
Epoch 20/100
282/282 [==============================] - 10s 36ms/step - loss: 0.1044 - accuracy: 0.9829 - val_loss: 0.8688 - val_accuracy: 0.7799 - lr: 0.0250
Epoch 21/100
282/282 [==============================] - 10s 36ms/step - loss: 0.0914 - accuracy: 0.9861 - val_loss: 0.8843 - val_accuracy: 0.7809 - lr: 0.0250
Epoch 22/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0830 - accuracy: 0.9882 - val_loss: 0.9296 - val_accuracy: 0.7725 - lr: 0.0250
Epoch 23/100
282/282 [==============================] - 11s 38ms/step - loss: 0.0760 - accuracy: 0.9907 - val_loss: 0.9082 - val_accuracy: 0.7816 - lr: 0.0250
Epoch 24/100
282/282 [==============================] - 11s 39ms/step - loss: 0.0702 - accuracy: 0.9929 - val_loss: 0.9262 - val_accuracy: 0.7759 - lr: 0.0250
Epoch 25/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0647 - accuracy: 0.9936 - val_loss: 0.9054 - val_accuracy: 0.7863 - lr: 0.0250
Epoch 26/100
282/282 [==============================] - 10s 36ms/step - loss: 0.0607 - accuracy: 0.9930 - val_loss: 0.9303 - val_accuracy: 0.7796 - lr: 0.0250
Epoch 27/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0560 - accuracy: 0.9944 - val_loss: 0.9426 - val_accuracy: 0.7823 - lr: 0.0250
Epoch 28/100
282/282 [==============================] - 10s 36ms/step - loss: 0.0516 - accuracy: 0.9954 - val_loss: 0.9370 - val_accuracy: 0.7833 - lr: 0.0125
Epoch 29/100
282/282 [==============================] - 10s 36ms/step - loss: 0.0486 - accuracy: 0.9953 - val_loss: 0.9592 - val_accuracy: 0.7802 - lr: 0.0250
Epoch 30/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0414 - accuracy: 0.9978 - val_loss: 0.9615 - val_accuracy: 0.7826 - lr: 0.0125
Epoch 31/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0401 - accuracy: 0.9974 - val_loss: 0.9591 - val_accuracy: 0.7819 - lr: 0.0063
Epoch 32/100
282/282 [==============================] - 10s 37ms/step - loss: 0.0383 - accuracy: 0.9982 - val_loss: 0.9841 - val_accuracy: 0.7799 - lr: 0.0125
Epoch 33/100
282/282 [==============================] - 10s 36ms/step - loss: 0.0371 - accuracy: 0.9980 - val_loss: 0.9717 - val_accuracy: 0.7823 - lr: 0.0125
Epoch 34/100
282/282 [==============================] - 11s 39ms/step - loss: 0.0361 - accuracy: 0.9981 - val_loss: 0.9775 - val_accuracy: 0.7809 - lr: 0.0063
Epoch 35/100
282/282 [==============================] - 11s 40ms/step - loss: 0.0348 - accuracy: 0.9983 - val_loss: 0.9908 - val_accuracy: 0.7789 - lr: 0.0125
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/6bd69abc4bc32fba9662392e536d3582eaf0383c.png" /></p>
</div>
</div>
<div class="cell markdown">
<ol>
<li>There might be a need for regularization techniques, data
augmentation, to address overfitting.</li>
<li>Imbalanced data performed the best for 31x31 images surprisingly.
This demonstrates a neural network's ability to generalize inspite of
imbalanced data</li>
<li>This smaller CNN architecture seems to work for 31x31.</li>
</ol>
</div>
<section id="observations" class="cell markdown">
<h4><strong>Observations</strong></h4>
<p>Notice that the distrubution of labels affected our base models very
little. It seems deep nureal networks can deal with imbalanced data
because the results we got from the imbalanced dataset were the best for
31x31, and there is little difference for 128x128. However, taking a
look at the huge difference between training accuracy and validation
accuracy, we can see an obvious case of overfitting. Let us try data
augmentation, a powerful regularization technique, to solve this
issue.</p>
</section>
<section id="base-model-with-augmentation-128x128"
class="cell markdown">
<h4><strong>BASE MODEL WITH AUGMENTATION (128x128)</strong></h4>
</section>
<div class="cell code" data-execution_count="53">
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_128)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>base_model_augmented_history <span class="op">=</span> base_model.fit(</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a>        tensorboard,</span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_undersampled_history, description<span class="op">=</span><span class="st">&quot;128x128 augmented&quot;</span>)</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_augmented_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 22s 78ms/step - loss: 2.6420 - accuracy: 0.1203 - val_loss: 2.7336 - val_accuracy: 0.1213 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 22s 78ms/step - loss: 2.5266 - accuracy: 0.1784 - val_loss: 2.5756 - val_accuracy: 0.1683 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 23s 82ms/step - loss: 2.4073 - accuracy: 0.2185 - val_loss: 2.5592 - val_accuracy: 0.1626 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 26s 92ms/step - loss: 2.3655 - accuracy: 0.2342 - val_loss: 2.4566 - val_accuracy: 0.2218 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 25s 88ms/step - loss: 2.2939 - accuracy: 0.2541 - val_loss: 2.2534 - val_accuracy: 0.2927 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 22s 77ms/step - loss: 2.2222 - accuracy: 0.2842 - val_loss: 2.1346 - val_accuracy: 0.3236 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 22s 78ms/step - loss: 2.1531 - accuracy: 0.3140 - val_loss: 2.1512 - val_accuracy: 0.2964 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 22s 77ms/step - loss: 2.0484 - accuracy: 0.3469 - val_loss: 2.1614 - val_accuracy: 0.2947 - lr: 0.1000
Epoch 9/100
282/282 [==============================] - 22s 77ms/step - loss: 2.0657 - accuracy: 0.3422 - val_loss: 2.0724 - val_accuracy: 0.3041 - lr: 0.0500
Epoch 10/100
282/282 [==============================] - 22s 77ms/step - loss: 1.8236 - accuracy: 0.4190 - val_loss: 1.8118 - val_accuracy: 0.3955 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 22s 78ms/step - loss: 1.7558 - accuracy: 0.4420 - val_loss: 2.0059 - val_accuracy: 0.3290 - lr: 0.0500
Epoch 12/100
282/282 [==============================] - 22s 78ms/step - loss: 1.7321 - accuracy: 0.4459 - val_loss: 1.7819 - val_accuracy: 0.4231 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 24s 85ms/step - loss: 1.6758 - accuracy: 0.4642 - val_loss: 1.6233 - val_accuracy: 0.4822 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 25s 88ms/step - loss: 1.6510 - accuracy: 0.4710 - val_loss: 1.5975 - val_accuracy: 0.4775 - lr: 0.0500
Epoch 15/100
282/282 [==============================] - 25s 87ms/step - loss: 1.6130 - accuracy: 0.4818 - val_loss: 1.5705 - val_accuracy: 0.4845 - lr: 0.0500
Epoch 16/100
282/282 [==============================] - 25s 89ms/step - loss: 1.5869 - accuracy: 0.4986 - val_loss: 1.5649 - val_accuracy: 0.4919 - lr: 0.0500
Epoch 17/100
282/282 [==============================] - 22s 78ms/step - loss: 1.5402 - accuracy: 0.5059 - val_loss: 1.4124 - val_accuracy: 0.5531 - lr: 0.0500
Epoch 18/100
282/282 [==============================] - 24s 85ms/step - loss: 1.5168 - accuracy: 0.5109 - val_loss: 1.6915 - val_accuracy: 0.4701 - lr: 0.0500
Epoch 19/100
282/282 [==============================] - 25s 88ms/step - loss: 1.4907 - accuracy: 0.5245 - val_loss: 1.3452 - val_accuracy: 0.5733 - lr: 0.0500
Epoch 20/100
282/282 [==============================] - 24s 86ms/step - loss: 1.3747 - accuracy: 0.5620 - val_loss: 1.3983 - val_accuracy: 0.5598 - lr: 0.0250
Epoch 21/100
282/282 [==============================] - 24s 87ms/step - loss: 1.3615 - accuracy: 0.5655 - val_loss: 1.3382 - val_accuracy: 0.5615 - lr: 0.0250
Epoch 22/100
282/282 [==============================] - 24s 86ms/step - loss: 1.3376 - accuracy: 0.5684 - val_loss: 1.2440 - val_accuracy: 0.6075 - lr: 0.0250
Epoch 23/100
282/282 [==============================] - 24s 86ms/step - loss: 1.3307 - accuracy: 0.5780 - val_loss: 1.2746 - val_accuracy: 0.6011 - lr: 0.0250
Epoch 24/100
282/282 [==============================] - 24s 86ms/step - loss: 1.3139 - accuracy: 0.5776 - val_loss: 1.3111 - val_accuracy: 0.5850 - lr: 0.0250
Epoch 25/100
282/282 [==============================] - 24s 86ms/step - loss: 1.3086 - accuracy: 0.5833 - val_loss: 1.2475 - val_accuracy: 0.6005 - lr: 0.0125
Epoch 26/100
282/282 [==============================] - 24s 86ms/step - loss: 1.2751 - accuracy: 0.5956 - val_loss: 1.4525 - val_accuracy: 0.5272 - lr: 0.0250
Epoch 27/100
282/282 [==============================] - 24s 85ms/step - loss: 1.2613 - accuracy: 0.5972 - val_loss: 1.4508 - val_accuracy: 0.5484 - lr: 0.0250
Epoch 28/100
282/282 [==============================] - 24s 86ms/step - loss: 1.2601 - accuracy: 0.6008 - val_loss: 1.2841 - val_accuracy: 0.5837 - lr: 0.0125
Epoch 29/100
282/282 [==============================] - 24s 85ms/step - loss: 1.2357 - accuracy: 0.6083 - val_loss: 1.2696 - val_accuracy: 0.5776 - lr: 0.0250
Epoch 30/100
282/282 [==============================] - 24s 86ms/step - loss: 1.1901 - accuracy: 0.6236 - val_loss: 1.1529 - val_accuracy: 0.6190 - lr: 0.0125
Epoch 31/100
282/282 [==============================] - 24s 85ms/step - loss: 1.1662 - accuracy: 0.6337 - val_loss: 1.1985 - val_accuracy: 0.6106 - lr: 0.0125
Epoch 32/100
282/282 [==============================] - 24s 86ms/step - loss: 1.1638 - accuracy: 0.6367 - val_loss: 1.1257 - val_accuracy: 0.6384 - lr: 0.0125
Epoch 33/100
282/282 [==============================] - 22s 78ms/step - loss: 1.1574 - accuracy: 0.6338 - val_loss: 1.1412 - val_accuracy: 0.6284 - lr: 0.0125
Epoch 34/100
282/282 [==============================] - 21s 76ms/step - loss: 1.1486 - accuracy: 0.6381 - val_loss: 1.1071 - val_accuracy: 0.6374 - lr: 0.0125
Epoch 35/100
282/282 [==============================] - 22s 78ms/step - loss: 1.1503 - accuracy: 0.6397 - val_loss: 1.0865 - val_accuracy: 0.6489 - lr: 0.0125
Epoch 36/100
282/282 [==============================] - 21s 76ms/step - loss: 1.1443 - accuracy: 0.6371 - val_loss: 1.0622 - val_accuracy: 0.6579 - lr: 0.0125
Epoch 37/100
282/282 [==============================] - 22s 77ms/step - loss: 1.1311 - accuracy: 0.6457 - val_loss: 1.0721 - val_accuracy: 0.6509 - lr: 0.0125
Epoch 38/100
282/282 [==============================] - 24s 87ms/step - loss: 1.1268 - accuracy: 0.6453 - val_loss: 1.1232 - val_accuracy: 0.6394 - lr: 0.0125
Epoch 39/100
282/282 [==============================] - 25s 89ms/step - loss: 1.1076 - accuracy: 0.6475 - val_loss: 1.1032 - val_accuracy: 0.6442 - lr: 0.0063
Epoch 40/100
282/282 [==============================] - 26s 91ms/step - loss: 1.0853 - accuracy: 0.6607 - val_loss: 1.0983 - val_accuracy: 0.6472 - lr: 0.0063
Epoch 41/100
282/282 [==============================] - 25s 89ms/step - loss: 1.0752 - accuracy: 0.6595 - val_loss: 1.0209 - val_accuracy: 0.6700 - lr: 0.0063
Epoch 42/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0714 - accuracy: 0.6627 - val_loss: 1.1750 - val_accuracy: 0.6267 - lr: 0.0063
Epoch 43/100
282/282 [==============================] - 26s 91ms/step - loss: 1.0545 - accuracy: 0.6632 - val_loss: 1.0760 - val_accuracy: 0.6552 - lr: 0.0063
Epoch 44/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0601 - accuracy: 0.6641 - val_loss: 1.0503 - val_accuracy: 0.6620 - lr: 0.0031
Epoch 45/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0481 - accuracy: 0.6655 - val_loss: 1.0331 - val_accuracy: 0.6667 - lr: 0.0063
Epoch 46/100
282/282 [==============================] - 26s 91ms/step - loss: 1.0509 - accuracy: 0.6653 - val_loss: 1.0533 - val_accuracy: 0.6610 - lr: 0.0063
Epoch 47/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0377 - accuracy: 0.6724 - val_loss: 1.0684 - val_accuracy: 0.6569 - lr: 0.0031
Epoch 48/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0593 - accuracy: 0.6595 - val_loss: 1.0453 - val_accuracy: 0.6677 - lr: 0.0063
Epoch 49/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0453 - accuracy: 0.6644 - val_loss: 1.0107 - val_accuracy: 0.6744 - lr: 0.0063
Epoch 50/100
282/282 [==============================] - 25s 88ms/step - loss: 1.0305 - accuracy: 0.6716 - val_loss: 1.0401 - val_accuracy: 0.6630 - lr: 0.0031
Epoch 51/100
282/282 [==============================] - 26s 91ms/step - loss: 1.0370 - accuracy: 0.6773 - val_loss: 1.0625 - val_accuracy: 0.6586 - lr: 0.0031
Epoch 52/100
282/282 [==============================] - 26s 92ms/step - loss: 1.0112 - accuracy: 0.6807 - val_loss: 1.0214 - val_accuracy: 0.6700 - lr: 0.0016
Epoch 53/100
282/282 [==============================] - 25s 88ms/step - loss: 1.0211 - accuracy: 0.6784 - val_loss: 1.0011 - val_accuracy: 0.6731 - lr: 0.0031
Epoch 54/100
282/282 [==============================] - 26s 91ms/step - loss: 1.0182 - accuracy: 0.6815 - val_loss: 1.0072 - val_accuracy: 0.6737 - lr: 0.0031
Epoch 55/100
282/282 [==============================] - 26s 92ms/step - loss: 1.0212 - accuracy: 0.6802 - val_loss: 1.0937 - val_accuracy: 0.6458 - lr: 0.0016
Epoch 56/100
282/282 [==============================] - 25s 90ms/step - loss: 0.9960 - accuracy: 0.6832 - val_loss: 1.0370 - val_accuracy: 0.6673 - lr: 0.0031
Epoch 57/100
282/282 [==============================] - 26s 92ms/step - loss: 1.0087 - accuracy: 0.6832 - val_loss: 0.9834 - val_accuracy: 0.6841 - lr: 0.0031
Epoch 58/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0004 - accuracy: 0.6886 - val_loss: 0.9719 - val_accuracy: 0.6791 - lr: 0.0031
Epoch 59/100
282/282 [==============================] - 25s 90ms/step - loss: 1.0149 - accuracy: 0.6832 - val_loss: 1.0056 - val_accuracy: 0.6757 - lr: 0.0031
Epoch 60/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0096 - accuracy: 0.6794 - val_loss: 0.9914 - val_accuracy: 0.6764 - lr: 7.8125e-04
Epoch 61/100
282/282 [==============================] - 27s 94ms/step - loss: 0.9853 - accuracy: 0.6893 - val_loss: 0.9597 - val_accuracy: 0.6828 - lr: 0.0016
Epoch 62/100
282/282 [==============================] - 26s 91ms/step - loss: 0.9896 - accuracy: 0.6920 - val_loss: 0.9992 - val_accuracy: 0.6781 - lr: 0.0016
Epoch 63/100
282/282 [==============================] - 26s 94ms/step - loss: 0.9939 - accuracy: 0.6863 - val_loss: 0.9627 - val_accuracy: 0.6875 - lr: 0.0016
Epoch 64/100
282/282 [==============================] - 26s 93ms/step - loss: 0.9890 - accuracy: 0.6873 - val_loss: 0.9601 - val_accuracy: 0.6875 - lr: 0.0016
Epoch 65/100
282/282 [==============================] - 25s 90ms/step - loss: 0.9794 - accuracy: 0.6970 - val_loss: 0.9757 - val_accuracy: 0.6818 - lr: 0.0016
Epoch 66/100
282/282 [==============================] - 26s 93ms/step - loss: 0.9799 - accuracy: 0.6966 - val_loss: 0.9573 - val_accuracy: 0.6915 - lr: 0.0016
Epoch 67/100
282/282 [==============================] - 26s 91ms/step - loss: 0.9880 - accuracy: 0.6935 - val_loss: 0.9604 - val_accuracy: 0.6902 - lr: 0.0016
Epoch 68/100
282/282 [==============================] - 26s 90ms/step - loss: 0.9763 - accuracy: 0.6891 - val_loss: 0.9783 - val_accuracy: 0.6835 - lr: 0.0016
Epoch 69/100
282/282 [==============================] - 26s 94ms/step - loss: 0.9936 - accuracy: 0.6889 - val_loss: 0.9924 - val_accuracy: 0.6781 - lr: 7.8125e-04
Epoch 70/100
282/282 [==============================] - 26s 92ms/step - loss: 0.9660 - accuracy: 0.6955 - val_loss: 0.9537 - val_accuracy: 0.6868 - lr: 7.8125e-04
Epoch 71/100
282/282 [==============================] - 25s 90ms/step - loss: 0.9894 - accuracy: 0.6879 - val_loss: 0.9533 - val_accuracy: 0.6865 - lr: 7.8125e-04
Epoch 72/100
282/282 [==============================] - 26s 92ms/step - loss: 0.9833 - accuracy: 0.6912 - val_loss: 0.9712 - val_accuracy: 0.6788 - lr: 3.9063e-04
Epoch 73/100
282/282 [==============================] - 26s 91ms/step - loss: 0.9670 - accuracy: 0.6932 - val_loss: 0.9828 - val_accuracy: 0.6788 - lr: 7.8125e-04
Epoch 74/100
282/282 [==============================] - 26s 91ms/step - loss: 0.9880 - accuracy: 0.6889 - val_loss: 0.9716 - val_accuracy: 0.6804 - lr: 7.8125e-04
Epoch 75/100
282/282 [==============================] - 26s 91ms/step - loss: 0.9774 - accuracy: 0.6932 - val_loss: 0.9709 - val_accuracy: 0.6808 - lr: 3.9063e-04
Epoch 76/100
282/282 [==============================] - 25s 89ms/step - loss: 0.9671 - accuracy: 0.6951 - val_loss: 0.9746 - val_accuracy: 0.6815 - lr: 7.8125e-04
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/1ae8446edc5be7938781a64213b5c8b1f71117cd.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>This is amazing.</p>
<p>With augmentation, our validation results follow training results
almost exactly. The model is generalizing to unseen images extremely
well. Augmentation works great for 128x128 images.</p>
<p>However the max validation accuracy is still capped at 0.6915 which
is most likely due to the simple architecture. We'll try a more robust
model architecture later.</p>
</div>
<section id="base-model-with-augmentation-31x31" class="cell markdown">
<h4><strong>BASE MODEL WITH AUGMENTATION (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="54">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>reset_model(base_model)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> instantiate_base_model(INPUT_SHAPE_31)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>base_model_augmented_history <span class="op">=</span> base_model.fit(</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>        tensorboard,</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>storeResult(base_model_augmented_history, description<span class="op">=</span><span class="st">&quot;31x31 augmented&quot;</span>)</span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>plot_history(base_model_augmented_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 17s 59ms/step - loss: 2.6085 - accuracy: 0.1235 - val_loss: 2.6397 - val_accuracy: 0.1290 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 16s 57ms/step - loss: 2.5280 - accuracy: 0.1729 - val_loss: 2.6284 - val_accuracy: 0.1267 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 16s 58ms/step - loss: 2.4706 - accuracy: 0.1923 - val_loss: 2.4971 - val_accuracy: 0.1983 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 16s 56ms/step - loss: 2.3946 - accuracy: 0.2242 - val_loss: 2.5379 - val_accuracy: 0.1455 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 16s 58ms/step - loss: 2.3423 - accuracy: 0.2437 - val_loss: 2.4456 - val_accuracy: 0.2251 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 16s 58ms/step - loss: 2.2734 - accuracy: 0.2619 - val_loss: 2.2995 - val_accuracy: 0.2692 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 16s 57ms/step - loss: 2.2249 - accuracy: 0.2831 - val_loss: 2.2448 - val_accuracy: 0.2782 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 16s 58ms/step - loss: 2.1706 - accuracy: 0.2959 - val_loss: 2.1402 - val_accuracy: 0.3142 - lr: 0.1000
Epoch 9/100
282/282 [==============================] - 15s 53ms/step - loss: 2.1221 - accuracy: 0.3112 - val_loss: 2.4740 - val_accuracy: 0.2399 - lr: 0.1000
Epoch 10/100
282/282 [==============================] - 17s 60ms/step - loss: 2.0192 - accuracy: 0.3450 - val_loss: 2.6325 - val_accuracy: 0.2261 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 17s 59ms/step - loss: 1.9942 - accuracy: 0.3573 - val_loss: 2.1139 - val_accuracy: 0.3390 - lr: 0.0500
Epoch 12/100
282/282 [==============================] - 17s 60ms/step - loss: 1.9655 - accuracy: 0.3568 - val_loss: 2.0956 - val_accuracy: 0.3404 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 17s 59ms/step - loss: 1.9660 - accuracy: 0.3600 - val_loss: 2.1984 - val_accuracy: 0.3179 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 17s 59ms/step - loss: 1.9431 - accuracy: 0.3768 - val_loss: 2.0690 - val_accuracy: 0.3575 - lr: 0.0500
Epoch 15/100
282/282 [==============================] - 17s 59ms/step - loss: 1.9159 - accuracy: 0.3891 - val_loss: 2.0438 - val_accuracy: 0.3669 - lr: 0.0500
Epoch 16/100
282/282 [==============================] - 17s 60ms/step - loss: 1.8906 - accuracy: 0.3900 - val_loss: 2.1701 - val_accuracy: 0.3337 - lr: 0.0500
Epoch 17/100
282/282 [==============================] - 17s 61ms/step - loss: 1.8778 - accuracy: 0.3961 - val_loss: 2.4382 - val_accuracy: 0.2890 - lr: 0.0500
Epoch 18/100
282/282 [==============================] - 17s 59ms/step - loss: 1.8566 - accuracy: 0.3990 - val_loss: 2.2509 - val_accuracy: 0.3142 - lr: 0.0250
Epoch 19/100
282/282 [==============================] - 17s 59ms/step - loss: 1.8410 - accuracy: 0.4050 - val_loss: 1.7482 - val_accuracy: 0.4533 - lr: 0.0500
Epoch 20/100
282/282 [==============================] - 17s 60ms/step - loss: 1.7631 - accuracy: 0.4302 - val_loss: 1.8874 - val_accuracy: 0.4133 - lr: 0.0250
Epoch 21/100
282/282 [==============================] - 17s 59ms/step - loss: 1.7398 - accuracy: 0.4456 - val_loss: 2.0859 - val_accuracy: 0.3653 - lr: 0.0250
Epoch 22/100
282/282 [==============================] - 17s 59ms/step - loss: 1.7378 - accuracy: 0.4405 - val_loss: 1.8526 - val_accuracy: 0.4251 - lr: 0.0125
Epoch 23/100
282/282 [==============================] - 17s 60ms/step - loss: 1.7123 - accuracy: 0.4478 - val_loss: 1.9025 - val_accuracy: 0.3992 - lr: 0.0250
Epoch 24/100
282/282 [==============================] - 17s 60ms/step - loss: 1.7012 - accuracy: 0.4576 - val_loss: 1.9828 - val_accuracy: 0.3921 - lr: 0.0250
Epoch 25/100
282/282 [==============================] - 17s 60ms/step - loss: 1.6804 - accuracy: 0.4604 - val_loss: 1.6408 - val_accuracy: 0.4856 - lr: 0.0250
Epoch 26/100
282/282 [==============================] - 17s 60ms/step - loss: 1.6836 - accuracy: 0.4561 - val_loss: 1.9431 - val_accuracy: 0.4022 - lr: 0.0250
Epoch 27/100
282/282 [==============================] - 17s 60ms/step - loss: 1.6616 - accuracy: 0.4673 - val_loss: 1.9164 - val_accuracy: 0.4217 - lr: 0.0250
Epoch 28/100
282/282 [==============================] - 17s 61ms/step - loss: 1.6524 - accuracy: 0.4670 - val_loss: 1.6477 - val_accuracy: 0.4872 - lr: 0.0250
Epoch 29/100
282/282 [==============================] - 17s 60ms/step - loss: 1.6389 - accuracy: 0.4737 - val_loss: 1.7357 - val_accuracy: 0.4550 - lr: 0.0250
Epoch 30/100
282/282 [==============================] - 17s 60ms/step - loss: 1.5743 - accuracy: 0.4961 - val_loss: 2.1156 - val_accuracy: 0.3690 - lr: 0.0125
Epoch 31/100
282/282 [==============================] - 17s 61ms/step - loss: 1.5696 - accuracy: 0.4956 - val_loss: 2.0397 - val_accuracy: 0.3824 - lr: 0.0063
Epoch 32/100
282/282 [==============================] - 17s 59ms/step - loss: 1.5572 - accuracy: 0.5013 - val_loss: 2.3559 - val_accuracy: 0.3152 - lr: 0.0125
Epoch 33/100
282/282 [==============================] - 17s 60ms/step - loss: 1.5600 - accuracy: 0.4996 - val_loss: 1.8484 - val_accuracy: 0.4355 - lr: 0.0125
Epoch 34/100
282/282 [==============================] - 17s 60ms/step - loss: 1.5337 - accuracy: 0.5030 - val_loss: 2.1905 - val_accuracy: 0.3673 - lr: 0.0063
Epoch 35/100
282/282 [==============================] - 17s 60ms/step - loss: 1.5382 - accuracy: 0.5043 - val_loss: 1.9683 - val_accuracy: 0.3992 - lr: 0.0125
Epoch 36/100
282/282 [==============================] - 17s 60ms/step - loss: 1.5286 - accuracy: 0.5103 - val_loss: 1.9779 - val_accuracy: 0.4049 - lr: 0.0125
Epoch 37/100
282/282 [==============================] - 17s 59ms/step - loss: 1.5379 - accuracy: 0.5028 - val_loss: 1.8893 - val_accuracy: 0.4227 - lr: 0.0063
Epoch 38/100
282/282 [==============================] - 17s 59ms/step - loss: 1.5278 - accuracy: 0.5092 - val_loss: 1.7640 - val_accuracy: 0.4452 - lr: 0.0125
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/2d10575c7eceef094bd10b9faa25205990b4c275.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>With some data augmentation, our training and validation accuracy
follows our training accuracy until it starts to become extremely
haphazard, consistently below training accuracy, and validation loss
increasing as the number of epochs increased.</p>
<p>The validation accuracy we got was also significantly lower than
without augmentation. We can conclude that data augmentation is not
helpful for the 31x31 image sizes.</p>
</div>
<section id="robust-models" class="cell markdown">
<h2><strong>ROBUST MODELS</strong></h2>
<blockquote>
<p>These models are supposed to give us the best performance</p>
</blockquote>
</section>
<section id="adapted-alexnet" class="cell markdown">
<h3><strong>ADAPTED AlexNet</strong></h3>
<p>AlexNet is a pioneering deep convolutional neural network
architecture that made a significant impact on computer vision and deep
learning. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey
Hinton, it won the 2012 ImageNet Large Scale Visual Recognition
Challenge (ILSVRC) by showcasing the advantages of deep architectures.
With its eight-layer design, featuring five convolutional layers,
AlexNet excelled in capturing hierarchical image features. It introduced
innovations like ReLU activations, data augmentation, dropout layers,
and GPU acceleration, setting the stage for the deep learning revolution
in image recognition and inspiring the development of subsequent
advanced neural network architectures.</p>
<p>It was originally trained on 227×227×3 image sizes. I expect our
adapted AlexNet to perform well on the 128x128 images, due to its larger
kernel size <code>(11x11)</code> and large number of filters, giving it
more trainable parameters to fit to the 128x128 images that contain more
data.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:1400/1*3B8iO-se13vA2QfZ4OBRSw.png"/></p>
</section>
<div class="cell code" data-execution_count="32">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of neurons in the fully connected layer has been decreased</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of Convolutional Layer with ReLU activation has been reduced</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of Filters have been reduced</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_alexnet_model(input_shape):</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the adapted_alexnet_model</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model <span class="op">=</span> Sequential(name<span class="op">=</span><span class="st">&quot;alexnet&quot;</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 1: Convolutional Layer with ReLU activation and Max-Pooling</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>        Conv2D(</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>,</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">11</span>, <span class="dv">11</span>),</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>            strides<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>),</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>            activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>,</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>            input_shape<span class="op">=</span>(<span class="op">*</span>input_shape, <span class="dv">1</span>),</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(MaxPooling2D((<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>)))</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 2: Convolutional Layer with ReLU activation and Max-Pooling</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Conv2D(<span class="dv">126</span>, (<span class="dv">5</span>, <span class="dv">5</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(MaxPooling2D((<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>))</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 3: Convolutional Layer with ReLU activation</span></span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Conv2D(<span class="dv">512</span>, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 5: Convolutional Layer with ReLU activation and Max-Pooling</span></span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Conv2D(<span class="dv">128</span>, (<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(MaxPooling2D((<span class="dv">3</span>, <span class="dv">3</span>), strides<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">4</span>), padding<span class="op">=</span><span class="st">&quot;same&quot;</span>))</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 6: Flatten Layer</span></span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Flatten())</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 7: Fully Connected Layer with ReLU activation and Dropout</span></span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb45-36"><a href="#cb45-36" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Dropout(<span class="fl">0.5</span>))</span>
<span id="cb45-37"><a href="#cb45-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-38"><a href="#cb45-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 8: Fully Connected Layer with ReLU activation and Dropout</span></span>
<span id="cb45-39"><a href="#cb45-39" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb45-40"><a href="#cb45-40" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Dropout(<span class="fl">0.5</span>))</span>
<span id="cb45-41"><a href="#cb45-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-42"><a href="#cb45-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layer 9: Fully Connected Layer (Output Layer) with Softmax activation</span></span>
<span id="cb45-43"><a href="#cb45-43" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.add(Dense(NUM_CLASSES, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>))</span>
<span id="cb45-44"><a href="#cb45-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-45"><a href="#cb45-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compile the adapted_alexnet_model</span></span>
<span id="cb45-46"><a href="#cb45-46" aria-hidden="true" tabindex="-1"></a>    adapted_alexnet_model.<span class="bu">compile</span>(</span>
<span id="cb45-47"><a href="#cb45-47" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">&quot;sgd&quot;</span>, loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>, metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>]</span>
<span id="cb45-48"><a href="#cb45-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb45-49"><a href="#cb45-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> adapted_alexnet_model</span>
<span id="cb45-50"><a href="#cb45-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-51"><a href="#cb45-51" aria-hidden="true" tabindex="-1"></a>adapted_alexnet_model <span class="op">=</span> instantiate_alexnet_model(DEFAULT_SHAPE)</span>
<span id="cb45-52"><a href="#cb45-52" aria-hidden="true" tabindex="-1"></a>adapted_alexnet_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;alexnet&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4 (Conv2D)           (None, 54, 54, 128)       15616     
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 26, 26, 128)      0         
 2D)                                                             
                                                                 
 conv2d_5 (Conv2D)           (None, 26, 26, 256)       819456    
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 13, 13, 256)      0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 13, 13, 512)       1180160   
                                                                 
 conv2d_7 (Conv2D)           (None, 13, 13, 512)       2359808   
                                                                 
 max_pooling2d_5 (MaxPooling  (None, 4, 4, 512)        0         
 2D)                                                             
                                                                 
 flatten_1 (Flatten)         (None, 8192)              0         
                                                                 
 dense_3 (Dense)             (None, 256)               2097408   
                                                                 
 dropout_2 (Dropout)         (None, 256)               0         
                                                                 
 dense_4 (Dense)             (None, 128)               32896     
                                                                 
 dropout_3 (Dropout)         (None, 128)               0         
                                                                 
 dense_5 (Dense)             (None, 15)                1935      
                                                                 
=================================================================
Total params: 6,507,279
Trainable params: 6,507,279
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<section id="alexnet-128x128" class="cell markdown">
<h4><strong>AlexNet (128x128)</strong></h4>
<p>Trained with data augmentation</p>
</section>
<div class="cell code" data-execution_count="34">
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>reset_model(adapted_alexnet_model)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> instantiate_alexnet_model(INPUT_SHAPE_128)</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>alexnet_history <span class="op">=</span> alexnet.fit(</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a>storeResult(alexnet_history, description<span class="op">=</span><span class="st">&quot;128x128&quot;</span>)</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a>plot_history(alexnet_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 28s 96ms/step - loss: 2.6492 - accuracy: 0.1010 - val_loss: 2.7618 - val_accuracy: 0.0662 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 27s 94ms/step - loss: 2.6362 - accuracy: 0.1096 - val_loss: 2.7627 - val_accuracy: 0.0964 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 27s 94ms/step - loss: 2.6275 - accuracy: 0.1146 - val_loss: 2.7495 - val_accuracy: 0.1038 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 26s 93ms/step - loss: 2.6178 - accuracy: 0.1301 - val_loss: 2.7484 - val_accuracy: 0.1038 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 27s 94ms/step - loss: 2.5694 - accuracy: 0.1511 - val_loss: 2.7081 - val_accuracy: 0.0954 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 27s 94ms/step - loss: 2.5460 - accuracy: 0.1612 - val_loss: 2.6935 - val_accuracy: 0.1132 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 27s 95ms/step - loss: 2.5097 - accuracy: 0.1763 - val_loss: 2.6348 - val_accuracy: 0.1263 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 27s 94ms/step - loss: 2.4771 - accuracy: 0.1882 - val_loss: 2.5235 - val_accuracy: 0.1566 - lr: 0.1000
Epoch 9/100
282/282 [==============================] - 26s 93ms/step - loss: 2.4370 - accuracy: 0.1976 - val_loss: 2.6457 - val_accuracy: 0.1260 - lr: 0.1000
Epoch 10/100
282/282 [==============================] - 27s 95ms/step - loss: 2.3426 - accuracy: 0.2239 - val_loss: 2.3761 - val_accuracy: 0.1949 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 27s 95ms/step - loss: 2.2999 - accuracy: 0.2358 - val_loss: 2.3344 - val_accuracy: 0.2251 - lr: 0.0500
Epoch 12/100
282/282 [==============================] - 27s 94ms/step - loss: 2.2699 - accuracy: 0.2451 - val_loss: 2.5198 - val_accuracy: 0.1744 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 27s 95ms/step - loss: 2.2332 - accuracy: 0.2572 - val_loss: 2.3132 - val_accuracy: 0.2335 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 28s 98ms/step - loss: 2.1957 - accuracy: 0.2791 - val_loss: 2.1783 - val_accuracy: 0.2826 - lr: 0.0500
Epoch 15/100
282/282 [==============================] - 27s 97ms/step - loss: 2.1605 - accuracy: 0.2850 - val_loss: 2.1257 - val_accuracy: 0.2964 - lr: 0.0500
Epoch 16/100
282/282 [==============================] - 28s 99ms/step - loss: 2.1237 - accuracy: 0.3006 - val_loss: 2.1370 - val_accuracy: 0.2870 - lr: 0.0500
Epoch 17/100
282/282 [==============================] - 27s 96ms/step - loss: 2.0967 - accuracy: 0.3058 - val_loss: 2.0780 - val_accuracy: 0.2977 - lr: 0.0500
Epoch 18/100
282/282 [==============================] - 28s 99ms/step - loss: 2.0551 - accuracy: 0.3196 - val_loss: 2.1003 - val_accuracy: 0.3075 - lr: 0.0500
Epoch 19/100
282/282 [==============================] - 27s 97ms/step - loss: 2.0048 - accuracy: 0.3449 - val_loss: 1.9611 - val_accuracy: 0.3592 - lr: 0.0500
Epoch 20/100
282/282 [==============================] - 28s 99ms/step - loss: 1.8548 - accuracy: 0.3976 - val_loss: 1.8446 - val_accuracy: 0.3871 - lr: 0.0250
Epoch 21/100
282/282 [==============================] - 27s 95ms/step - loss: 1.8224 - accuracy: 0.4106 - val_loss: 1.7255 - val_accuracy: 0.4425 - lr: 0.0250
Epoch 22/100
282/282 [==============================] - 27s 94ms/step - loss: 1.7962 - accuracy: 0.4203 - val_loss: 1.6750 - val_accuracy: 0.4509 - lr: 0.0250
Epoch 23/100
282/282 [==============================] - 27s 94ms/step - loss: 1.7767 - accuracy: 0.4245 - val_loss: 1.6378 - val_accuracy: 0.4590 - lr: 0.0250
Epoch 24/100
282/282 [==============================] - 26s 93ms/step - loss: 1.7253 - accuracy: 0.4463 - val_loss: 1.5251 - val_accuracy: 0.4929 - lr: 0.0250
Epoch 25/100
282/282 [==============================] - 26s 93ms/step - loss: 1.6902 - accuracy: 0.4559 - val_loss: 1.5508 - val_accuracy: 0.4795 - lr: 0.0250
Epoch 26/100
282/282 [==============================] - 26s 93ms/step - loss: 1.6483 - accuracy: 0.4684 - val_loss: 1.4896 - val_accuracy: 0.5040 - lr: 0.0250
Epoch 27/100
282/282 [==============================] - 26s 93ms/step - loss: 1.6146 - accuracy: 0.4809 - val_loss: 1.4785 - val_accuracy: 0.5178 - lr: 0.0250
Epoch 28/100
282/282 [==============================] - 26s 94ms/step - loss: 1.5837 - accuracy: 0.4941 - val_loss: 1.6153 - val_accuracy: 0.4691 - lr: 0.0250
Epoch 29/100
282/282 [==============================] - 27s 94ms/step - loss: 1.5501 - accuracy: 0.5018 - val_loss: 1.6585 - val_accuracy: 0.4469 - lr: 0.0250
Epoch 30/100
282/282 [==============================] - 26s 94ms/step - loss: 1.3895 - accuracy: 0.5511 - val_loss: 1.1993 - val_accuracy: 0.6042 - lr: 0.0125
Epoch 31/100
282/282 [==============================] - 26s 93ms/step - loss: 1.3724 - accuracy: 0.5585 - val_loss: 1.1888 - val_accuracy: 0.6099 - lr: 0.0125
Epoch 32/100
282/282 [==============================] - 26s 94ms/step - loss: 1.3601 - accuracy: 0.5640 - val_loss: 1.2494 - val_accuracy: 0.5655 - lr: 0.0125
Epoch 33/100
282/282 [==============================] - 26s 92ms/step - loss: 1.3136 - accuracy: 0.5764 - val_loss: 1.1987 - val_accuracy: 0.6001 - lr: 0.0125
Epoch 34/100
282/282 [==============================] - 26s 92ms/step - loss: 1.3055 - accuracy: 0.5878 - val_loss: 1.0886 - val_accuracy: 0.6411 - lr: 0.0125
Epoch 35/100
282/282 [==============================] - 26s 92ms/step - loss: 1.2828 - accuracy: 0.5861 - val_loss: 1.0832 - val_accuracy: 0.6428 - lr: 0.0125
Epoch 36/100
282/282 [==============================] - 27s 94ms/step - loss: 1.2707 - accuracy: 0.5964 - val_loss: 1.1073 - val_accuracy: 0.6237 - lr: 0.0125
Epoch 37/100
282/282 [==============================] - 27s 94ms/step - loss: 1.2261 - accuracy: 0.6068 - val_loss: 0.9892 - val_accuracy: 0.6771 - lr: 0.0125
Epoch 38/100
282/282 [==============================] - 26s 93ms/step - loss: 1.2134 - accuracy: 0.6161 - val_loss: 0.9938 - val_accuracy: 0.6774 - lr: 0.0125
Epoch 39/100
282/282 [==============================] - 26s 93ms/step - loss: 1.1917 - accuracy: 0.6171 - val_loss: 1.0599 - val_accuracy: 0.6324 - lr: 0.0125
Epoch 40/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0892 - accuracy: 0.6500 - val_loss: 0.9331 - val_accuracy: 0.6949 - lr: 0.0063
Epoch 41/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0759 - accuracy: 0.6556 - val_loss: 0.9029 - val_accuracy: 0.7053 - lr: 0.0063
Epoch 42/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0703 - accuracy: 0.6595 - val_loss: 0.8858 - val_accuracy: 0.7117 - lr: 0.0063
Epoch 43/100
282/282 [==============================] - 26s 93ms/step - loss: 1.0271 - accuracy: 0.6733 - val_loss: 0.9297 - val_accuracy: 0.6882 - lr: 0.0063
Epoch 44/100
282/282 [==============================] - 28s 98ms/step - loss: 1.0239 - accuracy: 0.6766 - val_loss: 0.8588 - val_accuracy: 0.7090 - lr: 0.0063
Epoch 45/100
282/282 [==============================] - 27s 94ms/step - loss: 1.0221 - accuracy: 0.6821 - val_loss: 0.8334 - val_accuracy: 0.7251 - lr: 0.0063
Epoch 46/100
282/282 [==============================] - 26s 94ms/step - loss: 1.0244 - accuracy: 0.6749 - val_loss: 0.8257 - val_accuracy: 0.7389 - lr: 0.0063
Epoch 47/100
282/282 [==============================] - 27s 94ms/step - loss: 0.9834 - accuracy: 0.6912 - val_loss: 0.8820 - val_accuracy: 0.6986 - lr: 0.0063
Epoch 48/100
282/282 [==============================] - 27s 95ms/step - loss: 0.9855 - accuracy: 0.6903 - val_loss: 0.7760 - val_accuracy: 0.7406 - lr: 0.0063
Epoch 49/100
282/282 [==============================] - 27s 94ms/step - loss: 0.9760 - accuracy: 0.6882 - val_loss: 0.9156 - val_accuracy: 0.6952 - lr: 0.0063
Epoch 50/100
282/282 [==============================] - 27s 94ms/step - loss: 0.8996 - accuracy: 0.7159 - val_loss: 0.7259 - val_accuracy: 0.7601 - lr: 0.0031
Epoch 51/100
282/282 [==============================] - 27s 95ms/step - loss: 0.8940 - accuracy: 0.7134 - val_loss: 0.7392 - val_accuracy: 0.7503 - lr: 0.0031
Epoch 52/100
282/282 [==============================] - 27s 96ms/step - loss: 0.8819 - accuracy: 0.7196 - val_loss: 0.6977 - val_accuracy: 0.7712 - lr: 0.0031
Epoch 53/100
282/282 [==============================] - 27s 95ms/step - loss: 0.8695 - accuracy: 0.7277 - val_loss: 0.6937 - val_accuracy: 0.7819 - lr: 0.0031
Epoch 54/100
282/282 [==============================] - 27s 97ms/step - loss: 0.8621 - accuracy: 0.7334 - val_loss: 0.6875 - val_accuracy: 0.7759 - lr: 0.0031
Epoch 55/100
282/282 [==============================] - 27s 96ms/step - loss: 0.8600 - accuracy: 0.7274 - val_loss: 0.7063 - val_accuracy: 0.7692 - lr: 0.0031
Epoch 56/100
282/282 [==============================] - 26s 94ms/step - loss: 0.8448 - accuracy: 0.7381 - val_loss: 0.6927 - val_accuracy: 0.7762 - lr: 0.0016
Epoch 57/100
282/282 [==============================] - 27s 97ms/step - loss: 0.8581 - accuracy: 0.7312 - val_loss: 0.6575 - val_accuracy: 0.7870 - lr: 0.0031
Epoch 58/100
282/282 [==============================] - 27s 96ms/step - loss: 0.8539 - accuracy: 0.7272 - val_loss: 0.6585 - val_accuracy: 0.7853 - lr: 0.0031
Epoch 59/100
282/282 [==============================] - 27s 94ms/step - loss: 0.8452 - accuracy: 0.7333 - val_loss: 0.6790 - val_accuracy: 0.7739 - lr: 0.0031
Epoch 60/100
282/282 [==============================] - 27s 94ms/step - loss: 0.7977 - accuracy: 0.7473 - val_loss: 0.6293 - val_accuracy: 0.7933 - lr: 0.0016
Epoch 61/100
282/282 [==============================] - 27s 94ms/step - loss: 0.7995 - accuracy: 0.7499 - val_loss: 0.6023 - val_accuracy: 0.8125 - lr: 0.0016
Epoch 62/100
282/282 [==============================] - 26s 94ms/step - loss: 0.7990 - accuracy: 0.7481 - val_loss: 0.6268 - val_accuracy: 0.8014 - lr: 0.0016
Epoch 63/100
282/282 [==============================] - 26s 94ms/step - loss: 0.7866 - accuracy: 0.7511 - val_loss: 0.6848 - val_accuracy: 0.7759 - lr: 0.0016
Epoch 64/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7668 - accuracy: 0.7553 - val_loss: 0.6102 - val_accuracy: 0.8081 - lr: 7.8125e-04
Epoch 65/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7773 - accuracy: 0.7609 - val_loss: 0.5783 - val_accuracy: 0.8202 - lr: 0.0016
Epoch 66/100
282/282 [==============================] - 27s 94ms/step - loss: 0.7832 - accuracy: 0.7560 - val_loss: 0.5883 - val_accuracy: 0.8135 - lr: 0.0016
Epoch 67/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7593 - accuracy: 0.7631 - val_loss: 0.5827 - val_accuracy: 0.8135 - lr: 0.0016
Epoch 68/100
282/282 [==============================] - 27s 97ms/step - loss: 0.7659 - accuracy: 0.7560 - val_loss: 0.5950 - val_accuracy: 0.8185 - lr: 7.8125e-04
Epoch 69/100
282/282 [==============================] - 27s 97ms/step - loss: 0.7564 - accuracy: 0.7613 - val_loss: 0.6067 - val_accuracy: 0.8065 - lr: 0.0016
Epoch 70/100
282/282 [==============================] - 26s 94ms/step - loss: 0.7406 - accuracy: 0.7626 - val_loss: 0.5733 - val_accuracy: 0.8222 - lr: 7.8125e-04
Epoch 71/100
282/282 [==============================] - 27s 96ms/step - loss: 0.7350 - accuracy: 0.7688 - val_loss: 0.5917 - val_accuracy: 0.8122 - lr: 7.8125e-04
Epoch 72/100
282/282 [==============================] - 26s 94ms/step - loss: 0.7436 - accuracy: 0.7649 - val_loss: 0.5813 - val_accuracy: 0.8185 - lr: 7.8125e-04
Epoch 73/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7407 - accuracy: 0.7613 - val_loss: 0.5842 - val_accuracy: 0.8175 - lr: 3.9063e-04
Epoch 74/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7404 - accuracy: 0.7669 - val_loss: 0.5489 - val_accuracy: 0.8323 - lr: 7.8125e-04
Epoch 75/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7387 - accuracy: 0.7687 - val_loss: 0.5610 - val_accuracy: 0.8226 - lr: 7.8125e-04
Epoch 76/100
282/282 [==============================] - 26s 93ms/step - loss: 0.7305 - accuracy: 0.7656 - val_loss: 0.5674 - val_accuracy: 0.8212 - lr: 7.8125e-04
Epoch 77/100
282/282 [==============================] - 26s 94ms/step - loss: 0.7432 - accuracy: 0.7651 - val_loss: 0.5523 - val_accuracy: 0.8303 - lr: 3.9063e-04
Epoch 78/100
282/282 [==============================] - 27s 94ms/step - loss: 0.7366 - accuracy: 0.7703 - val_loss: 0.5558 - val_accuracy: 0.8243 - lr: 7.8125e-04
Epoch 79/100
282/282 [==============================] - 27s 94ms/step - loss: 0.7106 - accuracy: 0.7737 - val_loss: 0.5590 - val_accuracy: 0.8266 - lr: 7.8125e-04
Epoch 80/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7171 - accuracy: 0.7750 - val_loss: 0.5595 - val_accuracy: 0.8229 - lr: 1.9531e-04
Epoch 81/100
282/282 [==============================] - 27s 96ms/step - loss: 0.7186 - accuracy: 0.7735 - val_loss: 0.5518 - val_accuracy: 0.8310 - lr: 3.9063e-04
Epoch 82/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7155 - accuracy: 0.7805 - val_loss: 0.5416 - val_accuracy: 0.8323 - lr: 3.9063e-04
Epoch 83/100
282/282 [==============================] - 27s 96ms/step - loss: 0.7040 - accuracy: 0.7808 - val_loss: 0.5331 - val_accuracy: 0.8360 - lr: 3.9063e-04
Epoch 84/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7168 - accuracy: 0.7796 - val_loss: 0.5339 - val_accuracy: 0.8337 - lr: 3.9063e-04
Epoch 85/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7246 - accuracy: 0.7740 - val_loss: 0.5336 - val_accuracy: 0.8350 - lr: 3.9063e-04
Epoch 86/100
282/282 [==============================] - 27s 96ms/step - loss: 0.7145 - accuracy: 0.7715 - val_loss: 0.5274 - val_accuracy: 0.8330 - lr: 1.9531e-04
Epoch 87/100
282/282 [==============================] - 27s 97ms/step - loss: 0.7062 - accuracy: 0.7797 - val_loss: 0.5427 - val_accuracy: 0.8327 - lr: 3.9063e-04
Epoch 88/100
282/282 [==============================] - 27s 95ms/step - loss: 0.7163 - accuracy: 0.7790 - val_loss: 0.5310 - val_accuracy: 0.8367 - lr: 3.9063e-04
Epoch 89/100
282/282 [==============================] - 27s 96ms/step - loss: 0.7153 - accuracy: 0.7735 - val_loss: 0.5503 - val_accuracy: 0.8286 - lr: 3.9063e-04
Epoch 90/100
282/282 [==============================] - 28s 99ms/step - loss: 0.6964 - accuracy: 0.7830 - val_loss: 0.5312 - val_accuracy: 0.8353 - lr: 1.9531e-04
Epoch 91/100
282/282 [==============================] - 27s 97ms/step - loss: 0.6966 - accuracy: 0.7837 - val_loss: 0.5351 - val_accuracy: 0.8387 - lr: 1.9531e-04
Epoch 92/100
282/282 [==============================] - 27s 97ms/step - loss: 0.6945 - accuracy: 0.7783 - val_loss: 0.5359 - val_accuracy: 0.8360 - lr: 1.9531e-04
Epoch 93/100
282/282 [==============================] - 27s 97ms/step - loss: 0.7024 - accuracy: 0.7810 - val_loss: 0.5294 - val_accuracy: 0.8353 - lr: 1.9531e-04
Epoch 94/100
282/282 [==============================] - 28s 98ms/step - loss: 0.6898 - accuracy: 0.7821 - val_loss: 0.5320 - val_accuracy: 0.8384 - lr: 9.7656e-05
Epoch 95/100
282/282 [==============================] - 28s 98ms/step - loss: 0.7040 - accuracy: 0.7812 - val_loss: 0.5394 - val_accuracy: 0.8350 - lr: 1.9531e-04
Epoch 96/100
282/282 [==============================] - 28s 98ms/step - loss: 0.7002 - accuracy: 0.7732 - val_loss: 0.5316 - val_accuracy: 0.8380 - lr: 1.9531e-04
Epoch 97/100
282/282 [==============================] - 28s 99ms/step - loss: 0.6979 - accuracy: 0.7800 - val_loss: 0.5248 - val_accuracy: 0.8397 - lr: 1.9531e-04
Epoch 98/100
282/282 [==============================] - 28s 98ms/step - loss: 0.7002 - accuracy: 0.7801 - val_loss: 0.5402 - val_accuracy: 0.8353 - lr: 1.9531e-04
Epoch 99/100
282/282 [==============================] - 27s 97ms/step - loss: 0.6964 - accuracy: 0.7800 - val_loss: 0.5245 - val_accuracy: 0.8377 - lr: 1.9531e-04
Epoch 100/100
282/282 [==============================] - 28s 98ms/step - loss: 0.6953 - accuracy: 0.7830 - val_loss: 0.5321 - val_accuracy: 0.8387 - lr: 4.8828e-05
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/122df44c1d3d5f4c133d95a7e6193f2326fb3cab.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>AlexNet performs great on the 128x128 image size, with the validation
accuracy and loss closely following training results. It reached a peak
max validation accuracy of <code>0.84</code>.</p>
<p>This model is capable of generalizing to the larger image size so
well that validation results consistently performed better than training
accuracy.</p>
<p>Though the results are amazing, I think we can further improve
validation accuracy with an even better model.</p>
</div>
<section id="alexnet-31x31" class="cell markdown">
<h4><strong>AlexNet (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="63">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> train_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>N_VAL_STEPS <span class="op">=</span> valid_gen.samples <span class="op">//</span> BATCH_SIZE</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>N_EPOCHS <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> instantiate_alexnet_model(INPUT_SHAPE_31)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>alexnet_history <span class="op">=</span> alexnet.fit(</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    steps_per_epoch<span class="op">=</span>N_STEPS,</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>N_EPOCHS,</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>    validation_steps<span class="op">=</span>N_VAL_STEPS,</span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a>storeResult(alexnet_history, description<span class="op">=</span><span class="st">&quot;31x31&quot;</span>)</span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>plot_history(alexnet_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
282/282 [==============================] - 13s 44ms/step - loss: 2.6502 - accuracy: 0.1014 - val_loss: 2.7692 - val_accuracy: 0.0655 - lr: 0.1000
Epoch 2/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6410 - accuracy: 0.0999 - val_loss: 2.7715 - val_accuracy: 0.0669 - lr: 0.1000
Epoch 3/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6390 - accuracy: 0.1028 - val_loss: 2.7813 - val_accuracy: 0.0665 - lr: 0.1000
Epoch 4/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6372 - accuracy: 0.1020 - val_loss: 2.7865 - val_accuracy: 0.0662 - lr: 0.1000
Epoch 5/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6371 - accuracy: 0.1038 - val_loss: 2.7788 - val_accuracy: 0.0672 - lr: 0.1000
Epoch 6/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6371 - accuracy: 0.1048 - val_loss: 2.7920 - val_accuracy: 0.0672 - lr: 0.1000
Epoch 7/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6358 - accuracy: 0.1058 - val_loss: 2.7849 - val_accuracy: 0.0672 - lr: 0.1000
Epoch 8/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6369 - accuracy: 0.1050 - val_loss: 2.7840 - val_accuracy: 0.0659 - lr: 0.0500
Epoch 9/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6369 - accuracy: 0.1059 - val_loss: 2.7882 - val_accuracy: 0.0662 - lr: 0.1000
Epoch 10/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6357 - accuracy: 0.1056 - val_loss: 2.7933 - val_accuracy: 0.0659 - lr: 0.0500
Epoch 11/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6353 - accuracy: 0.1057 - val_loss: 2.7900 - val_accuracy: 0.0669 - lr: 0.0250
Epoch 12/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6356 - accuracy: 0.1056 - val_loss: 2.7873 - val_accuracy: 0.0672 - lr: 0.0500
Epoch 13/100
282/282 [==============================] - 12s 44ms/step - loss: 2.6356 - accuracy: 0.1065 - val_loss: 2.7897 - val_accuracy: 0.0669 - lr: 0.0500
Epoch 14/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6354 - accuracy: 0.1058 - val_loss: 2.7894 - val_accuracy: 0.0655 - lr: 0.0250
Epoch 15/100
282/282 [==============================] - 12s 43ms/step - loss: 2.6355 - accuracy: 0.1059 - val_loss: 2.7892 - val_accuracy: 0.0672 - lr: 0.0500
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/3ac1832da993672148a2865096cbc6898ad7eb1c.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>AlexNet did terrible with the 31x31 images.</p>
<p>This is likely because the 31x31 image size is so small that the
11x11 filter size of AlexNet could not capture details from the image.
There was absolutely no change in the accuracy during training. This
tells me that smaller images require smaller filter sizes.</p>
</div>
<section id="adapted-vgg-16-model" class="cell markdown">
<h3><strong>ADAPTED VGG-16 MODEL</strong></h3>
</section>
<div class="cell markdown">
<p>VGG16 is a convolutional neural network model that’s used for image
recognition. It’s unique in that it has only 16 layers that have
weights, as opposed to relying on a large number of hyper-parameters.
It’s considered one of the best vision model architectures.</p>
<p>VGG16 improves on AlexNet and <strong>replaces the large filters with
sequences of smaller 3×3 filters</strong>. In AlexNet, the kernel size
is 11 for the first convolutional layer and 5 for the second layer.</p>
<p>VGG16 was originally used with a 224x224 image input size. I do
expect VGG to work well on our larger image size, 128x128, due to its
deep architecture.</p>
<p>I would not be surprised if it worked with the 31x31 image size as
well due to its small 3x3 filter, allowing the model to capture details
from tiny images.</p>
<p><img
src="https://datagen.tech/wp-content/uploads/2022/11/image2-1.png"
alt="VGG-16 visualized" /></p>
</div>
<div class="cell markdown">
<p><strong>VGG block</strong></p>
<p>VGG is made up of repeating blocks of Convolution + ReLU + Max
Pooling</p>
</div>
<div class="cell code" data-execution_count="64">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vgg_block(num_conv, num_channels):</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>    blk <span class="op">=</span> Sequential()</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_conv):</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        blk.add(Conv2D(num_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        blk.add(BatchNormalization())</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>        blk.add(MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>))</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> blk</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="65">
<div class="sourceCode" id="cb52"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_vgg_model(input_shape):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> Input((<span class="op">*</span>input_shape, <span class="dv">1</span>))  <span class="co"># Input</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> inputs</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block(<span class="dv">2</span>, <span class="dv">16</span>)(x)  <span class="co"># Less filters compared to VGG16</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block(<span class="dv">2</span>, <span class="dv">32</span>)(x)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block(<span class="dv">2</span>, <span class="dv">64</span>)(x)</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dense(<span class="dv">64</span>, <span class="st">&quot;relu&quot;</span>)(x)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dense(NUM_CLASSES, <span class="st">&quot;softmax&quot;</span>)(x)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    vgg_model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>x, name<span class="op">=</span><span class="st">&quot;vgg_model&quot;</span>)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a>    vgg_model.<span class="bu">compile</span>(</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>],</span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vgg_model</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>vgg_model <span class="op">=</span> instantiate_vgg_model(DEFAULT_SHAPE)</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>vgg_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;vgg_model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 1)]     0         
                                                                 
 sequential (Sequential)     (None, 56, 56, 16)        2608      
                                                                 
 sequential_1 (Sequential)   (None, 14, 14, 32)        14144     
                                                                 
 sequential_2 (Sequential)   (None, 4, 4, 64)          55936     
                                                                 
 global_average_pooling2d (G  (None, 64)               0         
 lobalAveragePooling2D)                                          
                                                                 
 dense_6 (Dense)             (None, 64)                4160      
                                                                 
 dropout_4 (Dropout)         (None, 64)                0         
                                                                 
 dense_7 (Dense)             (None, 15)                975       
                                                                 
=================================================================
Total params: 77,823
Trainable params: 77,375
Non-trainable params: 448
_________________________________________________________________
</code></pre>
</div>
</div>
<section id="vgg-128x128" class="cell markdown">
<h4><strong>VGG (128x128)</strong></h4>
<p>Trained on augmented data</p>
</section>
<div class="cell code" data-execution_count="68">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>reset_model(vgg_model)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>vgg_model <span class="op">=</span> instantiate_vgg_model(INPUT_SHAPE_128)</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>vgg_model_augmented_history <span class="op">=</span> vgg_model.fit(</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>storeResult(</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>    vgg_model_augmented_history, description<span class="op">=</span><span class="st">&quot;128x128&quot;</span></span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>plot_history(vgg_model_augmented_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 23s 78ms/step - loss: 2.5797 - accuracy: 0.1447 - val_loss: 3.3036 - val_accuracy: 0.0940 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 22s 76ms/step - loss: 2.4144 - accuracy: 0.1927 - val_loss: 5.9791 - val_accuracy: 0.0603 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 22s 77ms/step - loss: 2.1770 - accuracy: 0.2836 - val_loss: 2.2778 - val_accuracy: 0.2843 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 22s 78ms/step - loss: 1.9728 - accuracy: 0.3458 - val_loss: 3.3628 - val_accuracy: 0.2407 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 22s 78ms/step - loss: 1.9288 - accuracy: 0.3716 - val_loss: 2.1164 - val_accuracy: 0.2667 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 21s 74ms/step - loss: 1.8312 - accuracy: 0.4021 - val_loss: 2.4530 - val_accuracy: 0.3337 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 21s 73ms/step - loss: 1.6845 - accuracy: 0.4540 - val_loss: 2.7372 - val_accuracy: 0.2653 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 21s 73ms/step - loss: 1.5854 - accuracy: 0.4852 - val_loss: 1.4862 - val_accuracy: 0.5293 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 22s 78ms/step - loss: 1.5310 - accuracy: 0.5105 - val_loss: 1.4326 - val_accuracy: 0.5210 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 22s 77ms/step - loss: 1.2674 - accuracy: 0.5860 - val_loss: 1.2097 - val_accuracy: 0.5847 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 22s 78ms/step - loss: 1.1614 - accuracy: 0.6222 - val_loss: 1.3418 - val_accuracy: 0.5533 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 22s 77ms/step - loss: 1.1492 - accuracy: 0.6283 - val_loss: 5.1901 - val_accuracy: 0.2367 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 22s 78ms/step - loss: 1.0682 - accuracy: 0.6563 - val_loss: 1.0029 - val_accuracy: 0.6770 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 22s 77ms/step - loss: 1.0141 - accuracy: 0.6729 - val_loss: 2.1251 - val_accuracy: 0.5047 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 22s 76ms/step - loss: 1.0028 - accuracy: 0.6772 - val_loss: 2.7255 - val_accuracy: 0.3717 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 22s 76ms/step - loss: 0.9672 - accuracy: 0.6870 - val_loss: 1.8254 - val_accuracy: 0.5613 - lr: 0.0250
Epoch 17/100
283/283 [==============================] - 22s 77ms/step - loss: 0.9447 - accuracy: 0.7013 - val_loss: 1.6550 - val_accuracy: 0.5263 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 22s 76ms/step - loss: 0.9445 - accuracy: 0.6974 - val_loss: 1.1041 - val_accuracy: 0.6683 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 22s 76ms/step - loss: 0.8986 - accuracy: 0.7136 - val_loss: 1.0772 - val_accuracy: 0.6750 - lr: 0.0250
Epoch 20/100
283/283 [==============================] - 22s 77ms/step - loss: 0.7482 - accuracy: 0.7566 - val_loss: 1.6968 - val_accuracy: 0.5787 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 22s 77ms/step - loss: 0.6778 - accuracy: 0.7832 - val_loss: 1.5823 - val_accuracy: 0.5957 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 21s 74ms/step - loss: 0.6863 - accuracy: 0.7741 - val_loss: 1.2295 - val_accuracy: 0.6480 - lr: 0.0125
Epoch 23/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6955 - accuracy: 0.7792 - val_loss: 0.7783 - val_accuracy: 0.7583 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6537 - accuracy: 0.7947 - val_loss: 0.7382 - val_accuracy: 0.7740 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6460 - accuracy: 0.7895 - val_loss: 0.9385 - val_accuracy: 0.7217 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6271 - accuracy: 0.7982 - val_loss: 1.1151 - val_accuracy: 0.6823 - lr: 0.0250
Epoch 27/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6435 - accuracy: 0.7959 - val_loss: 1.2352 - val_accuracy: 0.6273 - lr: 0.0125
Epoch 28/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6284 - accuracy: 0.7979 - val_loss: 0.6775 - val_accuracy: 0.7760 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 21s 73ms/step - loss: 0.6269 - accuracy: 0.7987 - val_loss: 0.6637 - val_accuracy: 0.7897 - lr: 0.0250
Epoch 30/100
283/283 [==============================] - 21s 73ms/step - loss: 0.5392 - accuracy: 0.8245 - val_loss: 0.7753 - val_accuracy: 0.7697 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 21s 73ms/step - loss: 0.5064 - accuracy: 0.8330 - val_loss: 0.6326 - val_accuracy: 0.8050 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 21s 73ms/step - loss: 0.5298 - accuracy: 0.8341 - val_loss: 0.4968 - val_accuracy: 0.8250 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 21s 75ms/step - loss: 0.5034 - accuracy: 0.8358 - val_loss: 1.0421 - val_accuracy: 0.7020 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 22s 78ms/step - loss: 0.4988 - accuracy: 0.8425 - val_loss: 0.7627 - val_accuracy: 0.7677 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 22s 78ms/step - loss: 0.4960 - accuracy: 0.8404 - val_loss: 0.3990 - val_accuracy: 0.8677 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 22s 79ms/step - loss: 0.5088 - accuracy: 0.8363 - val_loss: 0.5208 - val_accuracy: 0.8367 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 22s 78ms/step - loss: 0.4701 - accuracy: 0.8443 - val_loss: 0.6898 - val_accuracy: 0.8033 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 22s 78ms/step - loss: 0.4823 - accuracy: 0.8412 - val_loss: 1.3476 - val_accuracy: 0.6323 - lr: 0.0063
Epoch 39/100
283/283 [==============================] - 694s 2s/step - loss: 0.4805 - accuracy: 0.8469 - val_loss: 0.9657 - val_accuracy: 0.7043 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4497 - accuracy: 0.8546 - val_loss: 0.4991 - val_accuracy: 0.8373 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 21s 76ms/step - loss: 0.4244 - accuracy: 0.8604 - val_loss: 0.3797 - val_accuracy: 0.8763 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4260 - accuracy: 0.8614 - val_loss: 0.7191 - val_accuracy: 0.7767 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4329 - accuracy: 0.8619 - val_loss: 0.4003 - val_accuracy: 0.8643 - lr: 0.0063
Epoch 44/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4400 - accuracy: 0.8600 - val_loss: 0.3901 - val_accuracy: 0.8713 - lr: 0.0031
Epoch 45/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4214 - accuracy: 0.8651 - val_loss: 0.3969 - val_accuracy: 0.8673 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 21s 76ms/step - loss: 0.4445 - accuracy: 0.8571 - val_loss: 0.3375 - val_accuracy: 0.8913 - lr: 0.0063
Epoch 47/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4244 - accuracy: 0.8633 - val_loss: 0.3257 - val_accuracy: 0.8943 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4143 - accuracy: 0.8658 - val_loss: 0.6205 - val_accuracy: 0.8020 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 21s 76ms/step - loss: 0.4227 - accuracy: 0.8661 - val_loss: 0.4091 - val_accuracy: 0.8637 - lr: 0.0063
Epoch 50/100
283/283 [==============================] - 21s 76ms/step - loss: 0.3901 - accuracy: 0.8738 - val_loss: 0.5725 - val_accuracy: 0.8203 - lr: 0.0016
Epoch 51/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4084 - accuracy: 0.8691 - val_loss: 0.3707 - val_accuracy: 0.8760 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 21s 75ms/step - loss: 0.4042 - accuracy: 0.8710 - val_loss: 0.4466 - val_accuracy: 0.8540 - lr: 0.0031
Epoch 53/100
283/283 [==============================] - 21s 76ms/step - loss: 0.3925 - accuracy: 0.8757 - val_loss: 0.6073 - val_accuracy: 0.8120 - lr: 0.0016
Epoch 54/100
283/283 [==============================] - 459s 2s/step - loss: 0.4062 - accuracy: 0.8705 - val_loss: 0.4912 - val_accuracy: 0.8420 - lr: 0.0031
Epoch 55/100
283/283 [==============================] - 22s 77ms/step - loss: 0.4014 - accuracy: 0.8752 - val_loss: 0.3296 - val_accuracy: 0.8937 - lr: 0.0031
Epoch 56/100
283/283 [==============================] - 22s 77ms/step - loss: 0.4015 - accuracy: 0.8695 - val_loss: 0.4022 - val_accuracy: 0.8770 - lr: 0.0016
Epoch 57/100
283/283 [==============================] - 22s 78ms/step - loss: 0.3862 - accuracy: 0.8757 - val_loss: 0.6124 - val_accuracy: 0.8097 - lr: 0.0031
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/dafb0d176fb323f781fc1c993ccc8bb7982f0a73.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>VGG achieved even higher accuracy (<code>89%</code>) than AlexNet on
the 128x128 images. However validation results seem to fluctuate which
may indicate some overfitting. I will be trying to use regularization to
fix this issue.</p>
</div>
<section id="vgg-31x31" class="cell markdown">
<h4><strong>VGG (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="69">
<div class="sourceCode" id="cb56"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>reset_model(vgg_model)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>vgg_model <span class="op">=</span> instantiate_vgg_model(INPUT_SHAPE_31)</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>vgg_model_augmented_history <span class="op">=</span> vgg_model.fit(</span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>storeResult(</span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>    vgg_model_augmented_history, description<span class="op">=</span><span class="st">&quot;31x31&quot;</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a>plot_history(vgg_model_augmented_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 11s 38ms/step - loss: 2.2092 - accuracy: 0.2868 - val_loss: 2.5744 - val_accuracy: 0.2040 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 10s 36ms/step - loss: 1.7678 - accuracy: 0.4261 - val_loss: 4.1188 - val_accuracy: 0.2040 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 10s 37ms/step - loss: 1.5763 - accuracy: 0.5049 - val_loss: 1.9836 - val_accuracy: 0.4100 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 11s 37ms/step - loss: 1.4445 - accuracy: 0.5541 - val_loss: 1.3604 - val_accuracy: 0.5773 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 11s 37ms/step - loss: 1.3667 - accuracy: 0.5841 - val_loss: 3.5963 - val_accuracy: 0.2430 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 11s 37ms/step - loss: 1.2448 - accuracy: 0.6197 - val_loss: 10.3626 - val_accuracy: 0.0880 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 10s 37ms/step - loss: 1.1853 - accuracy: 0.6433 - val_loss: 1.1806 - val_accuracy: 0.6460 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 10s 37ms/step - loss: 1.2855 - accuracy: 0.6135 - val_loss: 6.5027 - val_accuracy: 0.1520 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 10s 36ms/step - loss: 1.1372 - accuracy: 0.6612 - val_loss: 2.0392 - val_accuracy: 0.5020 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 10s 36ms/step - loss: 0.8104 - accuracy: 0.7547 - val_loss: 0.7796 - val_accuracy: 0.7720 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 10s 36ms/step - loss: 0.7476 - accuracy: 0.7800 - val_loss: 0.9134 - val_accuracy: 0.7263 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 10s 36ms/step - loss: 0.7104 - accuracy: 0.7895 - val_loss: 1.0174 - val_accuracy: 0.7263 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 10s 36ms/step - loss: 1.3098 - accuracy: 0.5918 - val_loss: 2.2003 - val_accuracy: 0.3330 - lr: 0.0250
Epoch 14/100
283/283 [==============================] - 10s 36ms/step - loss: 1.3526 - accuracy: 0.5790 - val_loss: 2.3305 - val_accuracy: 0.4703 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 10s 36ms/step - loss: 0.7616 - accuracy: 0.7637 - val_loss: 1.0428 - val_accuracy: 0.6883 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 10s 36ms/step - loss: 0.7015 - accuracy: 0.7890 - val_loss: 0.7787 - val_accuracy: 0.7590 - lr: 0.0250
Epoch 17/100
283/283 [==============================] - 10s 37ms/step - loss: 0.6267 - accuracy: 0.8068 - val_loss: 4.0816 - val_accuracy: 0.3277 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 10s 37ms/step - loss: 0.6689 - accuracy: 0.7979 - val_loss: 0.6981 - val_accuracy: 0.8043 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 10s 37ms/step - loss: 0.6418 - accuracy: 0.8032 - val_loss: 0.6413 - val_accuracy: 0.8137 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 10s 36ms/step - loss: 0.4680 - accuracy: 0.8551 - val_loss: 0.6395 - val_accuracy: 0.8263 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 11s 37ms/step - loss: 0.4027 - accuracy: 0.8724 - val_loss: 0.5374 - val_accuracy: 0.8447 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 10s 37ms/step - loss: 0.3474 - accuracy: 0.8900 - val_loss: 0.9853 - val_accuracy: 0.7527 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 11s 37ms/step - loss: 0.5339 - accuracy: 0.8414 - val_loss: 0.6915 - val_accuracy: 0.8010 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 10s 37ms/step - loss: 0.4931 - accuracy: 0.8454 - val_loss: 0.6548 - val_accuracy: 0.8140 - lr: 0.0125
Epoch 25/100
283/283 [==============================] - 10s 36ms/step - loss: 0.4346 - accuracy: 0.8683 - val_loss: 0.7538 - val_accuracy: 0.7967 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 10s 36ms/step - loss: 0.5120 - accuracy: 0.8456 - val_loss: 0.5964 - val_accuracy: 0.8237 - lr: 0.0250
Epoch 27/100
283/283 [==============================] - 10s 37ms/step - loss: 0.4209 - accuracy: 0.8712 - val_loss: 0.5698 - val_accuracy: 0.8373 - lr: 0.0125
Epoch 28/100
283/283 [==============================] - 11s 38ms/step - loss: 0.3640 - accuracy: 0.8875 - val_loss: 0.4805 - val_accuracy: 0.8633 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 10s 36ms/step - loss: 0.3869 - accuracy: 0.8824 - val_loss: 0.6094 - val_accuracy: 0.8340 - lr: 0.0250
Epoch 30/100
283/283 [==============================] - 11s 37ms/step - loss: 0.3061 - accuracy: 0.9011 - val_loss: 0.4926 - val_accuracy: 0.8607 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2676 - accuracy: 0.9157 - val_loss: 0.5479 - val_accuracy: 0.8473 - lr: 0.0063
Epoch 32/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2419 - accuracy: 0.9217 - val_loss: 0.4928 - val_accuracy: 0.8660 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2748 - accuracy: 0.9177 - val_loss: 1.0354 - val_accuracy: 0.7143 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 10s 36ms/step - loss: 0.4897 - accuracy: 0.8514 - val_loss: 0.5650 - val_accuracy: 0.8443 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 10s 37ms/step - loss: 0.3232 - accuracy: 0.9003 - val_loss: 0.6087 - val_accuracy: 0.8393 - lr: 0.0063
Epoch 36/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2874 - accuracy: 0.9094 - val_loss: 0.4847 - val_accuracy: 0.8730 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2468 - accuracy: 0.9217 - val_loss: 0.5209 - val_accuracy: 0.8590 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2540 - accuracy: 0.9204 - val_loss: 0.4945 - val_accuracy: 0.8717 - lr: 0.0125
Epoch 39/100
283/283 [==============================] - 10s 36ms/step - loss: 0.2294 - accuracy: 0.9282 - val_loss: 0.4980 - val_accuracy: 0.8733 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 10s 37ms/step - loss: 0.1949 - accuracy: 0.9390 - val_loss: 0.4946 - val_accuracy: 0.8833 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1936 - accuracy: 0.9383 - val_loss: 0.5147 - val_accuracy: 0.8757 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1936 - accuracy: 0.9363 - val_loss: 0.5211 - val_accuracy: 0.8647 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1799 - accuracy: 0.9422 - val_loss: 0.4945 - val_accuracy: 0.8833 - lr: 0.0031
Epoch 44/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1828 - accuracy: 0.9425 - val_loss: 0.5387 - val_accuracy: 0.8700 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1728 - accuracy: 0.9474 - val_loss: 0.5130 - val_accuracy: 0.8793 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1675 - accuracy: 0.9459 - val_loss: 0.5162 - val_accuracy: 0.8783 - lr: 0.0031
Epoch 47/100
283/283 [==============================] - 11s 39ms/step - loss: 0.1756 - accuracy: 0.9426 - val_loss: 0.5126 - val_accuracy: 0.8767 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 11s 41ms/step - loss: 0.1680 - accuracy: 0.9466 - val_loss: 0.5418 - val_accuracy: 0.8740 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 11s 41ms/step - loss: 0.1492 - accuracy: 0.9524 - val_loss: 1.2288 - val_accuracy: 0.7613 - lr: 0.0031
Epoch 50/100
283/283 [==============================] - 12s 41ms/step - loss: 0.1745 - accuracy: 0.9443 - val_loss: 0.5180 - val_accuracy: 0.8753 - lr: 0.0031
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/f4a5199eef997e91fc4e1c86b195a10b2145e89f.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>VGG worked great on our 31x31 image size. This is a large contrast
from AlexNet, likely due to its small 3x3 filter size and deeper layers,
which allowed it to analyze the tiny 31x31 images.</p>
<p>Even though VGG was made for larger models, it still worked great for
smaller image sizes (<code>88.3%</code>), albeit not as well.</p>
</div>
<section id="l2--l1-regularization" class="cell markdown">
<h5><strong>L2 &amp; L1 Regularization</strong></h5>
<p>L1 and L2 are the most common types of regularization. These update
the general cost function by adding another term known as the
regularization term.</p>
<p>Due to the addition of this regularization term, the values of weight
matrices decrease because it assumes that a neural network with smaller
weight matrices leads to simpler models. Therefore, it will also reduce
overfitting to quite an extent.</p>
<p>However, this regularization term differs in L1 and L2.</p>
</section>
<div class="cell markdown">
<p><strong>In L2, we have</strong></p>
<p>Here, lambda (λ) is the regularization parameter. It is the
hyperparameter whose value is optimized for better results. L2
regularization is also known as weight decay as it forces the weights to
decay towards zero (but not exactly zero).</p>
<p><strong>In L1, we have</strong></p>
<p>In this, we penalize the absolute value of the weights. Unlike L2,
the weights may be reduced to zero here.</p>
<p>Hence, it is very useful when we are trying to compress our model.
Otherwise, <strong>we usually prefer L2 over it</strong>.</p>
<p>Source: <a
href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/"
class="uri">https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/</a></p>
</div>
<section id="vgg-16-l2" class="cell markdown">
<h4><strong>VGG-16 L2</strong></h4>
</section>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb58"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vgg_block_l2(num_conv, num_channels, regularization_factor<span class="op">=</span><span class="fl">0.0001</span>):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    blk <span class="op">=</span> Sequential()</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_conv):</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        blk.add(</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>            Conv2D(</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>                num_channels,</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>,</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>                kernel_regularizer<span class="op">=</span>regularizers.l2(regularization_factor),</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>        blk.add(BatchNormalization())</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>        blk.add(MaxPooling2D(pool_size<span class="op">=</span><span class="dv">2</span>, strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>))</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> blk</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="51">
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_vgg_l2_model(input_shape):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> Input((<span class="op">*</span>input_shape, <span class="dv">1</span>))  <span class="co"># Input</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> inputs</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block_l2(<span class="dv">2</span>, <span class="dv">16</span>)(x)  <span class="co"># Less filters compared to VGG16</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block_l2(<span class="dv">2</span>, <span class="dv">32</span>)(x)</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> vgg_block_l2(<span class="dv">2</span>, <span class="dv">64</span>)(x)</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dense(<span class="dv">256</span>, <span class="st">&quot;relu&quot;</span>)(x)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> Dense(NUM_CLASSES, <span class="st">&quot;softmax&quot;</span>)(x)</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>    vgg_l2 <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>x, name<span class="op">=</span><span class="st">&quot;vgg_l2&quot;</span>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>    vgg_l2.<span class="bu">compile</span>(</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span>SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>],</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> vgg_l2</span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a>vgg_l2 <span class="op">=</span> instantiate_vgg_l2_model(DEFAULT_SHAPE)</span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a>vgg_l2.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;vgg_l2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 224, 224, 1)]     0         
                                                                 
 sequential_3 (Sequential)   (None, 56, 56, 16)        2608      
                                                                 
 sequential_4 (Sequential)   (None, 14, 14, 32)        14144     
                                                                 
 sequential_5 (Sequential)   (None, 4, 4, 64)          55936     
                                                                 
 global_average_pooling2d_1   (None, 64)               0         
 (GlobalAveragePooling2D)                                        
                                                                 
 dense_2 (Dense)             (None, 256)               16640     
                                                                 
 dropout_1 (Dropout)         (None, 256)               0         
                                                                 
 dense_3 (Dense)             (None, 15)                3855      
                                                                 
=================================================================
Total params: 93,183
Trainable params: 92,735
Non-trainable params: 448
_________________________________________________________________
</code></pre>
</div>
</div>
<section id="vgg-with-l2-128x128" class="cell markdown">
<h4><strong>VGG With L2 (128x128)</strong></h4>
</section>
<div class="cell code" data-execution_count="72">
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>reset_model(vgg_l2)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>vgg_l2 <span class="op">=</span> instantiate_vgg_l2_model(INPUT_SHAPE_128)</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>vgg_l2_history <span class="op">=</span> vgg_l2.fit(</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb61-22"><a href="#cb61-22" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb61-23"><a href="#cb61-23" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb61-24"><a href="#cb61-24" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb61-25"><a href="#cb61-25" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb61-26"><a href="#cb61-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-27"><a href="#cb61-27" aria-hidden="true" tabindex="-1"></a>storeResult(vgg_l2_history, description<span class="op">=</span><span class="st">&quot;128x128 with L2&quot;</span>)</span>
<span id="cb61-28"><a href="#cb61-28" aria-hidden="true" tabindex="-1"></a>plot_history(vgg_l2_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 26s 88ms/step - loss: 2.4086 - accuracy: 0.2235 - val_loss: 5.7881 - val_accuracy: 0.1327 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 25s 87ms/step - loss: 2.0311 - accuracy: 0.3519 - val_loss: 2.7215 - val_accuracy: 0.2237 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 25s 87ms/step - loss: 1.8968 - accuracy: 0.3990 - val_loss: 5.5691 - val_accuracy: 0.1720 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 25s 87ms/step - loss: 1.7722 - accuracy: 0.4525 - val_loss: 3.3465 - val_accuracy: 0.2280 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 24s 86ms/step - loss: 1.6910 - accuracy: 0.4906 - val_loss: 4.8939 - val_accuracy: 0.2603 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 24s 86ms/step - loss: 1.6587 - accuracy: 0.5102 - val_loss: 1.9555 - val_accuracy: 0.4253 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 25s 87ms/step - loss: 1.6164 - accuracy: 0.5382 - val_loss: 1.9550 - val_accuracy: 0.4757 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 25s 87ms/step - loss: 1.6103 - accuracy: 0.5454 - val_loss: 4.0047 - val_accuracy: 0.3133 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 24s 86ms/step - loss: 1.6097 - accuracy: 0.5528 - val_loss: 1.9994 - val_accuracy: 0.4600 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 25s 87ms/step - loss: 1.2486 - accuracy: 0.6555 - val_loss: 2.0631 - val_accuracy: 0.5003 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 25s 87ms/step - loss: 1.1367 - accuracy: 0.6990 - val_loss: 1.1117 - val_accuracy: 0.6993 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 25s 87ms/step - loss: 1.0682 - accuracy: 0.7149 - val_loss: 1.5947 - val_accuracy: 0.5733 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 25s 87ms/step - loss: 1.0428 - accuracy: 0.7274 - val_loss: 1.9869 - val_accuracy: 0.5483 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 25s 87ms/step - loss: 1.0260 - accuracy: 0.7328 - val_loss: 1.5364 - val_accuracy: 0.5943 - lr: 0.0250
Epoch 15/100
283/283 [==============================] - 25s 87ms/step - loss: 0.9814 - accuracy: 0.7493 - val_loss: 1.8309 - val_accuracy: 0.5510 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 25s 87ms/step - loss: 0.9581 - accuracy: 0.7568 - val_loss: 1.9578 - val_accuracy: 0.5770 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 25s 87ms/step - loss: 0.9893 - accuracy: 0.7519 - val_loss: 1.5718 - val_accuracy: 0.6050 - lr: 0.0250
Epoch 18/100
283/283 [==============================] - 25s 87ms/step - loss: 0.9560 - accuracy: 0.7589 - val_loss: 3.2369 - val_accuracy: 0.4100 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 25s 87ms/step - loss: 0.9073 - accuracy: 0.7744 - val_loss: 2.3899 - val_accuracy: 0.5063 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 25s 87ms/step - loss: 0.7854 - accuracy: 0.8116 - val_loss: 0.6819 - val_accuracy: 0.8370 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 25s 87ms/step - loss: 0.7110 - accuracy: 0.8324 - val_loss: 1.0243 - val_accuracy: 0.7523 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 25s 87ms/step - loss: 0.7100 - accuracy: 0.8331 - val_loss: 0.8979 - val_accuracy: 0.7783 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6727 - accuracy: 0.8434 - val_loss: 0.9245 - val_accuracy: 0.7677 - lr: 0.0125
Epoch 24/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6678 - accuracy: 0.8481 - val_loss: 0.8540 - val_accuracy: 0.7907 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6512 - accuracy: 0.8530 - val_loss: 1.9728 - val_accuracy: 0.5963 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6537 - accuracy: 0.8473 - val_loss: 1.0715 - val_accuracy: 0.7460 - lr: 0.0125
Epoch 27/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6186 - accuracy: 0.8581 - val_loss: 1.0159 - val_accuracy: 0.7390 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6481 - accuracy: 0.8535 - val_loss: 0.8140 - val_accuracy: 0.8223 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 25s 88ms/step - loss: 0.6282 - accuracy: 0.8566 - val_loss: 0.8155 - val_accuracy: 0.8150 - lr: 0.0125
Epoch 30/100
283/283 [==============================] - 25s 88ms/step - loss: 0.5492 - accuracy: 0.8818 - val_loss: 0.5845 - val_accuracy: 0.8690 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 25s 88ms/step - loss: 0.5242 - accuracy: 0.8902 - val_loss: 0.6527 - val_accuracy: 0.8627 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 25s 88ms/step - loss: 0.5232 - accuracy: 0.8846 - val_loss: 0.6170 - val_accuracy: 0.8663 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 25s 89ms/step - loss: 0.5114 - accuracy: 0.8928 - val_loss: 0.4790 - val_accuracy: 0.9080 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 25s 88ms/step - loss: 0.4977 - accuracy: 0.8965 - val_loss: 0.7432 - val_accuracy: 0.8310 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 25s 88ms/step - loss: 0.4881 - accuracy: 0.9022 - val_loss: 0.7623 - val_accuracy: 0.8130 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 25s 88ms/step - loss: 0.4860 - accuracy: 0.9011 - val_loss: 0.5690 - val_accuracy: 0.8677 - lr: 0.0063
Epoch 37/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4722 - accuracy: 0.9041 - val_loss: 0.5528 - val_accuracy: 0.8850 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4774 - accuracy: 0.8988 - val_loss: 0.6860 - val_accuracy: 0.8477 - lr: 0.0125
Epoch 39/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4837 - accuracy: 0.8949 - val_loss: 0.5070 - val_accuracy: 0.8913 - lr: 0.0063
Epoch 40/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4345 - accuracy: 0.9129 - val_loss: 0.7658 - val_accuracy: 0.8280 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4099 - accuracy: 0.9211 - val_loss: 0.7894 - val_accuracy: 0.8257 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4316 - accuracy: 0.9118 - val_loss: 0.6027 - val_accuracy: 0.8647 - lr: 0.0031
Epoch 43/100
283/283 [==============================] - 25s 88ms/step - loss: 0.4212 - accuracy: 0.9193 - val_loss: 0.4417 - val_accuracy: 0.9190 - lr: 0.0063
Epoch 44/100
283/283 [==============================] - 24s 86ms/step - loss: 0.4097 - accuracy: 0.9189 - val_loss: 0.4996 - val_accuracy: 0.8980 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4169 - accuracy: 0.9185 - val_loss: 0.4228 - val_accuracy: 0.9180 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 25s 87ms/step - loss: 0.3902 - accuracy: 0.9256 - val_loss: 0.6479 - val_accuracy: 0.8597 - lr: 0.0031
Epoch 47/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4098 - accuracy: 0.9212 - val_loss: 0.5467 - val_accuracy: 0.8807 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 25s 87ms/step - loss: 0.3993 - accuracy: 0.9233 - val_loss: 0.4850 - val_accuracy: 0.9017 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4046 - accuracy: 0.9229 - val_loss: 0.7203 - val_accuracy: 0.8413 - lr: 0.0031
Epoch 50/100
283/283 [==============================] - 25s 87ms/step - loss: 0.3791 - accuracy: 0.9265 - val_loss: 0.4433 - val_accuracy: 0.9147 - lr: 0.0031
Epoch 51/100
283/283 [==============================] - 25s 87ms/step - loss: 0.3873 - accuracy: 0.9274 - val_loss: 0.4532 - val_accuracy: 0.9063 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 24s 86ms/step - loss: 0.3699 - accuracy: 0.9292 - val_loss: 0.4395 - val_accuracy: 0.9090 - lr: 0.0016
Epoch 53/100
283/283 [==============================] - 25s 87ms/step - loss: 0.3650 - accuracy: 0.9304 - val_loss: 0.4925 - val_accuracy: 0.8953 - lr: 0.0031
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/b75089faf29c35f51803ce5cb93c81f80792cd25.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>L2 regularization managed to fix the drop in training and validation
accuracy earlier between 34 to 37 epochs. It was even able to get a
higher max validation accuracy of <code>95.9%</code>. However before 25
epochs, training was extremely volatile. This could be due to the new
regularization parameter <code>(λ)</code> introduced, whose value has
not had time to be optimized for better results. Over time, we can see
that it has significantly regularized training to be more consistent and
regular.</p>
</div>
<section id="vgg-with-l2-31x31" class="cell markdown">
<h4><strong>VGG With L2 (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="73">
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>reset_model(vgg_l2)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>vgg_l2 <span class="op">=</span> instantiate_vgg_l2_model(INPUT_SHAPE_31)</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>vgg_l2_history <span class="op">=</span> vgg_l2.fit(</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>storeResult(vgg_l2_history, description<span class="op">=</span><span class="st">&quot;31x31 with L2&quot;</span>)</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>plot_history(vgg_l2_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 13s 41ms/step - loss: 2.1674 - accuracy: 0.3393 - val_loss: 7.8682 - val_accuracy: 0.0660 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 11s 40ms/step - loss: 1.8352 - accuracy: 0.4492 - val_loss: 3.2217 - val_accuracy: 0.2670 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 12s 41ms/step - loss: 1.6466 - accuracy: 0.5225 - val_loss: 2.0707 - val_accuracy: 0.4433 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 11s 40ms/step - loss: 1.5105 - accuracy: 0.5691 - val_loss: 5.3080 - val_accuracy: 0.1917 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 11s 41ms/step - loss: 1.4642 - accuracy: 0.6035 - val_loss: 1.8896 - val_accuracy: 0.5273 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 12s 41ms/step - loss: 1.6124 - accuracy: 0.5740 - val_loss: 7.1410 - val_accuracy: 0.1467 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 11s 41ms/step - loss: 1.5654 - accuracy: 0.6052 - val_loss: 3.6119 - val_accuracy: 0.2840 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 11s 40ms/step - loss: 1.3653 - accuracy: 0.6829 - val_loss: 3.7469 - val_accuracy: 0.3333 - lr: 0.0500
Epoch 9/100
283/283 [==============================] - 11s 40ms/step - loss: 1.3066 - accuracy: 0.7003 - val_loss: 1.8822 - val_accuracy: 0.5970 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 11s 40ms/step - loss: 1.0808 - accuracy: 0.7612 - val_loss: 3.6402 - val_accuracy: 0.3563 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 11s 40ms/step - loss: 0.9532 - accuracy: 0.8043 - val_loss: 0.9720 - val_accuracy: 0.7903 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 12s 41ms/step - loss: 0.8474 - accuracy: 0.8289 - val_loss: 3.6955 - val_accuracy: 0.4293 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 11s 40ms/step - loss: 0.9651 - accuracy: 0.8024 - val_loss: 1.0329 - val_accuracy: 0.7850 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 11s 40ms/step - loss: 1.0306 - accuracy: 0.7870 - val_loss: 1.2160 - val_accuracy: 0.7393 - lr: 0.0250
Epoch 15/100
283/283 [==============================] - 11s 40ms/step - loss: 0.8471 - accuracy: 0.8332 - val_loss: 1.0659 - val_accuracy: 0.7697 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 11s 40ms/step - loss: 0.8201 - accuracy: 0.8468 - val_loss: 1.4420 - val_accuracy: 0.6867 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 11s 40ms/step - loss: 0.8139 - accuracy: 0.8487 - val_loss: 1.3069 - val_accuracy: 0.7443 - lr: 0.0250
Epoch 18/100
283/283 [==============================] - 11s 40ms/step - loss: 0.7877 - accuracy: 0.8553 - val_loss: 1.4237 - val_accuracy: 0.7003 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 11s 40ms/step - loss: 0.8207 - accuracy: 0.8447 - val_loss: 7.7577 - val_accuracy: 0.2023 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 11s 40ms/step - loss: 0.7185 - accuracy: 0.8776 - val_loss: 0.8312 - val_accuracy: 0.8610 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6167 - accuracy: 0.9067 - val_loss: 2.5198 - val_accuracy: 0.5523 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 11s 40ms/step - loss: 0.7218 - accuracy: 0.8770 - val_loss: 1.7255 - val_accuracy: 0.6897 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 11s 41ms/step - loss: 0.6699 - accuracy: 0.8937 - val_loss: 1.7014 - val_accuracy: 0.6650 - lr: 0.0125
Epoch 24/100
283/283 [==============================] - 11s 40ms/step - loss: 0.6239 - accuracy: 0.9018 - val_loss: 0.8583 - val_accuracy: 0.8533 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 11s 40ms/step - loss: 0.5459 - accuracy: 0.9251 - val_loss: 1.1656 - val_accuracy: 0.8030 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 11s 40ms/step - loss: 0.5295 - accuracy: 0.9290 - val_loss: 1.2125 - val_accuracy: 0.7877 - lr: 0.0125
Epoch 27/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4969 - accuracy: 0.9363 - val_loss: 0.9014 - val_accuracy: 0.8597 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 11s 40ms/step - loss: 0.5074 - accuracy: 0.9383 - val_loss: 0.7436 - val_accuracy: 0.8753 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 11s 40ms/step - loss: 0.5008 - accuracy: 0.9361 - val_loss: 0.7375 - val_accuracy: 0.8847 - lr: 0.0250
Epoch 30/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4273 - accuracy: 0.9571 - val_loss: 0.9947 - val_accuracy: 0.8427 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4636 - accuracy: 0.9472 - val_loss: 0.9672 - val_accuracy: 0.8317 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4294 - accuracy: 0.9523 - val_loss: 0.8510 - val_accuracy: 0.8760 - lr: 0.0063
Epoch 33/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4106 - accuracy: 0.9593 - val_loss: 0.9111 - val_accuracy: 0.8627 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4030 - accuracy: 0.9622 - val_loss: 1.1985 - val_accuracy: 0.8070 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3773 - accuracy: 0.9665 - val_loss: 0.7548 - val_accuracy: 0.8893 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3755 - accuracy: 0.9682 - val_loss: 0.8727 - val_accuracy: 0.8597 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3872 - accuracy: 0.9648 - val_loss: 0.7187 - val_accuracy: 0.8937 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3663 - accuracy: 0.9687 - val_loss: 0.7224 - val_accuracy: 0.8950 - lr: 0.0125
Epoch 39/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3796 - accuracy: 0.9648 - val_loss: 0.7579 - val_accuracy: 0.8910 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3455 - accuracy: 0.9739 - val_loss: 0.8043 - val_accuracy: 0.8923 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 12s 41ms/step - loss: 0.3381 - accuracy: 0.9751 - val_loss: 0.7806 - val_accuracy: 0.8977 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3364 - accuracy: 0.9755 - val_loss: 0.6973 - val_accuracy: 0.9040 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3277 - accuracy: 0.9785 - val_loss: 0.7506 - val_accuracy: 0.9007 - lr: 0.0063
Epoch 44/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3251 - accuracy: 0.9780 - val_loss: 0.7644 - val_accuracy: 0.8980 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3133 - accuracy: 0.9800 - val_loss: 0.7717 - val_accuracy: 0.8970 - lr: 0.0031
Epoch 46/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3075 - accuracy: 0.9821 - val_loss: 0.8609 - val_accuracy: 0.8847 - lr: 0.0063
Epoch 47/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3086 - accuracy: 0.9814 - val_loss: 0.7655 - val_accuracy: 0.8997 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2972 - accuracy: 0.9852 - val_loss: 0.7856 - val_accuracy: 0.8977 - lr: 0.0031
Epoch 49/100
283/283 [==============================] - 11s 40ms/step - loss: 0.3004 - accuracy: 0.9821 - val_loss: 0.7399 - val_accuracy: 0.9040 - lr: 0.0063
Epoch 50/100
283/283 [==============================] - 12s 41ms/step - loss: 0.2909 - accuracy: 0.9863 - val_loss: 0.7509 - val_accuracy: 0.9063 - lr: 0.0031
Epoch 51/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2898 - accuracy: 0.9866 - val_loss: 0.7305 - val_accuracy: 0.9103 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2871 - accuracy: 0.9874 - val_loss: 0.7685 - val_accuracy: 0.9023 - lr: 0.0031
Epoch 53/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2977 - accuracy: 0.9824 - val_loss: 0.7471 - val_accuracy: 0.9070 - lr: 0.0031
Epoch 54/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2859 - accuracy: 0.9873 - val_loss: 0.7249 - val_accuracy: 0.9123 - lr: 0.0031
Epoch 55/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2802 - accuracy: 0.9891 - val_loss: 0.7325 - val_accuracy: 0.9110 - lr: 0.0031
Epoch 56/100
283/283 [==============================] - 11s 41ms/step - loss: 0.2913 - accuracy: 0.9858 - val_loss: 0.7451 - val_accuracy: 0.9060 - lr: 0.0031
Epoch 57/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2842 - accuracy: 0.9874 - val_loss: 0.7250 - val_accuracy: 0.9113 - lr: 0.0016
Epoch 58/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2837 - accuracy: 0.9873 - val_loss: 0.7521 - val_accuracy: 0.9053 - lr: 0.0031
Epoch 59/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2764 - accuracy: 0.9891 - val_loss: 0.7667 - val_accuracy: 0.9067 - lr: 0.0031
Epoch 60/100
283/283 [==============================] - 12s 41ms/step - loss: 0.2832 - accuracy: 0.9869 - val_loss: 0.7369 - val_accuracy: 0.9083 - lr: 7.8125e-04
Epoch 61/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2692 - accuracy: 0.9908 - val_loss: 0.7438 - val_accuracy: 0.9103 - lr: 0.0016
Epoch 62/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2721 - accuracy: 0.9909 - val_loss: 0.7689 - val_accuracy: 0.9020 - lr: 0.0016
Epoch 63/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2751 - accuracy: 0.9887 - val_loss: 0.7330 - val_accuracy: 0.9103 - lr: 7.8125e-04
Epoch 64/100
283/283 [==============================] - 11s 40ms/step - loss: 0.2776 - accuracy: 0.9877 - val_loss: 0.7404 - val_accuracy: 0.9097 - lr: 0.0016
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/b39b564de2f87f83e72fb9cf4a456c00dc7767b3.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>L2 regularization managed to fix the abnormal drop in training and
validation accuracy earlier around 33 to 48 epochs. It was even able to
get a higher max validation accuracy of <code>91.2%</code>. However
before 25 epochs, training was extremely volatile. This could be due to
the new regularization parameter <code>(λ)</code> introduced, whose
value has not had time to be optimized for better results. Over time, we
can see that it has significantly regularized training to be more
consistent and regular.</p>
</div>
<div class="cell markdown">
<p><strong>Adapted VGG Takeaways</strong></p>
<p>This model was able to work with both large and small image sizes
after changing the number of filters and blocks from the original VGG-16
model that was made for 224x224 image sizes.</p>
<p>Smaller kernel sizes allow CNNs to capture more details in an image,
thus improving validation results. Smaller kernel sizes also work for
both large and small images, whereas large kernel sizes are terrible for
small images.</p>
</div>
<section id="adapted-resnet18-model" class="cell markdown">
<h3><strong>ADAPTED ResNet18 MODEL</strong></h3>
</section>
<div class="cell markdown">
<p><strong>ResNet</strong>, short for Residual Networks, is a classic
neural network used in the field of deep learning, particularly in image
recognition and computer vision. It was introduced by Kaiming He et al.
in their 2015 paper, "Deep Residual Learning for Image Recognition." The
key innovation of ResNet is the introduction of "residual blocks" with
skip connections.</p>
<p>ResNet's main feature compared to VGG is its use of residual blocks.
These blocks allow the network to skip one or more layers, using a
so-called "skip connection" or "shortcut connection." This helps in
addressing the vanishing gradient problem by allowing the gradient to
flow through the network more effectively during training.</p>
<p>ResNet18 was built for small images. It was trained on images 32x32.
I expect it to perform better on 31x31 than on 128x128.</p>
<p><img src="https://www.researchgate.net/publication/343957273/figure/fig2/AS:930467717083136@1598852199057/ResNet-18-architecture-used-in-the-proposed-method-Res-block1-is-a-regular-ResNet-block.png" width="700"/></p>
</div>
<div class="cell code" data-execution_count="74">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResnetBlock(Model):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="co">    A standard resnet block.</span></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, channels: <span class="bu">int</span>, down_sample<span class="op">=</span><span class="va">False</span>, regularization<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a><span class="co">        channels: same as number of convolution kernels</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.__channels <span class="op">=</span> channels</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.__down_sample <span class="op">=</span> down_sample</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.__regularization <span class="op">=</span> regularization</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.__strides <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">1</span>] <span class="cf">if</span> down_sample <span class="cf">else</span> [<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>        KERNEL_SIZE <span class="op">=</span> (<span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>        INIT_SCHEME <span class="op">=</span> <span class="st">&quot;he_normal&quot;</span></span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_1 <span class="op">=</span> Conv2D(</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.__channels,</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>            strides<span class="op">=</span><span class="va">self</span>.__strides[<span class="dv">0</span>],</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>KERNEL_SIZE,</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>            kernel_initializer<span class="op">=</span>INIT_SCHEME,</span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span><span class="va">self</span>.__regularization,</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn_1 <span class="op">=</span> BatchNormalization()</span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_2 <span class="op">=</span> Conv2D(</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.__channels,</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>            strides<span class="op">=</span><span class="va">self</span>.__strides[<span class="dv">1</span>],</span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>KERNEL_SIZE,</span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>            kernel_initializer<span class="op">=</span>INIT_SCHEME,</span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>            kernel_regularizer<span class="op">=</span><span class="va">self</span>.__regularization,</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn_2 <span class="op">=</span> BatchNormalization()</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.merge <span class="op">=</span> Add()</span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.__down_sample:</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_conv <span class="op">=</span> Conv2D(</span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.__channels,</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>                strides<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a>                kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>                kernel_initializer<span class="op">=</span>INIT_SCHEME,</span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>                kernel_regularizer<span class="op">=</span><span class="va">self</span>.__regularization,</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>                padding<span class="op">=</span><span class="st">&quot;same&quot;</span>,</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_bn <span class="op">=</span> BatchNormalization()</span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> inputs</span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv_1(inputs)</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn_1(x)</span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> tf.nn.relu(x)</span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv_2(x)</span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn_2(x)</span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-60"><a href="#cb65-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.__down_sample:</span>
<span id="cb65-61"><a href="#cb65-61" aria-hidden="true" tabindex="-1"></a>            res <span class="op">=</span> <span class="va">self</span>.res_conv(res)</span>
<span id="cb65-62"><a href="#cb65-62" aria-hidden="true" tabindex="-1"></a>            res <span class="op">=</span> <span class="va">self</span>.res_bn(res)</span>
<span id="cb65-63"><a href="#cb65-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-64"><a href="#cb65-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if not perform down sample, then add a shortcut directly</span></span>
<span id="cb65-65"><a href="#cb65-65" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.merge([x, res])</span>
<span id="cb65-66"><a href="#cb65-66" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> tf.nn.relu(x)</span>
<span id="cb65-67"><a href="#cb65-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="75">
<div class="sourceCode" id="cb66"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ResNet18(Model):</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes, regularizer, <span class="op">**</span>kwargs):</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co">        num_classes: number of classes in specific classification task.</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_1 <span class="op">=</span> Conv2D(</span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, (<span class="dv">7</span>, <span class="dv">7</span>), strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>, kernel_initializer<span class="op">=</span><span class="st">&quot;he_normal&quot;</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_bn <span class="op">=</span> BatchNormalization()</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool_2 <span class="op">=</span> MaxPool2D(pool_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>), strides<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="st">&quot;same&quot;</span>)</span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_1_1 <span class="op">=</span> ResnetBlock(<span class="dv">64</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_1_2 <span class="op">=</span> ResnetBlock(<span class="dv">64</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_2_1 <span class="op">=</span> ResnetBlock(<span class="dv">128</span>, down_sample<span class="op">=</span><span class="va">True</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_2_2 <span class="op">=</span> ResnetBlock(<span class="dv">128</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_3_1 <span class="op">=</span> ResnetBlock(<span class="dv">256</span>, down_sample<span class="op">=</span><span class="va">True</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_3_2 <span class="op">=</span> ResnetBlock(<span class="dv">256</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_4_1 <span class="op">=</span> ResnetBlock(<span class="dv">512</span>, down_sample<span class="op">=</span><span class="va">True</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res_4_2 <span class="op">=</span> ResnetBlock(<span class="dv">512</span>, regularization<span class="op">=</span>regularizer)</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.avg_pool <span class="op">=</span> GlobalAveragePooling2D()</span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flat <span class="op">=</span> Flatten()</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> Dense(num_classes, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>)</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_1(inputs)</span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.init_bn(out)</span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> tf.nn.relu(out)</span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.pool_2(out)</span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> res_block <span class="kw">in</span> [</span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_1_1,</span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_1_2,</span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_2_1,</span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_2_2,</span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_3_1,</span>
<span id="cb66-35"><a href="#cb66-35" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_3_2,</span>
<span id="cb66-36"><a href="#cb66-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_4_1,</span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.res_4_2,</span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a>        ]:</span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> res_block(out)</span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb66-41"><a href="#cb66-41" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.avg_pool(out)</span>
<span id="cb66-42"><a href="#cb66-42" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.flat(out)</span>
<span id="cb66-43"><a href="#cb66-43" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.fc(out)</span>
<span id="cb66-44"><a href="#cb66-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="76">
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> instantiate_resnet18_model(input_shape, regularizer<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    resnet_18_model <span class="op">=</span> ResNet18(NUM_CLASSES, regularizer)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    resnet_18_model.build(input_shape <span class="op">=</span> (<span class="va">None</span>,<span class="op">*</span>input_shape,<span class="dv">1</span>))</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    resnet_18_model.<span class="bu">compile</span>(optimizer<span class="op">=</span>SGD(learning_rate<span class="op">=</span><span class="fl">0.1</span>,momentum<span class="op">=</span><span class="fl">0.9</span>),loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>]) </span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> resnet_18_model</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>resnet18_model <span class="op">=</span> instantiate_resnet18_model(DEFAULT_SHAPE)</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>resnet18_model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:AutoGraph could not transform &lt;bound method ResnetBlock.call of &lt;__main__.ResnetBlock object at 0x0000021C2ED27BE0&gt;&gt; and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform &lt;bound method ResnetBlock.call of &lt;__main__.ResnetBlock object at 0x0000021C2ED27BE0&gt;&gt; and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Model: &quot;res_net18&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_6 (Conv2D)           multiple                  3200      
                                                                 
 batch_normalization_6 (Batc  multiple                 256       
 hNormalization)                                                 
                                                                 
 max_pooling2d_6 (MaxPooling  multiple                 0         
 2D)                                                             
                                                                 
 resnet_block (ResnetBlock)  multiple                  74368     
                                                                 
 resnet_block_1 (ResnetBlock  multiple                 74368     
 )                                                               
                                                                 
 resnet_block_2 (ResnetBlock  multiple                 231296    
 )                                                               
                                                                 
 resnet_block_3 (ResnetBlock  multiple                 296192    
 )                                                               
                                                                 
 resnet_block_4 (ResnetBlock  multiple                 921344    
 )                                                               
                                                                 
 resnet_block_5 (ResnetBlock  multiple                 1182208   
 )                                                               
                                                                 
 resnet_block_6 (ResnetBlock  multiple                 3677696   
 )                                                               
                                                                 
 resnet_block_7 (ResnetBlock  multiple                 4723712   
 )                                                               
                                                                 
 global_average_pooling2d_1   multiple                 0         
 (GlobalAveragePooling2D)                                        
                                                                 
 flatten (Flatten)           multiple                  0         
                                                                 
 dense_2 (Dense)             multiple                  7695      
                                                                 
=================================================================
Total params: 11,192,335
Trainable params: 11,182,735
Non-trainable params: 9,600
_________________________________________________________________
</code></pre>
</div>
</div>
<section id="resnet-128x128" class="cell markdown">
<h4><strong>ResNet (128x128)</strong></h4>
<p>Trained on augmented data</p>
</section>
<div class="cell code" data-execution_count="77">
<div class="sourceCode" id="cb69"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>resnet18_model <span class="op">=</span> instantiate_resnet18_model(INPUT_SHAPE_128)</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>resnet18_model_history <span class="op">=</span> resnet18_model.fit(</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb69-20"><a href="#cb69-20" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb69-21"><a href="#cb69-21" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb69-22"><a href="#cb69-22" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb69-23"><a href="#cb69-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb69-24"><a href="#cb69-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-25"><a href="#cb69-25" aria-hidden="true" tabindex="-1"></a>storeResult(resnet18_model_history, description<span class="op">=</span><span class="st">&quot;128x128&quot;</span>)</span>
<span id="cb69-26"><a href="#cb69-26" aria-hidden="true" tabindex="-1"></a>plot_history(resnet18_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 27s 88ms/step - loss: 3.1770 - accuracy: 0.1442 - val_loss: 8.8415 - val_accuracy: 0.0647 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 24s 85ms/step - loss: 2.3586 - accuracy: 0.2180 - val_loss: 3.0284 - val_accuracy: 0.0797 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 24s 86ms/step - loss: 2.0979 - accuracy: 0.3004 - val_loss: 3.6925 - val_accuracy: 0.0613 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 24s 86ms/step - loss: 1.8637 - accuracy: 0.3776 - val_loss: 5.2398 - val_accuracy: 0.0620 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 24s 86ms/step - loss: 1.7073 - accuracy: 0.4274 - val_loss: 5.0175 - val_accuracy: 0.0740 - lr: 0.0500
Epoch 6/100
283/283 [==============================] - 24s 86ms/step - loss: 1.4836 - accuracy: 0.5042 - val_loss: 8.0683 - val_accuracy: 0.0693 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 24s 86ms/step - loss: 1.3443 - accuracy: 0.5500 - val_loss: 6.5681 - val_accuracy: 0.0960 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 24s 86ms/step - loss: 1.1601 - accuracy: 0.6092 - val_loss: 14.0399 - val_accuracy: 0.0533 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 24s 86ms/step - loss: 1.0555 - accuracy: 0.6509 - val_loss: 10.8677 - val_accuracy: 0.0690 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 24s 86ms/step - loss: 0.7926 - accuracy: 0.7367 - val_loss: 12.8835 - val_accuracy: 0.0877 - lr: 0.0250
Epoch 11/100
283/283 [==============================] - 24s 86ms/step - loss: 0.7197 - accuracy: 0.7576 - val_loss: 17.6868 - val_accuracy: 0.1143 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 24s 86ms/step - loss: 0.6329 - accuracy: 0.7877 - val_loss: 15.3762 - val_accuracy: 0.0793 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 24s 86ms/step - loss: 0.6016 - accuracy: 0.7988 - val_loss: 12.3171 - val_accuracy: 0.0957 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 24s 86ms/step - loss: 0.5597 - accuracy: 0.8137 - val_loss: 17.0378 - val_accuracy: 0.0893 - lr: 0.0250
Epoch 15/100
283/283 [==============================] - 24s 86ms/step - loss: 0.5293 - accuracy: 0.8284 - val_loss: 15.0612 - val_accuracy: 0.1157 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 24s 86ms/step - loss: 0.4940 - accuracy: 0.8325 - val_loss: 11.6103 - val_accuracy: 0.1023 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 25s 87ms/step - loss: 0.4420 - accuracy: 0.8502 - val_loss: 15.0044 - val_accuracy: 0.1220 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 24s 86ms/step - loss: 0.4592 - accuracy: 0.8488 - val_loss: 22.4554 - val_accuracy: 0.0913 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 24s 86ms/step - loss: 0.3994 - accuracy: 0.8655 - val_loss: 17.8489 - val_accuracy: 0.1073 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 24s 86ms/step - loss: 0.3162 - accuracy: 0.8960 - val_loss: 20.1858 - val_accuracy: 0.0913 - lr: 0.0125
Epoch 21/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2716 - accuracy: 0.9106 - val_loss: 17.6943 - val_accuracy: 0.1150 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2554 - accuracy: 0.9153 - val_loss: 17.5853 - val_accuracy: 0.1070 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2562 - accuracy: 0.9153 - val_loss: 17.0477 - val_accuracy: 0.0603 - lr: 0.0125
Epoch 24/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2733 - accuracy: 0.9125 - val_loss: 22.6949 - val_accuracy: 0.0977 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2301 - accuracy: 0.9262 - val_loss: 18.6224 - val_accuracy: 0.0993 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 24s 86ms/step - loss: 0.2309 - accuracy: 0.9237 - val_loss: 24.4214 - val_accuracy: 0.0927 - lr: 0.0125
Epoch 27/100
283/283 [==============================] - 24s 86ms/step - loss: 0.1963 - accuracy: 0.9362 - val_loss: 16.0348 - val_accuracy: 0.0880 - lr: 0.0250
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/7468ce54e24029339e4dd84aef9a125d9f599a09.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>ResNet18 performed terribly on our 128x128 image sizes despite the
use of a small kernel size. Validation accuracy did not improve at all,
while training accuracy is sky high. In fact validation loss was
consistently increasing after 1 epoch. This tells me that larger models
may work for small images, but smaller models likely wont work with
images sizes too big for it.</p>
<p>Resnet18 has too few residual blocks to get more information from the
128x128 images, which caused it to overfit very quickly.</p>
</div>
<section id="resnet-31x31" class="cell markdown">
<h4><strong>ResNet (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="78">
<div class="sourceCode" id="cb71"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>resnet18_model <span class="op">=</span> instantiate_resnet18_model(INPUT_SHAPE_31)</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>resnet18_model_history <span class="op">=</span> resnet18_model.fit(</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>storeResult(resnet18_model_history, description<span class="op">=</span><span class="st">&quot;31x31&quot;</span>)</span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>plot_history(resnet18_model_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 14s 43ms/step - loss: 3.3681 - accuracy: 0.1490 - val_loss: 2.5861 - val_accuracy: 0.1613 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 12s 41ms/step - loss: 2.2763 - accuracy: 0.2538 - val_loss: 2.4196 - val_accuracy: 0.1897 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 12s 41ms/step - loss: 2.1156 - accuracy: 0.3161 - val_loss: 2.1690 - val_accuracy: 0.3083 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 11s 41ms/step - loss: 1.9035 - accuracy: 0.3941 - val_loss: 2.5066 - val_accuracy: 0.3107 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 11s 40ms/step - loss: 1.6323 - accuracy: 0.4821 - val_loss: 2.2986 - val_accuracy: 0.3300 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 12s 41ms/step - loss: 1.4065 - accuracy: 0.5569 - val_loss: 3.1921 - val_accuracy: 0.2200 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 12s 41ms/step - loss: 1.2104 - accuracy: 0.6103 - val_loss: 1.7041 - val_accuracy: 0.4967 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0184 - accuracy: 0.6761 - val_loss: 1.0829 - val_accuracy: 0.6663 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0967 - accuracy: 0.6520 - val_loss: 1.2094 - val_accuracy: 0.6190 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7100 - accuracy: 0.7678 - val_loss: 0.8760 - val_accuracy: 0.7233 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 11s 40ms/step - loss: 0.5483 - accuracy: 0.8201 - val_loss: 3.5539 - val_accuracy: 0.3503 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 12s 41ms/step - loss: 0.4910 - accuracy: 0.8414 - val_loss: 1.4088 - val_accuracy: 0.6077 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 11s 40ms/step - loss: 0.4119 - accuracy: 0.8655 - val_loss: 0.9651 - val_accuracy: 0.7233 - lr: 0.0250
Epoch 14/100
283/283 [==============================] - 12s 41ms/step - loss: 0.3748 - accuracy: 0.8803 - val_loss: 0.9165 - val_accuracy: 0.7467 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 12s 41ms/step - loss: 0.2880 - accuracy: 0.9027 - val_loss: 0.8419 - val_accuracy: 0.7737 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 12s 41ms/step - loss: 0.2873 - accuracy: 0.9089 - val_loss: 3.8352 - val_accuracy: 0.3727 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 12s 41ms/step - loss: 0.3110 - accuracy: 0.8998 - val_loss: 0.9317 - val_accuracy: 0.7587 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 12s 41ms/step - loss: 0.1981 - accuracy: 0.9346 - val_loss: 0.7980 - val_accuracy: 0.7947 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 12s 41ms/step - loss: 0.3023 - accuracy: 0.9061 - val_loss: 0.5935 - val_accuracy: 0.8460 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 12s 41ms/step - loss: 0.1044 - accuracy: 0.9647 - val_loss: 0.5700 - val_accuracy: 0.8473 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0677 - accuracy: 0.9790 - val_loss: 0.5094 - val_accuracy: 0.8703 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0765 - accuracy: 0.9743 - val_loss: 0.5102 - val_accuracy: 0.8770 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 11s 40ms/step - loss: 0.0627 - accuracy: 0.9808 - val_loss: 1.2551 - val_accuracy: 0.7520 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0794 - accuracy: 0.9759 - val_loss: 0.8723 - val_accuracy: 0.7980 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0649 - accuracy: 0.9787 - val_loss: 0.6319 - val_accuracy: 0.8630 - lr: 0.0125
Epoch 26/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0624 - accuracy: 0.9794 - val_loss: 1.0570 - val_accuracy: 0.7863 - lr: 0.0250
Epoch 27/100
283/283 [==============================] - 11s 41ms/step - loss: 0.0534 - accuracy: 0.9828 - val_loss: 0.9681 - val_accuracy: 0.7933 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0628 - accuracy: 0.9802 - val_loss: 0.9924 - val_accuracy: 0.8003 - lr: 0.0125
Epoch 29/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0612 - accuracy: 0.9798 - val_loss: 0.5650 - val_accuracy: 0.8713 - lr: 0.0250
Epoch 30/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0402 - accuracy: 0.9878 - val_loss: 0.5173 - val_accuracy: 0.8850 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 11s 40ms/step - loss: 0.0261 - accuracy: 0.9907 - val_loss: 0.5222 - val_accuracy: 0.8830 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 11s 40ms/step - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.5292 - val_accuracy: 0.8863 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0254 - accuracy: 0.9924 - val_loss: 0.5137 - val_accuracy: 0.8893 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 11s 40ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.5792 - val_accuracy: 0.8737 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 11s 41ms/step - loss: 0.0298 - accuracy: 0.9915 - val_loss: 0.5376 - val_accuracy: 0.8903 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0250 - accuracy: 0.9927 - val_loss: 0.9107 - val_accuracy: 0.8297 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.5999 - val_accuracy: 0.8730 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 11s 41ms/step - loss: 0.0171 - accuracy: 0.9950 - val_loss: 0.6625 - val_accuracy: 0.8677 - lr: 0.0063
Epoch 39/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.5992 - val_accuracy: 0.8823 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.5235 - val_accuracy: 0.8923 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.5236 - val_accuracy: 0.8937 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.5927 - val_accuracy: 0.8803 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 0.5167 - val_accuracy: 0.8957 - lr: 0.0063
Epoch 44/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.5498 - val_accuracy: 0.8900 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0120 - accuracy: 0.9967 - val_loss: 0.5698 - val_accuracy: 0.8850 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.5331 - val_accuracy: 0.8967 - lr: 0.0063
Epoch 47/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.5279 - val_accuracy: 0.8967 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.5458 - val_accuracy: 0.8970 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.5308 - val_accuracy: 0.8947 - lr: 0.0063
Epoch 50/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0139 - accuracy: 0.9966 - val_loss: 0.5157 - val_accuracy: 0.8970 - lr: 0.0031
Epoch 51/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 0.5138 - val_accuracy: 0.8973 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 11s 40ms/step - loss: 0.0087 - accuracy: 0.9978 - val_loss: 0.5241 - val_accuracy: 0.8947 - lr: 0.0031
Epoch 53/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.5371 - val_accuracy: 0.8917 - lr: 0.0031
Epoch 54/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.5220 - val_accuracy: 0.8953 - lr: 0.0016
Epoch 55/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0145 - accuracy: 0.9962 - val_loss: 0.5330 - val_accuracy: 0.8917 - lr: 0.0031
Epoch 56/100
283/283 [==============================] - 11s 41ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.5740 - val_accuracy: 0.8827 - lr: 0.0031
Epoch 57/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.5402 - val_accuracy: 0.8900 - lr: 0.0016
Epoch 58/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.5390 - val_accuracy: 0.8917 - lr: 0.0031
Epoch 59/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.5389 - val_accuracy: 0.8947 - lr: 0.0031
Epoch 60/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.5322 - val_accuracy: 0.8917 - lr: 7.8125e-04
Epoch 61/100
283/283 [==============================] - 12s 41ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.5310 - val_accuracy: 0.8937 - lr: 0.0016
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/861a089e48dca59845774de031d8c0276241c566.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>Compared to VGG, ResNet18 performed very similarly. However the
difference between training and validation accuracy looks larger here
than in VGG. Lets use L2 regularization to see if it helps.</p>
</div>
<section id="resnet-with-l2-31x31" class="cell markdown">
<h4><strong>ResNet With L2 (31x31)</strong></h4>
</section>
<div class="cell code" data-execution_count="79">
<div class="sourceCode" id="cb73"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>adapted_resnet50_l2 <span class="op">=</span> instantiate_resnet18_model(</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>    INPUT_SHAPE_31, regularizer<span class="op">=</span>regularizers.l2(<span class="fl">0.0001</span>)</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>adapted_resnet50_l2_augmented_history <span class="op">=</span> adapted_resnet50_l2.fit(</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a>storeResult(adapted_resnet50_l2_augmented_history, description<span class="op">=</span><span class="st">&quot;31x31 With L2&quot;</span>)</span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a>plot_history(adapted_resnet50_l2_augmented_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 15s 45ms/step - loss: 5.1911 - accuracy: 0.1891 - val_loss: 4.6330 - val_accuracy: 0.1693 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 12s 41ms/step - loss: 4.1005 - accuracy: 0.2473 - val_loss: 4.2778 - val_accuracy: 0.1787 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 12s 41ms/step - loss: 3.7107 - accuracy: 0.3181 - val_loss: 3.6172 - val_accuracy: 0.2987 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 12s 41ms/step - loss: 3.3584 - accuracy: 0.3819 - val_loss: 3.8698 - val_accuracy: 0.2700 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 12s 41ms/step - loss: 2.9639 - accuracy: 0.4689 - val_loss: 3.1013 - val_accuracy: 0.4040 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 12s 41ms/step - loss: 2.5382 - accuracy: 0.5625 - val_loss: 6.6085 - val_accuracy: 0.1657 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 12s 41ms/step - loss: 2.2670 - accuracy: 0.6262 - val_loss: 4.4568 - val_accuracy: 0.3377 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 12s 41ms/step - loss: 1.9542 - accuracy: 0.7043 - val_loss: 2.8447 - val_accuracy: 0.4903 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 12s 41ms/step - loss: 1.8466 - accuracy: 0.7212 - val_loss: 2.8675 - val_accuracy: 0.5250 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 12s 42ms/step - loss: 1.4302 - accuracy: 0.8360 - val_loss: 1.6457 - val_accuracy: 0.7820 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 12s 41ms/step - loss: 1.3555 - accuracy: 0.8536 - val_loss: 1.8026 - val_accuracy: 0.7350 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 12s 41ms/step - loss: 1.2515 - accuracy: 0.8745 - val_loss: 3.4344 - val_accuracy: 0.4517 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 12s 41ms/step - loss: 1.1887 - accuracy: 0.8865 - val_loss: 1.9728 - val_accuracy: 0.6883 - lr: 0.0250
Epoch 14/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0885 - accuracy: 0.9102 - val_loss: 2.5629 - val_accuracy: 0.5990 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0084 - accuracy: 0.9238 - val_loss: 1.5419 - val_accuracy: 0.8010 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0074 - accuracy: 0.9197 - val_loss: 4.1232 - val_accuracy: 0.5637 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0303 - accuracy: 0.9095 - val_loss: 1.4227 - val_accuracy: 0.8170 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 12s 41ms/step - loss: 1.0201 - accuracy: 0.9106 - val_loss: 1.4846 - val_accuracy: 0.8047 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 12s 41ms/step - loss: 0.8852 - accuracy: 0.9438 - val_loss: 1.4967 - val_accuracy: 0.7950 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7871 - accuracy: 0.9742 - val_loss: 1.7174 - val_accuracy: 0.7647 - lr: 0.0125
Epoch 21/100
283/283 [==============================] - 12s 42ms/step - loss: 0.7874 - accuracy: 0.9681 - val_loss: 1.4917 - val_accuracy: 0.8323 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7418 - accuracy: 0.9775 - val_loss: 2.2821 - val_accuracy: 0.6567 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7676 - accuracy: 0.9680 - val_loss: 1.4529 - val_accuracy: 0.8133 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7376 - accuracy: 0.9760 - val_loss: 1.2159 - val_accuracy: 0.8550 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 12s 41ms/step - loss: 0.7050 - accuracy: 0.9828 - val_loss: 1.4315 - val_accuracy: 0.8300 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6797 - accuracy: 0.9847 - val_loss: 1.0663 - val_accuracy: 0.8947 - lr: 0.0250
Epoch 27/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6473 - accuracy: 0.9887 - val_loss: 1.1184 - val_accuracy: 0.8793 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 12s 42ms/step - loss: 0.6696 - accuracy: 0.9793 - val_loss: 1.0705 - val_accuracy: 0.8800 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6706 - accuracy: 0.9749 - val_loss: 1.0797 - val_accuracy: 0.8853 - lr: 0.0125
Epoch 30/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6129 - accuracy: 0.9907 - val_loss: 1.0106 - val_accuracy: 0.9027 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5893 - accuracy: 0.9961 - val_loss: 0.9668 - val_accuracy: 0.9120 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5794 - accuracy: 0.9971 - val_loss: 0.9888 - val_accuracy: 0.9120 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 12s 41ms/step - loss: 0.6100 - accuracy: 0.9850 - val_loss: 0.9968 - val_accuracy: 0.9010 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5882 - accuracy: 0.9909 - val_loss: 1.0430 - val_accuracy: 0.8883 - lr: 0.0063
Epoch 35/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5734 - accuracy: 0.9927 - val_loss: 0.9811 - val_accuracy: 0.9093 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5599 - accuracy: 0.9942 - val_loss: 0.9999 - val_accuracy: 0.9043 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 12s 42ms/step - loss: 0.5501 - accuracy: 0.9952 - val_loss: 1.6563 - val_accuracy: 0.7867 - lr: 0.0063
Epoch 38/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5558 - accuracy: 0.9939 - val_loss: 1.0099 - val_accuracy: 0.8960 - lr: 0.0125
Epoch 39/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5358 - accuracy: 0.9967 - val_loss: 0.9443 - val_accuracy: 0.9110 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 12s 41ms/step - loss: 0.5273 - accuracy: 0.9978 - val_loss: 1.0148 - val_accuracy: 0.9010 - lr: 0.0031
Epoch 41/100
283/283 [==============================] - 12s 42ms/step - loss: 0.5276 - accuracy: 0.9969 - val_loss: 0.9496 - val_accuracy: 0.9107 - lr: 0.0063
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/60b72779e1ae737193e945cd5be9c8e8b6c47de6.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>L2 regularization increased the max validation accuracy to
(<code>91.2%</code>), which is the same as VGG with L2. However, the
validation accuracy results from ResNet18 are more haphazard and
flunctuating. Which is why VGG would be a better pick.</p>
<p>ResNet18 manage to achieve the same accuracy as VGG, more than
<strong>20 epochs faster</strong>. This demonstrates the work of skip
connections in ResNet, allowing the model to converge faster, and
opening doors to even deeper neural networks.</p>
</div>
<section id="model-selection" class="cell markdown">
<h2><strong>MODEL SELECTION</strong></h2>
<p>Let's pick the best performing model from the different image sizes
to be fine tuned</p>
</section>
<div class="cell code" data-execution_count="97">
<div class="sourceCode" id="cb75"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>allResults.to_pickle(<span class="st">&quot;allResults&quot;</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>allResults.sort_values(by<span class="op">=</span>[<span class="st">&quot;Val Acc&quot;</span>], ascending<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="97">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model Name</th>
      <th>Description</th>
      <th>Epochs</th>
      <th>Batch Size</th>
      <th>Train Loss</th>
      <th>Val Loss</th>
      <th>Train Acc</th>
      <th>Val Acc</th>
      <th>[Train - Val] Acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14</th>
      <td>vgg_l2</td>
      <td>128x128 with L2</td>
      <td>53</td>
      <td>32</td>
      <td>0.421230</td>
      <td>0.441748</td>
      <td>0.919251</td>
      <td>0.919000</td>
      <td>0.000251</td>
    </tr>
    <tr>
      <th>15</th>
      <td>vgg_l2</td>
      <td>31x31 with L2</td>
      <td>64</td>
      <td>32</td>
      <td>0.285923</td>
      <td>0.724862</td>
      <td>0.987262</td>
      <td>0.912333</td>
      <td>0.074929</td>
    </tr>
    <tr>
      <th>18</th>
      <td>res_net18_3</td>
      <td>31x31 With L2</td>
      <td>41</td>
      <td>32</td>
      <td>0.589346</td>
      <td>0.966771</td>
      <td>0.996123</td>
      <td>0.912000</td>
      <td>0.084123</td>
    </tr>
    <tr>
      <th>17</th>
      <td>res_net18_2</td>
      <td>31x31</td>
      <td>61</td>
      <td>32</td>
      <td>0.008974</td>
      <td>0.513809</td>
      <td>0.998006</td>
      <td>0.897333</td>
      <td>0.100673</td>
    </tr>
    <tr>
      <th>12</th>
      <td>vgg_model</td>
      <td>128x128</td>
      <td>57</td>
      <td>32</td>
      <td>0.424421</td>
      <td>0.325744</td>
      <td>0.863314</td>
      <td>0.894333</td>
      <td>-0.031019</td>
    </tr>
    <tr>
      <th>13</th>
      <td>vgg_model</td>
      <td>31x31</td>
      <td>50</td>
      <td>32</td>
      <td>0.194910</td>
      <td>0.494621</td>
      <td>0.938968</td>
      <td>0.883333</td>
      <td>0.055634</td>
    </tr>
    <tr>
      <th>5</th>
      <td>base_model</td>
      <td>31x31 imbalanced</td>
      <td>35</td>
      <td>32</td>
      <td>0.064733</td>
      <td>0.905352</td>
      <td>0.993553</td>
      <td>0.786290</td>
      <td>0.207262</td>
    </tr>
    <tr>
      <th>6</th>
      <td>base_model</td>
      <td>128x128 augmented</td>
      <td>35</td>
      <td>32</td>
      <td>0.064733</td>
      <td>0.905352</td>
      <td>0.993553</td>
      <td>0.786290</td>
      <td>0.207262</td>
    </tr>
    <tr>
      <th>1</th>
      <td>base_model</td>
      <td>31x31 oversampled</td>
      <td>21</td>
      <td>32</td>
      <td>0.376571</td>
      <td>0.929273</td>
      <td>0.892325</td>
      <td>0.741599</td>
      <td>0.150725</td>
    </tr>
    <tr>
      <th>10</th>
      <td>alexnet</td>
      <td>128x128</td>
      <td>80</td>
      <td>32</td>
      <td>1.090381</td>
      <td>0.911680</td>
      <td>0.642397</td>
      <td>0.693884</td>
      <td>-0.051488</td>
    </tr>
    <tr>
      <th>3</th>
      <td>base_model</td>
      <td>31x31 undersampled</td>
      <td>47</td>
      <td>32</td>
      <td>0.146421</td>
      <td>1.272373</td>
      <td>0.977766</td>
      <td>0.686492</td>
      <td>0.291274</td>
    </tr>
    <tr>
      <th>0</th>
      <td>base_model</td>
      <td>128x128 oversampled</td>
      <td>28</td>
      <td>32</td>
      <td>0.000860</td>
      <td>2.994822</td>
      <td>1.000000</td>
      <td>0.631048</td>
      <td>0.368952</td>
    </tr>
    <tr>
      <th>4</th>
      <td>base_model</td>
      <td>128x128 imbalanced</td>
      <td>26</td>
      <td>32</td>
      <td>0.008087</td>
      <td>3.998235</td>
      <td>0.999444</td>
      <td>0.596102</td>
      <td>0.403342</td>
    </tr>
    <tr>
      <th>2</th>
      <td>base_model</td>
      <td>128x128 undersampled</td>
      <td>15</td>
      <td>32</td>
      <td>0.981946</td>
      <td>1.703261</td>
      <td>0.709870</td>
      <td>0.520833</td>
      <td>0.189037</td>
    </tr>
    <tr>
      <th>7</th>
      <td>base_model</td>
      <td>31x31 augmented</td>
      <td>38</td>
      <td>32</td>
      <td>1.652369</td>
      <td>1.647656</td>
      <td>0.466985</td>
      <td>0.487231</td>
      <td>-0.020246</td>
    </tr>
    <tr>
      <th>16</th>
      <td>res_net18_1</td>
      <td>128x128</td>
      <td>27</td>
      <td>32</td>
      <td>0.441967</td>
      <td>15.004394</td>
      <td>0.850244</td>
      <td>0.122000</td>
      <td>0.728244</td>
    </tr>
    <tr>
      <th>9</th>
      <td>alexnet</td>
      <td>128x128</td>
      <td>11</td>
      <td>32</td>
      <td>2.651931</td>
      <td>2.769775</td>
      <td>0.096932</td>
      <td>0.073925</td>
      <td>0.023007</td>
    </tr>
    <tr>
      <th>11</th>
      <td>alexnet</td>
      <td>31x31</td>
      <td>15</td>
      <td>32</td>
      <td>2.637116</td>
      <td>2.778830</td>
      <td>0.103824</td>
      <td>0.067204</td>
      <td>0.036620</td>
    </tr>
    <tr>
      <th>8</th>
      <td>alexnet</td>
      <td>128x128</td>
      <td>15</td>
      <td>32</td>
      <td>2.636989</td>
      <td>2.792042</td>
      <td>0.103713</td>
      <td>0.067204</td>
      <td>0.036508</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell markdown">
<p>VGG with L2 achieved the highest validation accuracy for both 128x128
and 31x31 image sizes</p>
</div>
<section id="model-improvement" class="cell markdown">
<h2><strong>MODEL IMPROVEMENT</strong></h2>
<p>Best 128x128 model = <strong>VGG With L2 (128x128)</strong></p>
<p>Best 31x31 model = <strong>VGG With L2 (31x31)</strong></p>
</section>
<section id="vgghypermodel" class="cell markdown">
<h3>VGGHyperModel</h3>
</section>
<section id="defining-the-model-with-hyperparameters"
class="cell markdown">
<h5><strong>Defining the Model with Hyperparameters</strong></h5>
<p>Hyperparameters under tuning</p>
<ol>
<li>Number of Convolutional Blocks</li>
<li>Number of Filters per Convolutional Block</li>
<li>Kernel Size in Convolutional Layers</li>
<li>Batch Normalization</li>
<li>Number of Dense Layers</li>
<li>Units in Dense Layers</li>
<li>Activation Function in Dense Layers</li>
<li>L2 Regularization in Dense Layers</li>
<li>Dropout Rate</li>
<li>Optimizer Type</li>
<li>Learning Rate</li>
</ol>
</section>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VGGHyperModel(HyperModel):</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_shape, num_classes, <span class="op">**</span>kwargs):</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_shape <span class="op">=</span> input_shape</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_classes <span class="op">=</span> num_classes</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, hp):</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> Input((<span class="op">*</span><span class="va">self</span>.input_shape, <span class="dv">1</span>))</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> inputs</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tuning the number of convolutional blocks</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>        num_blocks <span class="op">=</span> hp.Int(<span class="st">&#39;num_blocks&#39;</span>, min_value<span class="op">=</span><span class="dv">3</span>, max_value<span class="op">=</span><span class="dv">4</span>, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block_num <span class="kw">in</span> <span class="bu">range</span>(num_blocks):</span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Define num_filters inside the loop</span></span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>            num_filters <span class="op">=</span> hp.Int(</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;num_filters_block_&quot;</span> <span class="op">+</span> <span class="bu">str</span>(block_num),</span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a>                min_value<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb76-18"><a href="#cb76-18" aria-hidden="true" tabindex="-1"></a>                max_value<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb76-19"><a href="#cb76-19" aria-hidden="true" tabindex="-1"></a>                step<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb76-20"><a href="#cb76-20" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb76-21"><a href="#cb76-21" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> vgg_block_l2(</span>
<span id="cb76-22"><a href="#cb76-22" aria-hidden="true" tabindex="-1"></a>                hp.Choice(<span class="st">&#39;kernel_size&#39;</span>, values<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">5</span>]),</span>
<span id="cb76-23"><a href="#cb76-23" aria-hidden="true" tabindex="-1"></a>                num_filters</span>
<span id="cb76-24"><a href="#cb76-24" aria-hidden="true" tabindex="-1"></a>            )(x)</span>
<span id="cb76-25"><a href="#cb76-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> hp.Boolean(<span class="st">&#39;use_batch_norm&#39;</span>):</span>
<span id="cb76-26"><a href="#cb76-26" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> BatchNormalization()(x)</span>
<span id="cb76-27"><a href="#cb76-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-28"><a href="#cb76-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb76-29"><a href="#cb76-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-30"><a href="#cb76-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tuning the number of dense layers</span></span>
<span id="cb76-31"><a href="#cb76-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(hp.Int(<span class="st">&#39;num_dense_layers&#39;</span>, <span class="dv">1</span>, <span class="dv">3</span>)):</span>
<span id="cb76-32"><a href="#cb76-32" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> Dense(</span>
<span id="cb76-33"><a href="#cb76-33" aria-hidden="true" tabindex="-1"></a>                hp.Int(<span class="st">&#39;dense_units&#39;</span>, min_value<span class="op">=</span><span class="dv">64</span>, max_value<span class="op">=</span><span class="dv">512</span>, step<span class="op">=</span><span class="dv">64</span>), </span>
<span id="cb76-34"><a href="#cb76-34" aria-hidden="true" tabindex="-1"></a>                activation<span class="op">=</span>hp.Choice(<span class="st">&#39;dense_activation&#39;</span>, values<span class="op">=</span>[<span class="st">&#39;relu&#39;</span>, <span class="st">&#39;tanh&#39;</span>, <span class="st">&#39;elu&#39;</span>]),</span>
<span id="cb76-35"><a href="#cb76-35" aria-hidden="true" tabindex="-1"></a>                kernel_regularizer<span class="op">=</span>tf.keras.regularizers.l2(</span>
<span id="cb76-36"><a href="#cb76-36" aria-hidden="true" tabindex="-1"></a>                    hp.Float(<span class="st">&#39;l2_regularization&#39;</span>, min_value<span class="op">=</span><span class="fl">1e-4</span>, max_value<span class="op">=</span><span class="fl">1e-2</span>, sampling<span class="op">=</span><span class="st">&#39;LOG&#39;</span>)</span>
<span id="cb76-37"><a href="#cb76-37" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb76-38"><a href="#cb76-38" aria-hidden="true" tabindex="-1"></a>            )(x)</span>
<span id="cb76-39"><a href="#cb76-39" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> Dropout(hp.Float(<span class="st">&#39;dropout_rate&#39;</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, step<span class="op">=</span><span class="fl">0.1</span>))(x)</span>
<span id="cb76-40"><a href="#cb76-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-41"><a href="#cb76-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> Dense(<span class="va">self</span>.num_classes, activation<span class="op">=</span><span class="st">&quot;softmax&quot;</span>)(x)</span>
<span id="cb76-42"><a href="#cb76-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-43"><a href="#cb76-43" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>x, name<span class="op">=</span><span class="st">&quot;vgg_l2&quot;</span>)</span>
<span id="cb76-44"><a href="#cb76-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-45"><a href="#cb76-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tuning the optimizer</span></span>
<span id="cb76-46"><a href="#cb76-46" aria-hidden="true" tabindex="-1"></a>        optimizer_choice <span class="op">=</span> hp.Choice(<span class="st">&#39;optimizer&#39;</span>, [<span class="st">&#39;adam&#39;</span>, <span class="st">&#39;sgd&#39;</span>, <span class="st">&#39;rmsprop&#39;</span>])</span>
<span id="cb76-47"><a href="#cb76-47" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> hp.Float(<span class="st">&quot;learning_rate&quot;</span>, min_value<span class="op">=</span><span class="fl">1e-8</span>, max_value<span class="op">=</span><span class="fl">1e-2</span>, sampling<span class="op">=</span><span class="st">&quot;LOG&quot;</span>)</span>
<span id="cb76-48"><a href="#cb76-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> optimizer_choice <span class="op">==</span> <span class="st">&#39;adam&#39;</span>:</span>
<span id="cb76-49"><a href="#cb76-49" aria-hidden="true" tabindex="-1"></a>            optimizer <span class="op">=</span> Adam(learning_rate<span class="op">=</span>lr)</span>
<span id="cb76-50"><a href="#cb76-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> optimizer_choice <span class="op">==</span> <span class="st">&#39;sgd&#39;</span>:</span>
<span id="cb76-51"><a href="#cb76-51" aria-hidden="true" tabindex="-1"></a>            optimizer <span class="op">=</span> SGD(learning_rate<span class="op">=</span>lr, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb76-52"><a href="#cb76-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb76-53"><a href="#cb76-53" aria-hidden="true" tabindex="-1"></a>            optimizer <span class="op">=</span> RMSprop(learning_rate<span class="op">=</span>lr)</span>
<span id="cb76-54"><a href="#cb76-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-55"><a href="#cb76-55" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">compile</span>(</span>
<span id="cb76-56"><a href="#cb76-56" aria-hidden="true" tabindex="-1"></a>            optimizer<span class="op">=</span>optimizer,</span>
<span id="cb76-57"><a href="#cb76-57" aria-hidden="true" tabindex="-1"></a>            loss<span class="op">=</span><span class="st">&quot;categorical_crossentropy&quot;</span>,</span>
<span id="cb76-58"><a href="#cb76-58" aria-hidden="true" tabindex="-1"></a>            metrics<span class="op">=</span>[<span class="st">&quot;accuracy&quot;</span>],</span>
<span id="cb76-59"><a href="#cb76-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-60"><a href="#cb76-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb76-61"><a href="#cb76-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-62"><a href="#cb76-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> model_summary(<span class="va">self</span>):</span>
<span id="cb76-63"><a href="#cb76-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fixed hyperparameters to print example summary</span></span>
<span id="cb76-64"><a href="#cb76-64" aria-hidden="true" tabindex="-1"></a>        hp <span class="op">=</span> HyperParameters()</span>
<span id="cb76-65"><a href="#cb76-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add fixed values for the new hyperparameters</span></span>
<span id="cb76-66"><a href="#cb76-66" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;num_blocks&#39;</span>, value<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb76-67"><a href="#cb76-67" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;num_dense_layers&#39;</span>, value<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb76-68"><a href="#cb76-68" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;dense_units&#39;</span>, value<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb76-69"><a href="#cb76-69" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;dense_activation&#39;</span>, value<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</span>
<span id="cb76-70"><a href="#cb76-70" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;l2_regularization&#39;</span>, value<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb76-71"><a href="#cb76-71" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;dropout_rate&#39;</span>, value<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb76-72"><a href="#cb76-72" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;optimizer&#39;</span>, value<span class="op">=</span><span class="st">&#39;adam&#39;</span>)</span>
<span id="cb76-73"><a href="#cb76-73" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;learning_rate&#39;</span>, value<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb76-74"><a href="#cb76-74" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;kernel_size&#39;</span>, value<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb76-75"><a href="#cb76-75" aria-hidden="true" tabindex="-1"></a>        hp.Fixed(<span class="st">&#39;use_batch_norm&#39;</span>, value<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-76"><a href="#cb76-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Existing fixed hyperparameters</span></span>
<span id="cb76-77"><a href="#cb76-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block_num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):  <span class="co"># Assuming 3 blocks as fixed value</span></span>
<span id="cb76-78"><a href="#cb76-78" aria-hidden="true" tabindex="-1"></a>            hp.Fixed(<span class="st">&#39;num_filters_block_&#39;</span> <span class="op">+</span> <span class="bu">str</span>(block_num), value<span class="op">=</span><span class="dv">16</span> <span class="op">+</span> block_num <span class="op">*</span> <span class="dv">16</span>)</span>
<span id="cb76-79"><a href="#cb76-79" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> <span class="va">self</span>.build(hp)</span>
<span id="cb76-80"><a href="#cb76-80" aria-hidden="true" tabindex="-1"></a>        model.summary()</span></code></pre></div>
</div>
<section id="best-model-128x128" class="cell markdown">
<h2><strong>Best Model 128x128</strong></h2>
</section>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the hypermodel</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>hypermodel <span class="op">=</span> VGGHyperModel(input_shape<span class="op">=</span>INPUT_SHAPE_128, num_classes<span class="op">=</span>NUM_CLASSES)</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the tuner</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>model_128x128_tuner <span class="op">=</span> RandomSearch(</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    hypermodel,</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    objective<span class="op">=</span><span class="st">&#39;val_accuracy&#39;</span>,</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    max_trials<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Number of trials to run</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    executions_per_trial<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    directory<span class="op">=</span><span class="st">&#39;hyperparameter_dir&#39;</span>,  <span class="co"># Directory to save logs and models</span></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">&#39;vgg_tuning_128&#39;</span></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-26"><a href="#cb77-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the hyperparameter search</span></span>
<span id="cb77-27"><a href="#cb77-27" aria-hidden="true" tabindex="-1"></a>K.clear_session()</span>
<span id="cb77-28"><a href="#cb77-28" aria-hidden="true" tabindex="-1"></a>model_128x128_tuner.search(train_gen, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>valid_gen, callbacks<span class="op">=</span>[early_stop, reduce_lr, model_checkpoint(<span class="st">&#39;best_128_weights.h5&#39;</span>)])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Trial 10 Complete [02h 51m 15s]
val_accuracy: 0.06866666674613953

Best val_accuracy So Far: 0.45399999618530273
Total elapsed time: 1d 12h 35m 43s
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="16">
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>tf.get_logger().setLevel(<span class="st">&quot;ERROR&quot;</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the optimal hyperparameters and model</span></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>best_hps <span class="op">=</span> model_128x128_tuner.get_best_hyperparameters(num_trials<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>best_model_128 <span class="op">=</span> model_128x128_tuner.get_best_models()[<span class="dv">0</span>]</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>best_model_128.save(<span class="st">&#39;best_model_128.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>train_gen.reset()</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>valid_gen.reset()</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>    test_data,</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>best_model_128_history <span class="op">=</span> best_model_128.fit(</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a>best_model_128.save(<span class="st">&#39;best_model_128.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> best_model_128.evaluate(test_gen)</span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test Accuracy: </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb79-36"><a href="#cb79-36" aria-hidden="true" tabindex="-1"></a>plot_history(best_model_128_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 293s 1s/step - loss: 98.9114 - accuracy: 0.0762 - val_loss: 94.6637 - val_accuracy: 0.0617 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 276s 975ms/step - loss: 100.9999 - accuracy: 0.0749 - val_loss: 107.5512 - val_accuracy: 0.0667 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 276s 975ms/step - loss: 93.6103 - accuracy: 0.0720 - val_loss: 93.1570 - val_accuracy: 0.0667 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 276s 974ms/step - loss: 91.1001 - accuracy: 0.0762 - val_loss: 85.6973 - val_accuracy: 0.0667 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 280s 991ms/step - loss: 92.6721 - accuracy: 0.0776 - val_loss: 112.5749 - val_accuracy: 0.0667 - lr: 0.0500
Epoch 6/100
283/283 [==============================] - 288s 1s/step - loss: 96.7381 - accuracy: 0.0768 - val_loss: 92.2501 - val_accuracy: 0.0920 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 326s 1s/step - loss: 99.1463 - accuracy: 0.0816 - val_loss: 120.2414 - val_accuracy: 0.0667 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 336s 1s/step - loss: 105.3404 - accuracy: 0.0785 - val_loss: 87.9032 - val_accuracy: 0.0620 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 304s 1s/step - loss: 90.8491 - accuracy: 0.0775 - val_loss: 83.5697 - val_accuracy: 0.0697 - lr: 0.0500
Epoch 10/100
283/283 [==============================] - 316s 1s/step - loss: 46.1032 - accuracy: 0.0806 - val_loss: 35.9914 - val_accuracy: 0.0667 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 317s 1s/step - loss: 43.1036 - accuracy: 0.0774 - val_loss: 36.6362 - val_accuracy: 0.0667 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 304s 1s/step - loss: 44.2510 - accuracy: 0.0769 - val_loss: 38.2319 - val_accuracy: 0.0667 - lr: 0.0250
Epoch 13/100
283/283 [==============================] - 321s 1s/step - loss: 41.4014 - accuracy: 0.0765 - val_loss: 40.5826 - val_accuracy: 0.0730 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 322s 1s/step - loss: 40.3818 - accuracy: 0.0750 - val_loss: 37.9465 - val_accuracy: 0.0663 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 323s 1s/step - loss: 38.6837 - accuracy: 0.0779 - val_loss: 38.0135 - val_accuracy: 0.0667 - lr: 0.0250
Epoch 16/100
283/283 [==============================] - 324s 1s/step - loss: 36.4231 - accuracy: 0.0742 - val_loss: 33.0573 - val_accuracy: 0.0667 - lr: 0.0500
94/94 [==============================] - 55s 591ms/step - loss: 92.2376 - accuracy: 0.0857
Test Accuracy: 0.08566666394472122
Test Loss: 92.23759460449219
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/c7607676c60136c7e6ef062bbe12c85b23858d2d.png" /></p>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>test_gen.reset()</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>best_model_128_loaded <span class="op">=</span> load_model(<span class="st">&#39;best_model_128.keras&#39;</span>)</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>best_model_128_loaded.evaluate(test_gen)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>94/94 [==============================] - 4s 40ms/step - loss: 4.9619 - accuracy: 0.0667
</code></pre>
</div>
<div class="output execute_result" data-execution_count="28">
<pre><code>[4.961916446685791, 0.06666667014360428]</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Results from hyperparameter tuning were terrible, likely due to the
large number of hyperparameters there was to tune, and too little number
of trials to cover the best ones. Lets take the model before
hyperparameter tuning instead.</p>
</div>
<section id="rebuild-128x128-best-model" class="cell markdown">
<h3>Rebuild 128x128 best model</h3>
</section>
<div class="cell code" data-execution_count="52">
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_augment.flow_from_dataframe(</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_128)</span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>vgg_l2 <span class="op">=</span> instantiate_vgg_l2_model(INPUT_SHAPE_128)</span>
<span id="cb84-14"><a href="#cb84-14" aria-hidden="true" tabindex="-1"></a>vgg_l2_history <span class="op">=</span> vgg_l2.fit(</span>
<span id="cb84-15"><a href="#cb84-15" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb84-16"><a href="#cb84-16" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb84-17"><a href="#cb84-17" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb84-18"><a href="#cb84-18" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb84-19"><a href="#cb84-19" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb84-20"><a href="#cb84-20" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb84-21"><a href="#cb84-21" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb84-22"><a href="#cb84-22" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb84-23"><a href="#cb84-23" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb84-24"><a href="#cb84-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb84-25"><a href="#cb84-25" aria-hidden="true" tabindex="-1"></a>storeResult(vgg_l2_history, description<span class="op">=</span><span class="st">&quot;128x128 with L2&quot;</span>)</span>
<span id="cb84-26"><a href="#cb84-26" aria-hidden="true" tabindex="-1"></a>plot_history(vgg_l2_history)</span>
<span id="cb84-27"><a href="#cb84-27" aria-hidden="true" tabindex="-1"></a>vgg_l2.save(<span class="st">&#39;best_model_128.keras&#39;</span>) <span class="co"># Saving best model</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 28s 97ms/step - loss: 2.4434 - accuracy: 0.2185 - val_loss: 22.1164 - val_accuracy: 0.0667 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 27s 97ms/step - loss: 2.0563 - accuracy: 0.3367 - val_loss: 15.5668 - val_accuracy: 0.0740 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 27s 95ms/step - loss: 1.8885 - accuracy: 0.4061 - val_loss: 2.0388 - val_accuracy: 0.3783 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 27s 97ms/step - loss: 1.8039 - accuracy: 0.4521 - val_loss: 6.2621 - val_accuracy: 0.1873 - lr: 0.1000
Epoch 5/100
283/283 [==============================] - 27s 96ms/step - loss: 1.7138 - accuracy: 0.4825 - val_loss: 5.8137 - val_accuracy: 0.1860 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 28s 97ms/step - loss: 1.6381 - accuracy: 0.5127 - val_loss: 1.7452 - val_accuracy: 0.5290 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 27s 96ms/step - loss: 1.6497 - accuracy: 0.5222 - val_loss: 2.3845 - val_accuracy: 0.4540 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 27s 96ms/step - loss: 1.6859 - accuracy: 0.5189 - val_loss: 2.3711 - val_accuracy: 0.4590 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 27s 96ms/step - loss: 1.5724 - accuracy: 0.5626 - val_loss: 1.8716 - val_accuracy: 0.5223 - lr: 0.0500
Epoch 10/100
283/283 [==============================] - 27s 96ms/step - loss: 1.2075 - accuracy: 0.6817 - val_loss: 2.7168 - val_accuracy: 0.3933 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 27s 96ms/step - loss: 1.1002 - accuracy: 0.7066 - val_loss: 3.2759 - val_accuracy: 0.3970 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 27s 95ms/step - loss: 1.0778 - accuracy: 0.7184 - val_loss: 2.8469 - val_accuracy: 0.3950 - lr: 0.0250
Epoch 13/100
283/283 [==============================] - 27s 96ms/step - loss: 1.0426 - accuracy: 0.7281 - val_loss: 0.9889 - val_accuracy: 0.7263 - lr: 0.0500
Epoch 14/100
283/283 [==============================] - 27s 97ms/step - loss: 1.0372 - accuracy: 0.7315 - val_loss: 1.5942 - val_accuracy: 0.6137 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 27s 96ms/step - loss: 0.9686 - accuracy: 0.7537 - val_loss: 0.9691 - val_accuracy: 0.7600 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 27s 97ms/step - loss: 0.9638 - accuracy: 0.7606 - val_loss: 1.2921 - val_accuracy: 0.6763 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 27s 96ms/step - loss: 0.9741 - accuracy: 0.7580 - val_loss: 5.6602 - val_accuracy: 0.2470 - lr: 0.0500
Epoch 18/100
283/283 [==============================] - 27s 95ms/step - loss: 0.9357 - accuracy: 0.7692 - val_loss: 2.4374 - val_accuracy: 0.4510 - lr: 0.0250
Epoch 19/100
283/283 [==============================] - 27s 97ms/step - loss: 0.8885 - accuracy: 0.7839 - val_loss: 1.3359 - val_accuracy: 0.6727 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 27s 96ms/step - loss: 0.7709 - accuracy: 0.8176 - val_loss: 1.0423 - val_accuracy: 0.7390 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 27s 96ms/step - loss: 0.7092 - accuracy: 0.8345 - val_loss: 1.7404 - val_accuracy: 0.5647 - lr: 0.0125
Epoch 22/100
283/283 [==============================] - 27s 97ms/step - loss: 0.7008 - accuracy: 0.8387 - val_loss: 1.5881 - val_accuracy: 0.6067 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 27s 96ms/step - loss: 0.6732 - accuracy: 0.8442 - val_loss: 0.6621 - val_accuracy: 0.8410 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 28s 98ms/step - loss: 0.6583 - accuracy: 0.8487 - val_loss: 0.9234 - val_accuracy: 0.7617 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 27s 97ms/step - loss: 0.6531 - accuracy: 0.8478 - val_loss: 1.1600 - val_accuracy: 0.6953 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 27s 96ms/step - loss: 0.6423 - accuracy: 0.8540 - val_loss: 2.5503 - val_accuracy: 0.4883 - lr: 0.0125
Epoch 27/100
283/283 [==============================] - 27s 95ms/step - loss: 0.6231 - accuracy: 0.8574 - val_loss: 0.6966 - val_accuracy: 0.8350 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 27s 95ms/step - loss: 0.6168 - accuracy: 0.8590 - val_loss: 1.0135 - val_accuracy: 0.7747 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 27s 96ms/step - loss: 0.6204 - accuracy: 0.8568 - val_loss: 1.0485 - val_accuracy: 0.7637 - lr: 0.0125
Epoch 30/100
283/283 [==============================] - 27s 96ms/step - loss: 0.6062 - accuracy: 0.8661 - val_loss: 1.0615 - val_accuracy: 0.7380 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 27s 97ms/step - loss: 0.5331 - accuracy: 0.8888 - val_loss: 1.3714 - val_accuracy: 0.6710 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 27s 96ms/step - loss: 0.5118 - accuracy: 0.8909 - val_loss: 0.9625 - val_accuracy: 0.7757 - lr: 0.0063
Epoch 33/100
283/283 [==============================] - 27s 96ms/step - loss: 0.5262 - accuracy: 0.8895 - val_loss: 0.6198 - val_accuracy: 0.8527 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 27s 96ms/step - loss: 0.5081 - accuracy: 0.8937 - val_loss: 1.6286 - val_accuracy: 0.6440 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 27s 97ms/step - loss: 0.4894 - accuracy: 0.8965 - val_loss: 0.4870 - val_accuracy: 0.9013 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 27s 97ms/step - loss: 0.4940 - accuracy: 0.8984 - val_loss: 0.6517 - val_accuracy: 0.8430 - lr: 0.0125
Epoch 37/100
283/283 [==============================] - 27s 96ms/step - loss: 0.4621 - accuracy: 0.9056 - val_loss: 0.5476 - val_accuracy: 0.8847 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 28s 98ms/step - loss: 0.4675 - accuracy: 0.9016 - val_loss: 0.8038 - val_accuracy: 0.8123 - lr: 0.0063
Epoch 39/100
283/283 [==============================] - 28s 98ms/step - loss: 0.4755 - accuracy: 0.9026 - val_loss: 1.3549 - val_accuracy: 0.6660 - lr: 0.0125
Epoch 40/100
283/283 [==============================] - 28s 100ms/step - loss: 0.4520 - accuracy: 0.9085 - val_loss: 0.4638 - val_accuracy: 0.9090 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 28s 97ms/step - loss: 0.4364 - accuracy: 0.9124 - val_loss: 0.7893 - val_accuracy: 0.8173 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 27s 97ms/step - loss: 0.4206 - accuracy: 0.9158 - val_loss: 1.1296 - val_accuracy: 0.7557 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 27s 96ms/step - loss: 0.4165 - accuracy: 0.9189 - val_loss: 0.9796 - val_accuracy: 0.7903 - lr: 0.0031
Epoch 44/100
283/283 [==============================] - 27s 95ms/step - loss: 0.4241 - accuracy: 0.9183 - val_loss: 0.6369 - val_accuracy: 0.8553 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 27s 95ms/step - loss: 0.4155 - accuracy: 0.9184 - val_loss: 0.6160 - val_accuracy: 0.8590 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 27s 96ms/step - loss: 0.4142 - accuracy: 0.9193 - val_loss: 0.8038 - val_accuracy: 0.8150 - lr: 0.0031
Epoch 47/100
283/283 [==============================] - 28s 99ms/step - loss: 0.4066 - accuracy: 0.9209 - val_loss: 0.6013 - val_accuracy: 0.8660 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3976 - accuracy: 0.9249 - val_loss: 0.6303 - val_accuracy: 0.8530 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 27s 95ms/step - loss: 0.4053 - accuracy: 0.9198 - val_loss: 1.2048 - val_accuracy: 0.7413 - lr: 0.0031
Epoch 50/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3785 - accuracy: 0.9293 - val_loss: 0.4186 - val_accuracy: 0.9167 - lr: 0.0031
Epoch 51/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3921 - accuracy: 0.9211 - val_loss: 0.6742 - val_accuracy: 0.8473 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3859 - accuracy: 0.9250 - val_loss: 0.5897 - val_accuracy: 0.8680 - lr: 0.0031
Epoch 53/100
283/283 [==============================] - 27s 95ms/step - loss: 0.3767 - accuracy: 0.9294 - val_loss: 0.4858 - val_accuracy: 0.8970 - lr: 0.0016
Epoch 54/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3770 - accuracy: 0.9238 - val_loss: 0.5875 - val_accuracy: 0.8663 - lr: 0.0031
Epoch 55/100
283/283 [==============================] - 27s 95ms/step - loss: 0.3711 - accuracy: 0.9304 - val_loss: 1.2809 - val_accuracy: 0.7163 - lr: 0.0031
Epoch 56/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3725 - accuracy: 0.9269 - val_loss: 0.8198 - val_accuracy: 0.8073 - lr: 0.0016
Epoch 57/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3736 - accuracy: 0.9297 - val_loss: 0.5290 - val_accuracy: 0.8810 - lr: 0.0031
Epoch 58/100
283/283 [==============================] - 27s 95ms/step - loss: 0.3689 - accuracy: 0.9311 - val_loss: 0.5411 - val_accuracy: 0.8790 - lr: 0.0031
Epoch 59/100
283/283 [==============================] - 27s 96ms/step - loss: 0.3663 - accuracy: 0.9330 - val_loss: 0.5497 - val_accuracy: 0.8800 - lr: 0.0016
Epoch 60/100
283/283 [==============================] - 27s 97ms/step - loss: 0.3694 - accuracy: 0.9318 - val_loss: 0.5907 - val_accuracy: 0.8677 - lr: 0.0016
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/2af1c31976ee68708258d18bf57795043e8e9be7.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="58">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>test_gen.reset()</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>best_model_128_loaded <span class="op">=</span> load_model(<span class="st">&#39;best_model_128.keras&#39;</span>)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>best_model_128_loaded.evaluate(test_gen)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>94/94 [==============================] - 4s 43ms/step - loss: 0.4341 - accuracy: 0.9180
</code></pre>
</div>
<div class="output execute_result" data-execution_count="58">
<pre><code>[0.4340760111808777, 0.9179999828338623]</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Test accuracy was higher than max validation accuracy</p>
</div>
<section id="best-model-31x31" class="cell markdown">
<h2><strong>Best Model 31x31</strong></h2>
</section>
<div class="cell code" data-execution_count="25">
<div class="sourceCode" id="cb89"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the hypermodel</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>hypermodel <span class="op">=</span> VGGHyperModel(input_shape<span class="op">=</span>INPUT_SHAPE_31, num_classes<span class="op">=</span>NUM_CLASSES)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the tuner</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>model_31x31_tuner <span class="op">=</span> RandomSearch(</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>    hypermodel,</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>    objective<span class="op">=</span><span class="st">&#39;val_accuracy&#39;</span>,</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>    max_trials<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Number of trials to run</span></span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>    executions_per_trial<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>    directory<span class="op">=</span><span class="st">&#39;hyperparameter_dir&#39;</span>,  <span class="co"># Directory to save logs and models</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>    project_name<span class="op">=</span><span class="st">&#39;vgg_tuning_31&#39;</span></span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb89-20"><a href="#cb89-20" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb89-21"><a href="#cb89-21" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb89-22"><a href="#cb89-22" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb89-23"><a href="#cb89-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb89-24"><a href="#cb89-24" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb89-25"><a href="#cb89-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-26"><a href="#cb89-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the hyperparameter search</span></span>
<span id="cb89-27"><a href="#cb89-27" aria-hidden="true" tabindex="-1"></a>model_31x31_tuner.search(train_gen, epochs<span class="op">=</span><span class="dv">20</span>, validation_data<span class="op">=</span>valid_gen, callbacks<span class="op">=</span>[early_stop, reduce_lr, model_checkpoint(<span class="st">&#39;best_31_weights&#39;</span>)])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Trial 10 Complete [00h 06m 26s]
val_accuracy: 0.18000000715255737

Best val_accuracy So Far: 0.7643333077430725
Total elapsed time: 00h 55m 06s
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb91"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>tf.get_logger().setLevel(<span class="st">&quot;ERROR&quot;</span>)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the optimal hyperparameters and model</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>best_hps <span class="op">=</span> model_31x31_tuner.get_best_hyperparameters(num_trials<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>best_model_31 <span class="op">=</span> model_31x31_tuner.get_best_models()[<span class="dv">0</span>]</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>best_model_31.save(<span class="st">&#39;best_model_31.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>train_gen <span class="op">=</span> train_gen_no_augment.flow_from_dataframe(</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>    train_data,</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>    test_data,</span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>valid_gen <span class="op">=</span> validation_generator(INPUT_SHAPE_31)</span>
<span id="cb91-29"><a href="#cb91-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-30"><a href="#cb91-30" aria-hidden="true" tabindex="-1"></a>best_model_31_history <span class="op">=</span> best_model_31.fit(</span>
<span id="cb91-31"><a href="#cb91-31" aria-hidden="true" tabindex="-1"></a>    train_gen,</span>
<span id="cb91-32"><a href="#cb91-32" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb91-33"><a href="#cb91-33" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_gen,</span>
<span id="cb91-34"><a href="#cb91-34" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb91-35"><a href="#cb91-35" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb91-36"><a href="#cb91-36" aria-hidden="true" tabindex="-1"></a>        early_stop,</span>
<span id="cb91-37"><a href="#cb91-37" aria-hidden="true" tabindex="-1"></a>        reduce_lr,</span>
<span id="cb91-38"><a href="#cb91-38" aria-hidden="true" tabindex="-1"></a>        lr_scheduler,</span>
<span id="cb91-39"><a href="#cb91-39" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb91-40"><a href="#cb91-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb91-41"><a href="#cb91-41" aria-hidden="true" tabindex="-1"></a>best_model_31.save(<span class="st">&#39;best_model_31.keras&#39;</span>) <span class="co"># Saving best model</span></span>
<span id="cb91-42"><a href="#cb91-42" aria-hidden="true" tabindex="-1"></a>test_loss, test_acc <span class="op">=</span> best_model_31.evaluate(test_gen)</span>
<span id="cb91-43"><a href="#cb91-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test Accuracy: </span><span class="sc">{</span>test_acc<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb91-44"><a href="#cb91-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test Loss: </span><span class="sc">{</span>test_loss<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb91-45"><a href="#cb91-45" aria-hidden="true" tabindex="-1"></a>plot_history(best_model_31_history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 9028 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Found 3000 validated image filenames belonging to 15 classes.
Epoch 1/100
283/283 [==============================] - 18s 57ms/step - loss: 3.0108 - accuracy: 0.1856 - val_loss: 3.2783 - val_accuracy: 0.1630 - lr: 0.1000
Epoch 2/100
283/283 [==============================] - 15s 54ms/step - loss: 2.7883 - accuracy: 0.2385 - val_loss: 2.9390 - val_accuracy: 0.1487 - lr: 0.1000
Epoch 3/100
283/283 [==============================] - 15s 54ms/step - loss: 2.6898 - accuracy: 0.2677 - val_loss: 3.1743 - val_accuracy: 0.1220 - lr: 0.1000
Epoch 4/100
283/283 [==============================] - 15s 55ms/step - loss: 2.6191 - accuracy: 0.3007 - val_loss: 3.7074 - val_accuracy: 0.0870 - lr: 0.0500
Epoch 5/100
283/283 [==============================] - 15s 54ms/step - loss: 2.4844 - accuracy: 0.3310 - val_loss: 4.3855 - val_accuracy: 0.0727 - lr: 0.1000
Epoch 6/100
283/283 [==============================] - 15s 54ms/step - loss: 2.4506 - accuracy: 0.3299 - val_loss: 3.3133 - val_accuracy: 0.1477 - lr: 0.1000
Epoch 7/100
283/283 [==============================] - 15s 54ms/step - loss: 2.3172 - accuracy: 0.3785 - val_loss: 2.5133 - val_accuracy: 0.2767 - lr: 0.1000
Epoch 8/100
283/283 [==============================] - 15s 54ms/step - loss: 2.1956 - accuracy: 0.4066 - val_loss: 6.2235 - val_accuracy: 0.1390 - lr: 0.1000
Epoch 9/100
283/283 [==============================] - 15s 54ms/step - loss: 2.2025 - accuracy: 0.4164 - val_loss: 3.3672 - val_accuracy: 0.2287 - lr: 0.1000
Epoch 10/100
283/283 [==============================] - 15s 54ms/step - loss: 1.8539 - accuracy: 0.5202 - val_loss: 1.8841 - val_accuracy: 0.4727 - lr: 0.0500
Epoch 11/100
283/283 [==============================] - 15s 54ms/step - loss: 1.7417 - accuracy: 0.5614 - val_loss: 3.2715 - val_accuracy: 0.3160 - lr: 0.0500
Epoch 12/100
283/283 [==============================] - 15s 54ms/step - loss: 1.6843 - accuracy: 0.5959 - val_loss: 2.3381 - val_accuracy: 0.4077 - lr: 0.0500
Epoch 13/100
283/283 [==============================] - 15s 54ms/step - loss: 1.5988 - accuracy: 0.6207 - val_loss: 3.5488 - val_accuracy: 0.2077 - lr: 0.0250
Epoch 14/100
283/283 [==============================] - 15s 54ms/step - loss: 1.7192 - accuracy: 0.5881 - val_loss: 1.8248 - val_accuracy: 0.5073 - lr: 0.0500
Epoch 15/100
283/283 [==============================] - 15s 54ms/step - loss: 1.6236 - accuracy: 0.6202 - val_loss: 2.0601 - val_accuracy: 0.4733 - lr: 0.0500
Epoch 16/100
283/283 [==============================] - 15s 54ms/step - loss: 1.5441 - accuracy: 0.6543 - val_loss: 3.8412 - val_accuracy: 0.1627 - lr: 0.0500
Epoch 17/100
283/283 [==============================] - 15s 54ms/step - loss: 1.6608 - accuracy: 0.6232 - val_loss: 2.0656 - val_accuracy: 0.4303 - lr: 0.0250
Epoch 18/100
283/283 [==============================] - 15s 54ms/step - loss: 1.5547 - accuracy: 0.6643 - val_loss: 1.7219 - val_accuracy: 0.5980 - lr: 0.0500
Epoch 19/100
283/283 [==============================] - 15s 54ms/step - loss: 1.5043 - accuracy: 0.6913 - val_loss: 2.3021 - val_accuracy: 0.4387 - lr: 0.0500
Epoch 20/100
283/283 [==============================] - 15s 54ms/step - loss: 1.2401 - accuracy: 0.7661 - val_loss: 1.5388 - val_accuracy: 0.6690 - lr: 0.0250
Epoch 21/100
283/283 [==============================] - 15s 54ms/step - loss: 1.1317 - accuracy: 0.7956 - val_loss: 1.4414 - val_accuracy: 0.6980 - lr: 0.0250
Epoch 22/100
283/283 [==============================] - 15s 54ms/step - loss: 1.1131 - accuracy: 0.7942 - val_loss: 2.9464 - val_accuracy: 0.4113 - lr: 0.0250
Epoch 23/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0782 - accuracy: 0.8075 - val_loss: 3.6021 - val_accuracy: 0.2563 - lr: 0.0250
Epoch 24/100
283/283 [==============================] - 15s 54ms/step - loss: 1.3940 - accuracy: 0.7185 - val_loss: 1.3596 - val_accuracy: 0.7280 - lr: 0.0250
Epoch 25/100
283/283 [==============================] - 15s 54ms/step - loss: 1.1374 - accuracy: 0.8007 - val_loss: 1.2969 - val_accuracy: 0.7483 - lr: 0.0250
Epoch 26/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0785 - accuracy: 0.8152 - val_loss: 1.2905 - val_accuracy: 0.7627 - lr: 0.0250
Epoch 27/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0733 - accuracy: 0.8178 - val_loss: 1.3146 - val_accuracy: 0.7570 - lr: 0.0250
Epoch 28/100
283/283 [==============================] - 15s 54ms/step - loss: 1.1363 - accuracy: 0.8038 - val_loss: 2.5260 - val_accuracy: 0.4600 - lr: 0.0250
Epoch 29/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0549 - accuracy: 0.8254 - val_loss: 1.4674 - val_accuracy: 0.7187 - lr: 0.0125
Epoch 30/100
283/283 [==============================] - 15s 54ms/step - loss: 0.9144 - accuracy: 0.8727 - val_loss: 1.1207 - val_accuracy: 0.8053 - lr: 0.0125
Epoch 31/100
283/283 [==============================] - 15s 54ms/step - loss: 0.8177 - accuracy: 0.8926 - val_loss: 1.0882 - val_accuracy: 0.8070 - lr: 0.0125
Epoch 32/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7732 - accuracy: 0.9033 - val_loss: 2.0674 - val_accuracy: 0.5817 - lr: 0.0125
Epoch 33/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7517 - accuracy: 0.9106 - val_loss: 1.0486 - val_accuracy: 0.8267 - lr: 0.0125
Epoch 34/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7583 - accuracy: 0.9057 - val_loss: 1.3299 - val_accuracy: 0.7443 - lr: 0.0125
Epoch 35/100
283/283 [==============================] - 15s 54ms/step - loss: 0.8385 - accuracy: 0.8804 - val_loss: 1.5974 - val_accuracy: 0.6990 - lr: 0.0125
Epoch 36/100
283/283 [==============================] - 15s 54ms/step - loss: 0.9562 - accuracy: 0.8488 - val_loss: 3.9275 - val_accuracy: 0.2187 - lr: 0.0063
Epoch 37/100
283/283 [==============================] - 15s 54ms/step - loss: 1.2527 - accuracy: 0.7468 - val_loss: 1.1910 - val_accuracy: 0.7720 - lr: 0.0125
Epoch 38/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0627 - accuracy: 0.8213 - val_loss: 1.4232 - val_accuracy: 0.7340 - lr: 0.0125
Epoch 39/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0496 - accuracy: 0.8237 - val_loss: 3.2124 - val_accuracy: 0.2807 - lr: 0.0063
Epoch 40/100
283/283 [==============================] - 15s 54ms/step - loss: 1.0436 - accuracy: 0.8219 - val_loss: 1.5775 - val_accuracy: 0.6860 - lr: 0.0063
Epoch 41/100
283/283 [==============================] - 15s 54ms/step - loss: 0.8529 - accuracy: 0.8838 - val_loss: 1.0949 - val_accuracy: 0.8137 - lr: 0.0063
Epoch 42/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7767 - accuracy: 0.9016 - val_loss: 1.0348 - val_accuracy: 0.8323 - lr: 0.0063
Epoch 43/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7189 - accuracy: 0.9208 - val_loss: 1.0536 - val_accuracy: 0.8210 - lr: 0.0063
Epoch 44/100
283/283 [==============================] - 15s 54ms/step - loss: 0.7087 - accuracy: 0.9229 - val_loss: 1.3360 - val_accuracy: 0.7580 - lr: 0.0063
Epoch 45/100
283/283 [==============================] - 15s 54ms/step - loss: 0.6789 - accuracy: 0.9279 - val_loss: 0.9530 - val_accuracy: 0.8513 - lr: 0.0063
Epoch 46/100
283/283 [==============================] - 15s 54ms/step - loss: 0.6591 - accuracy: 0.9343 - val_loss: 0.9248 - val_accuracy: 0.8657 - lr: 0.0063
Epoch 47/100
283/283 [==============================] - 15s 54ms/step - loss: 0.6102 - accuracy: 0.9472 - val_loss: 0.9223 - val_accuracy: 0.8673 - lr: 0.0063
Epoch 48/100
283/283 [==============================] - 15s 54ms/step - loss: 0.6427 - accuracy: 0.9410 - val_loss: 0.9466 - val_accuracy: 0.8593 - lr: 0.0063
Epoch 49/100
283/283 [==============================] - 15s 54ms/step - loss: 0.6391 - accuracy: 0.9381 - val_loss: 0.9553 - val_accuracy: 0.8497 - lr: 0.0063
Epoch 50/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5939 - accuracy: 0.9469 - val_loss: 1.1013 - val_accuracy: 0.8140 - lr: 0.0016
Epoch 51/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5769 - accuracy: 0.9543 - val_loss: 0.9985 - val_accuracy: 0.8487 - lr: 0.0031
Epoch 52/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5613 - accuracy: 0.9585 - val_loss: 1.0074 - val_accuracy: 0.8380 - lr: 0.0031
Epoch 53/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5560 - accuracy: 0.9595 - val_loss: 0.8805 - val_accuracy: 0.8747 - lr: 0.0031
Epoch 54/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5494 - accuracy: 0.9607 - val_loss: 0.9921 - val_accuracy: 0.8437 - lr: 0.0031
Epoch 55/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5548 - accuracy: 0.9590 - val_loss: 0.9267 - val_accuracy: 0.8610 - lr: 0.0031
Epoch 56/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5349 - accuracy: 0.9656 - val_loss: 0.9929 - val_accuracy: 0.8433 - lr: 0.0016
Epoch 57/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5239 - accuracy: 0.9669 - val_loss: 0.8918 - val_accuracy: 0.8730 - lr: 0.0031
Epoch 58/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5214 - accuracy: 0.9669 - val_loss: 1.0010 - val_accuracy: 0.8390 - lr: 0.0031
Epoch 59/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5119 - accuracy: 0.9710 - val_loss: 0.9157 - val_accuracy: 0.8647 - lr: 0.0016
Epoch 60/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5182 - accuracy: 0.9693 - val_loss: 3.8950 - val_accuracy: 0.3610 - lr: 0.0016
Epoch 61/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5317 - accuracy: 0.9637 - val_loss: 0.8689 - val_accuracy: 0.8813 - lr: 0.0016
Epoch 62/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5157 - accuracy: 0.9671 - val_loss: 0.8699 - val_accuracy: 0.8797 - lr: 0.0016
Epoch 63/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5118 - accuracy: 0.9684 - val_loss: 0.8534 - val_accuracy: 0.8817 - lr: 0.0016
Epoch 64/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4977 - accuracy: 0.9725 - val_loss: 1.1198 - val_accuracy: 0.8110 - lr: 0.0016
Epoch 65/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4955 - accuracy: 0.9720 - val_loss: 0.8557 - val_accuracy: 0.8807 - lr: 0.0016
Epoch 66/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4938 - accuracy: 0.9714 - val_loss: 0.8589 - val_accuracy: 0.8790 - lr: 7.8125e-04
Epoch 67/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4808 - accuracy: 0.9787 - val_loss: 0.8535 - val_accuracy: 0.8813 - lr: 0.0016
Epoch 68/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4925 - accuracy: 0.9719 - val_loss: 0.8605 - val_accuracy: 0.8787 - lr: 0.0016
Epoch 69/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4875 - accuracy: 0.9724 - val_loss: 0.8533 - val_accuracy: 0.8840 - lr: 0.0016
Epoch 70/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4654 - accuracy: 0.9808 - val_loss: 0.9096 - val_accuracy: 0.8703 - lr: 7.8125e-04
Epoch 71/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4699 - accuracy: 0.9797 - val_loss: 0.8723 - val_accuracy: 0.8770 - lr: 7.8125e-04
Epoch 72/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4679 - accuracy: 0.9790 - val_loss: 0.8884 - val_accuracy: 0.8747 - lr: 3.9063e-04
Epoch 73/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4738 - accuracy: 0.9773 - val_loss: 0.8660 - val_accuracy: 0.8757 - lr: 7.8125e-04
Epoch 74/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4782 - accuracy: 0.9760 - val_loss: 0.8660 - val_accuracy: 0.8767 - lr: 7.8125e-04
Epoch 75/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4757 - accuracy: 0.9762 - val_loss: 2.2599 - val_accuracy: 0.6137 - lr: 3.9063e-04
Epoch 76/100
283/283 [==============================] - 15s 54ms/step - loss: 0.5078 - accuracy: 0.9652 - val_loss: 0.8696 - val_accuracy: 0.8757 - lr: 7.8125e-04
Epoch 77/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4883 - accuracy: 0.9722 - val_loss: 0.8571 - val_accuracy: 0.8777 - lr: 7.8125e-04
Epoch 78/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4748 - accuracy: 0.9747 - val_loss: 0.8700 - val_accuracy: 0.8793 - lr: 3.9063e-04
Epoch 79/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4745 - accuracy: 0.9762 - val_loss: 0.8458 - val_accuracy: 0.8843 - lr: 7.8125e-04
Epoch 80/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4642 - accuracy: 0.9811 - val_loss: 0.8446 - val_accuracy: 0.8837 - lr: 3.9063e-04
Epoch 81/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4696 - accuracy: 0.9792 - val_loss: 0.8460 - val_accuracy: 0.8820 - lr: 3.9063e-04
Epoch 82/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4686 - accuracy: 0.9780 - val_loss: 0.8423 - val_accuracy: 0.8827 - lr: 1.9531e-04
Epoch 83/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4758 - accuracy: 0.9770 - val_loss: 0.8464 - val_accuracy: 0.8827 - lr: 3.9063e-04
Epoch 84/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4753 - accuracy: 0.9759 - val_loss: 0.8532 - val_accuracy: 0.8833 - lr: 3.9063e-04
Epoch 85/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4645 - accuracy: 0.9798 - val_loss: 0.8403 - val_accuracy: 0.8830 - lr: 1.9531e-04
Epoch 86/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4646 - accuracy: 0.9783 - val_loss: 0.8376 - val_accuracy: 0.8840 - lr: 3.9063e-04
Epoch 87/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4666 - accuracy: 0.9795 - val_loss: 0.8457 - val_accuracy: 0.8827 - lr: 3.9063e-04
Epoch 88/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4634 - accuracy: 0.9814 - val_loss: 0.8357 - val_accuracy: 0.8847 - lr: 3.9063e-04
Epoch 89/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4653 - accuracy: 0.9792 - val_loss: 0.8386 - val_accuracy: 0.8833 - lr: 3.9063e-04
Epoch 90/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4631 - accuracy: 0.9792 - val_loss: 0.8411 - val_accuracy: 0.8817 - lr: 1.9531e-04
Epoch 91/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4647 - accuracy: 0.9786 - val_loss: 0.8302 - val_accuracy: 0.8840 - lr: 9.7656e-05
Epoch 92/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4626 - accuracy: 0.9798 - val_loss: 0.8405 - val_accuracy: 0.8813 - lr: 1.9531e-04
Epoch 93/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4602 - accuracy: 0.9817 - val_loss: 0.8361 - val_accuracy: 0.8823 - lr: 1.9531e-04
Epoch 94/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4585 - accuracy: 0.9804 - val_loss: 0.8284 - val_accuracy: 0.8853 - lr: 1.9531e-04
Epoch 95/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4575 - accuracy: 0.9822 - val_loss: 0.8299 - val_accuracy: 0.8830 - lr: 1.9531e-04
Epoch 96/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4602 - accuracy: 0.9791 - val_loss: 0.8327 - val_accuracy: 0.8857 - lr: 1.9531e-04
Epoch 97/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4623 - accuracy: 0.9802 - val_loss: 0.8293 - val_accuracy: 0.8850 - lr: 1.9531e-04
Epoch 98/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4561 - accuracy: 0.9803 - val_loss: 0.8346 - val_accuracy: 0.8850 - lr: 1.9531e-04
Epoch 99/100
283/283 [==============================] - 15s 54ms/step - loss: 0.4648 - accuracy: 0.9786 - val_loss: 0.8371 - val_accuracy: 0.8837 - lr: 9.7656e-05
Epoch 100/100
283/283 [==============================] - 15s 55ms/step - loss: 0.4634 - accuracy: 0.9776 - val_loss: 0.8354 - val_accuracy: 0.8843 - lr: 9.7656e-05
94/94 [==============================] - 4s 40ms/step - loss: 0.8427 - accuracy: 0.8887
Test Accuracy: 0.8886666893959045
Test Loss: 0.8426769971847534
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6db4b06264714ae48d2432d45cea95c0/6daaf4996274a8185942776fe27278cae4d6ede3.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="61">
<div class="sourceCode" id="cb93"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    test_data,</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3000 validated image filenames belonging to 15 classes.
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="62">
<div class="sourceCode" id="cb95"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>test_gen.reset()</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>best_model_31_loaded <span class="op">=</span> load_model(<span class="st">&#39;best_model_31.keras&#39;</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>best_model_31_loaded.evaluate(test_gen)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>94/94 [==============================] - 4s 39ms/step - loss: 0.8427 - accuracy: 0.8887
</code></pre>
</div>
<div class="output execute_result" data-execution_count="62">
<pre><code>[0.8426771759986877, 0.8886666893959045]</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Test accuracy was higher than max validation accuracy</p>
</div>
<section id="model-evaluation" class="cell markdown">
<h1><strong>MODEL EVALUATION</strong></h1>
</section>
<section id="best-31x31-model" class="cell markdown">
<h2>Best 31x31 Model</h2>
</section>
<div class="cell code" data-execution_count="20">
<div class="sourceCode" id="cb98"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    test_data,</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_31,</span>
<span id="cb98-6"><a href="#cb98-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb98-7"><a href="#cb98-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb98-8"><a href="#cb98-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb98-9"><a href="#cb98-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb98-10"><a href="#cb98-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb98-11"><a href="#cb98-11" aria-hidden="true" tabindex="-1"></a>test_gen.class_indices</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3000 validated image filenames belonging to 15 classes.
</code></pre>
</div>
<div class="output execute_result" data-execution_count="20">
<pre><code>{&#39;Bean&#39;: 0,
 &#39;Bitter_Gourd&#39;: 1,
 &#39;Bottle_Gourd&#39;: 2,
 &#39;Brinjal&#39;: 3,
 &#39;Broccoli&#39;: 4,
 &#39;Cabbage&#39;: 5,
 &#39;Capsicum&#39;: 6,
 &#39;Carrot&#39;: 7,
 &#39;Cauliflower&#39;: 8,
 &#39;Cucumber&#39;: 9,
 &#39;Papaya&#39;: 10,
 &#39;Potato&#39;: 11,
 &#39;Pumpkin&#39;: 12,
 &#39;Radish&#39;: 13,
 &#39;Tomato&#39;: 14}</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb101"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, confusion_matrix</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model_31_loaded.predict(test_gen)</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>y_pred_classes <span class="op">=</span> [np.argmax(element) <span class="cf">for</span> element <span class="kw">in</span> y_pred]</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_gen.classes</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_classes))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>94/94 [==============================] - 4s 40ms/step
              precision    recall  f1-score   support

           0       0.78      0.93      0.85       200
           1       0.97      0.90      0.93       200
           2       0.98      0.95      0.97       200
           3       0.87      0.92      0.90       200
           4       0.94      0.88      0.90       200
           5       0.81      0.91      0.86       200
           6       0.94      0.86      0.90       200
           7       0.96      0.81      0.88       200
           8       0.93      0.82      0.87       200
           9       0.96      0.93      0.94       200
          10       0.91      0.91      0.91       200
          11       0.72      0.93      0.81       200
          12       0.96      0.91      0.93       200
          13       0.91      0.83      0.87       200
          14       0.82      0.85      0.84       200

    accuracy                           0.89      3000
   macro avg       0.90      0.89      0.89      3000
weighted avg       0.90      0.89      0.89      3000

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The classification report reflects a <strong>Balanced
Dataset</strong> with each class represented by <code>200</code>
instances, ensuring no bias in model performance due to sample size
disparities. The <strong>High Accuracy</strong> of the model stands at
<code>0.89</code>, denoting its proficiency in accurately predicting the
correct class.</p>
<p>There is <strong>Consistent Performance</strong> across the board,
with macro and weighted averages for precision, recall, and f1-score
hovering around <code>0.89</code> to <code>0.90</code>, which points to
a uniform performance across all classes. However, Class-Specific
Variances are noted, as some classes like class 11 (Potato) display a
lower precision of <code>0.72</code>, hinting at <strong>possible false
positives</strong>.</p>
<p>The model maintains a <strong>Good Balance between Precision and
Recall</strong>, with f1-scores being relatively high for all classes.
This balance is particularly important in scenarios that are sensitive
to both false positives and false negatives.</p>
<p>There are potential areas for improvement; classes like class 11
(Potato) and class 14 (Tomato) exhibit lower f1-scores, signaling a need
for further investigation to enhance model performance for these
classes.</p>
<p>Overall, the model exudes <strong>Reliability</strong>, with high
precision, recall, and f1-score values across most classes, suggesting
it is dependable for predictions in this specific domain or dataset.</p>
</div>
<section id="best-128x128-model" class="cell markdown">
<h2>Best 128x128 Model</h2>
</section>
<div class="cell code" data-execution_count="21">
<div class="sourceCode" id="cb103"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>test_gen <span class="op">=</span> test_gen_format.flow_from_dataframe(</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>    test_data,</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    x_col<span class="op">=</span><span class="st">&quot;filepath&quot;</span>,</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>    y_col<span class="op">=</span><span class="st">&quot;label&quot;</span>,</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    target_size<span class="op">=</span>INPUT_SHAPE_128,</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>    class_mode<span class="op">=</span><span class="st">&quot;categorical&quot;</span>,</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>    color_mode<span class="op">=</span><span class="st">&quot;grayscale&quot;</span>,</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>test_gen.class_indices</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 3000 validated image filenames belonging to 15 classes.
</code></pre>
</div>
<div class="output execute_result" data-execution_count="21">
<pre><code>{&#39;Bean&#39;: 0,
 &#39;Bitter_Gourd&#39;: 1,
 &#39;Bottle_Gourd&#39;: 2,
 &#39;Brinjal&#39;: 3,
 &#39;Broccoli&#39;: 4,
 &#39;Cabbage&#39;: 5,
 &#39;Capsicum&#39;: 6,
 &#39;Carrot&#39;: 7,
 &#39;Cauliflower&#39;: 8,
 &#39;Cucumber&#39;: 9,
 &#39;Papaya&#39;: 10,
 &#39;Potato&#39;: 11,
 &#39;Pumpkin&#39;: 12,
 &#39;Radish&#39;: 13,
 &#39;Tomato&#39;: 14}</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="60">
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> best_model_128_loaded.predict(test_gen)</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>y_pred_classes <span class="op">=</span> [np.argmax(element) <span class="cf">for</span> element <span class="kw">in</span> y_pred]</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_gen.classes</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_test, y_pred_classes))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>94/94 [==============================] - 4s 41ms/step
              precision    recall  f1-score   support

           0       0.94      0.96      0.95       200
           1       0.98      0.85      0.91       200
           2       0.91      0.97      0.94       200
           3       0.86      0.89      0.87       200
           4       0.87      0.97      0.92       200
           5       0.81      0.92      0.86       200
           6       0.99      0.92      0.95       200
           7       0.97      0.83      0.90       200
           8       0.98      0.86      0.91       200
           9       0.93      0.97      0.95       200
          10       0.95      0.91      0.93       200
          11       0.97      0.85      0.90       200
          12       0.81      0.98      0.88       200
          13       0.99      0.91      0.95       200
          14       0.92      0.98      0.95       200

    accuracy                           0.92      3000
   macro avg       0.92      0.92      0.92      3000
weighted avg       0.92      0.92      0.92      3000

</code></pre>
</div>
</div>
<div class="cell markdown">
<p>The classification report indicates an <strong>overall high
performance</strong> with a model accuracy of <code>0.92</code>. This
suggests the model's robustness in predicting the correct classes.
Precision scores are notable across all classes, with the lowest being
<code>0.81</code> for class 5 (Cabbage), indicating a high likelihood of
correct predictions when the model assigns a class. Similarly, recall
scores are commendable, with the lowest at <code>0.85</code> for class 1
(Bitter Gourd), suggesting the model's competence in identifying the
majority of positive instances for each class.</p>
<p>F1-scores, which represent the harmonic mean of precision and recall,
are strong for all classes, signifying a well-balanced model. The
support for each class is <code>200</code>, demonstrating a balanced
dataset that aids in preventing class imbalance from skewing the model's
performance metrics.</p>
<p>Both macro and weighted averages stand at <code>0.92</code> for
precision, recall, and f1-score, which reflects the model's uniform
performance across all classes. Despite the high scores, there might be
potential for fine-tuning, especially for classes with the lowest
metrics to further enhance the model's performance.</p>
</div>
</body>
</html>
